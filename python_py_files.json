{
    "repo_name": "Sofcom/treeio",
    "ref": "refs/heads/2.0",
    "path": "treeio/core/tests.py",
    "copies": "2",
    "content": "# encoding: utf-8\n# Copyright 2011 Tree.io Limited\n# This file is part of Treeio.\n# License www.tree.io/license\n\n\"\"\"\nCore: test suites\nMiddleware: test chat\n\"\"\"\nimport datetime\n\nfrom django.test import TestCase\nfrom django.test.client import Client\nfrom django.core.urlresolvers import reverse\nfrom django.contrib.auth.models import User as DjangoUser\nfrom treeio.identities.models import Contact\nfrom treeio.core.models import User, Group, ModuleSetting, Module, Object, Perspective, AccessEntity\n\n\nclass CoreModelsTest(TestCase):\n    \"\"\"Core Model Tests\"\"\"\n    fixtures = ['myinitial_data.json']\n\n    def test_model_AccessEntity(self):\n        obj = AccessEntity()\n        obj.save()\n        self.assertTrue(obj.last_updated - datetime.datetime.now() \u003c datetime.timedelta(seconds=1))\n\n        self.assertIsNone(obj.get_entity())\n        self.assertFalse(obj.is_user())\n        self.assertEqual(obj.__unicode__(), str(obj.id))\n        self.assertEqual(obj.get_absolute_url(), '')\n\n    def test_model_Group_basic(self):\n        \"\"\"Test Group model\"\"\"\n        name = 'testgroup'\n        obj = Group(name=name)\n        obj.save()\n        self.assertIsNone(obj.parent)\n        self.assertIsNone(obj.details)\n        self.assertQuerysetEqual(obj.child_set.all(), [])\n        self.assertEqual(obj.get_absolute_url(), '/contacts/group/view/{}'.format(obj.id))\n        self.assertEqual(obj.get_root(), obj)\n        self.assertEqual(obj.get_tree_path(), [obj])\n        self.assertIsNone(obj.get_contact())\n        self.assertFalse(obj.has_contact())\n        self.assertEqual(obj.get_fullname(), name)\n        self.assertEqual(obj.get_perspective(), Perspective.objects.all()[0])\n        # todo obj.set_perspective()\n\n    def test_model_User_profile(self):\n        \"\"\"Test User model\"\"\"\n        username = \"testusername\"\n        password = \"password\"\n        user = DjangoUser(username=username)\n        user.set_password(password)\n        user.save()\n        self.assertEquals(user.username, username)\n        self.assertIsNotNone(user.id)\n        profile = user.profile\n        self.assertEquals(profile.name, username)\n        self.assertEquals(profile.default_group, Group.objects.all()[0])\n        self.assertQuerysetEqual(profile.other_groups.all(), [])\n        self.assertFalse(profile.disabled)\n        self.assertTrue(profile.last_access - datetime.datetime.now() \u003c datetime.timedelta(seconds=1))\n        self.assertEqual(profile.get_absolute_url(), '/contacts/user/view/{}'.format(profile.id))\n        oldpsw = profile.user.password\n        self.assertNotEqual(profile.generate_new_password(), oldpsw)\n        self.assertQuerysetEqual(profile.get_groups(), map(repr, [profile.default_group]))\n        self.assertTrue(profile.is_admin())\n        self.assertEqual(profile.get_username(), username)\n        self.assertEqual(profile.get_perspective(), Perspective.objects.get(name='Default'))\n        self.assertEqual(profile.get_contact(), Contact.objects.get(related_user=profile))\n        self.assertTrue(profile.has_contact())\n\n        self.assertEqual(profile.__unicode__(), username)\n\n    def test_model_User_profile_change_default_group(self):\n        username = \"testusername\"\n        user = DjangoUser(username=username)\n        user.save()\n        profile = user.profile\n        group = Group(name='testgroupname')\n        group.save()\n        profile.default_group = group\n        profile.save()\n        profile = User.objects.get(user=user)\n        self.assertEquals(profile.default_group, group)\n\n    def test_model_Module_basic(self):\n        \"\"\"Test Module model with minimum parameters\"\"\"\n        name = 'test module'\n        title = 'Test title'\n        url = '/test_url/'\n        obj = Module(name=name, title=title, url=url)\n        obj.save()\n        self.assertIsNotNone(obj.id)\n        obj = Module.objects.get(id=obj.id)\n        self.assertEquals(obj.name, name)\n        self.assertEquals(obj.title, title)\n        self.assertEquals(obj.url, url)\n        self.assertEquals(obj.details, '')\n        self.assertTrue(obj.display)\n        self.assertTrue(obj.system)\n        self.assertEqual(obj.get_absolute_url(), '/admin/module/view/{}'.format(obj.id))\n\n    def test_model_Perspective_basic(self):\n        \"\"\"Test Perspective model with minimum parameters\"\"\"\n        name = 'test'\n        obj = Perspective(name=name)\n        obj.save()\n        self.assertIsNotNone(obj.id)\n        obj = Perspective.objects.get(id=obj.id)\n        self.assertEqual(obj.name, name)\n        self.assertEqual(obj.details, '')\n        self.assertQuerysetEqual(obj.modules.all(), [])\n        # default is to have all modules available\n        self.assertQuerysetEqual(obj.get_modules(), map(repr, Module.objects.all()))\n        self.assertEqual(obj.get_absolute_url(), '/admin/perspective/view/{}'.format(obj.id))\n\n    def test_model_Perspective_full(self):\n        \"\"\"Test Perspective model with all parameters\"\"\"\n        name = 'test'\n        details = 'perspective details'\n        obj = Perspective(name='test', details=details)\n        obj.save()\n        self.assertIsNotNone(obj.id)\n        obj = Perspective.objects.get(id=obj.id)\n        self.assertEqual(obj.name, name)\n        self.assertEqual(obj.details, details)\n        module = Module.objects.all()[0]\n        obj.modules.add(module)\n        self.assertQuerysetEqual(obj.modules.all(), map(repr, [module]))\n        self.assertQuerysetEqual(obj.get_modules(), map(repr, [module]))\n        self.assertEqual(obj.get_absolute_url(), '/admin/perspective/view/{}'.format(obj.id))\n\n\nclass CoreViewsTestLoggedIn(TestCase):\n    \"\"\"Core View tests when logged in\"\"\"\n    username = \"test\"\n    password = \"password\"\n\n    def setUp(self):\n        self.group, created = Group.objects.get_or_create(name='test')\n        duser, created = DjangoUser.objects.get_or_create(username=self.username)\n        duser.set_password(self.password)\n        duser.save()\n        self.user = duser\n\n        self.perspective = Perspective(name='test')\n        self.perspective.set_default_user()\n        self.perspective.save()\n\n        self.client.login(username=self.username, password=self.password)\n\n    def test_user_logout(self):\n        \"\"\"Test logout page at /logout\"\"\"\n        response = self.client.get(reverse('user_logout'))\n        self.assertRedirects(response, reverse('user_login'))\n        self.assertNotIn('_auth_user_id', self.client.session)\n\n    def test_user_login(self):\n        \"\"\"Test login page at /login\"\"\"\n        response = self.client.post(reverse('user_login'), {'username': self.username, 'password': self.password})\n        self.assertRedirects(response, reverse('user_denied'))\n\n    def test_home_login(self):\n        \"\"\"Test home page with login at /\"\"\"\n        response = self.client.get('/')\n        self.assertEquals(response.status_code, 200)\n        self.assertEqual(self.client.session['_auth_user_id'], self.user.pk)\n\n    # Perspectives\n    def test_index_perspectives_login(self):\n        \"\"\"Test page with login at /admin/perspectives/\"\"\"\n        response = self.client.get(reverse('core_admin_index_perspectives'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_perspective_add(self):\n        \"\"\"Test index page with login at /admin/perspective/add\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_add'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_perspective_view(self):\n        \"\"\"Test index page with login at /admin/perspective/view/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_view', args=[self.perspective.id]))\n        self.assertEquals(response.status_code, 200)\n\n    def test_perspective_edit(self):\n        \"\"\"Test index page with login at /admin/perspective/edit/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_edit', args=[self.perspective.id]))\n        self.assertEquals(response.status_code, 200)\n\n    def test_perspective_delete(self):\n        \"\"\"Test index page with login at /admin/perspective/delete/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_delete', args=[self.perspective.id]))\n        self.assertEquals(response.status_code, 200)\n\n    # Modules\n    def test_index_modules_login(self):\n        \"\"\"Test page with login at /admin/modules/\"\"\"\n        response = self.client.get(reverse('core_admin_index_modules'))\n        self.assertEquals(response.status_code, 200)\n\n    # Users\n    def test_index_users_login(self):\n        \"\"\"Test page with login at /admin/users/\"\"\"\n        response = self.client.get(reverse('core_admin_index_users'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_user_add(self):\n        \"\"\"Test page with login at /admin/user/add\"\"\"\n        name = 'newuser'\n        password = 'newuserpsw'\n        data = {'name': name, 'password': password, 'password_again': password}\n        response = self.client.post(path=reverse('core_admin_user_add'), data=data)\n        self.assertEquals(response.status_code, 302)\n        profile = User.objects.get(name=name)\n\n        self.assertEquals(profile.name, name)\n        self.assertRedirects(response, reverse('core_admin_user_view', args=[profile.id]))\n        self.assertEquals(self.client.login(username=name, password=password), True)\n        self.client.logout()\n        response = self.client.post('/accounts/login', {'username': name, 'password': password})\n        self.assertRedirects(response, '/')\n\n    def test_core_user_delete(self):\n        \"\"\"Test page with login at /admin/user/delete\"\"\"\n        name = 'newuser'\n        password = 'newuserpsw'\n        user, created = DjangoUser.objects.get_or_create(username=name)\n        if created:\n            user.set_password(password)\n            user.save()\n        response = self.client.post(path=reverse('core_admin_user_delete', args=[user.profile.id]),\n                                    data={'delete': ''})\n        self.assertRedirects(response, reverse('core_admin_index_users'))\n        with self.assertRaises(User.DoesNotExist):\n            User.objects.get(name=name)\n        with self.assertRaises(DjangoUser.DoesNotExist):\n            DjangoUser.objects.get(username=name)\n\n    def test_core_user_invite(self):\n        \"\"\"Test page with login at /admin/user/invite\"\"\"\n        response = self.client.get(reverse('core_admin_user_invite'))\n        self.assertEquals(response.status_code, 200)\n\n    # Groups\n    def test_index_groups_login(self):\n        \"\"\"Test page with login at /admin/groups/\"\"\"\n        response = self.client.get(reverse('core_admin_index_groups'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_group_add(self):\n        \"\"\"Test page with login at /admin/group/add\"\"\"\n        response = self.client.get(reverse('core_admin_group_add'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_group_view(self):\n        \"\"\"Test index page with login at /admin/group/view/\u003cgroup_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_group_view', args=[self.group.id]))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_group_edit(self):\n        \"\"\"Test index page with login at /admin/group/edit/\u003cgroup_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_group_edit', args=[self.group.id]))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_group_delete(self):\n        \"\"\"Test index page with login at /admin/group/delete/\u003cgroup_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_group_delete', args=[self.group.id]))\n        self.assertEquals(response.status_code, 200)\n\n    # Settings\n    def test_core_settings_view(self):\n        \"\"\"Test index page with login at /admin/settings/view/\"\"\"\n        response = self.client.get(reverse('core_settings_view'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_core_settings_edit(self):\n        \"\"\"Test index page with login at /admin/settings/edit/\"\"\"\n        response = self.client.get(reverse('core_settings_edit'))\n        self.assertEquals(response.status_code, 200)\n\n\nclass CoreViewsTestNoLogin(TestCase):\n    \"\"\"Core View tests when not logged in\"\"\"\n    username = \"test\"\n    password = \"password\"\n\n    def setUp(self):\n        self.group, created = Group.objects.get_or_create(name='test')\n        duser, created = DjangoUser.objects.get_or_create(username=self.username)\n        duser.set_password(self.password)\n        duser.save()\n        self.user = duser\n\n        self.perspective = Perspective(name='test')\n        self.perspective.set_default_user()\n        self.perspective.save()\n\n    def test_user_logout(self):\n        \"\"\"A logout request from an already logged out user should be harmless\n        \"\"\"\n        response = self.client.get(reverse('user_logout'))\n        self.assertRedirects(response, reverse('user_login'))\n        self.assertNotIn('_auth_user_id', self.client.session)\n\n    def test_user_login(self):\n        \"\"\"Test login page at /login\"\"\"\n        response = self.client.post(reverse('user_login'), {'username': self.username, 'password': self.password})\n        self.assertRedirects(response, '/')\n        self.assertEqual(self.client.session['_auth_user_id'], self.user.pk)\n\n    def test_logo(self):\n        \"\"\"Just test that the logo view works\"\"\"\n        response = self.client.get(reverse('core_logo_image'))\n        self.assertEquals(response.status_code, 200)\n\n    def test_home(self):\n        \"\"\"Test home page at /\"\"\"\n        response = self.client.get('/')\n        # Redirects as unauthenticated\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_index_perspectives_out(self):\n        \"\"\"Test page at /admin/perspectives/\"\"\"\n        response = self.client.get(reverse('core_admin_index_perspectives'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_perspective_add_out(self):\n        \"\"\"Test add perspective page at /admin/perspective/add\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_add'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_perspective_view_out(self):\n        \"\"\"Test perspective view at /admin/perspective/view/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_view', args=[self.perspective.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_perspective_edit_out(self):\n        \"\"\"Test perspective add at /admin/perspective/edit/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_edit', args=[self.perspective.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_perspective_delete_out(self):\n        \"\"\"Test perspective delete at /admin/perspective/delete/\u003cperspective_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_perspective_delete', args=[self.perspective.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    # Modules\n    def test_index_modules_out(self):\n        \"\"\"Test index modules page at /admin/modules/\"\"\"\n        response = self.client.get(reverse('core_admin_index_modules'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    # Users\n    def test_index_users_out(self):\n        \"\"\"Test index users page at /admin/users/\"\"\"\n        response = self.client.get(reverse('core_admin_index_users'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_user_add_out(self):\n        \"\"\"Test user add at /admin/user/add\"\"\"\n        response = self.client.get(reverse('core_admin_user_add'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_user_invite_out(self):\n        \"\"\"Test user invite at /admin/user/invite\"\"\"\n        response = self.client.get(reverse('core_admin_user_invite'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    # Groups\n    def test_index_groups_out(self):\n        \"\"\"Test index groups at /admin/groups/\"\"\"\n        response = self.client.get(reverse('core_admin_index_groups'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_group_add_out(self):\n        \"\"\"Test group add at /admin/group/add\"\"\"\n        response = self.client.get(reverse('core_admin_group_add'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_group_view_out(self):\n        \"\"\"Test group view at /admin/group/view/\u003cgroup_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_group_view', args=[self.group.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_group_edit_out(self):\n        \"\"\"Test group edit at /admin/group/edit/\u003cgroup_id\u003e\"\"\"\n        response = self.client.get(reverse('core_admin_group_edit', args=[self.group.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_group_delete_out(self):\n        \"Test group delete at /admin/group/delete/\u003cgroup_id\u003e\"\n        response = self.client.get(reverse('core_admin_group_delete', args=[self.group.id]))\n        self.assertRedirects(response, reverse('user_login'))\n\n    # Settings\n    def test_core_settings_view_out(self):\n        \"\"\"Test isettings view at /admin/settings/view/\"\"\"\n        response = self.client.get(reverse('core_settings_view'))\n        self.assertRedirects(response, reverse('user_login'))\n\n    def test_core_settings_edit_out(self):\n        \"\"\"Test settings edit at /admin/settings/edit/\"\"\"\n        response = self.client.get(reverse('core_settings_edit'))\n        self.assertRedirects(response, reverse('user_login'))\n\n\nclass MiddlewareChatTest(TestCase):\n    \"\"\"Midleware chat tests\"\"\"\n    username = \"test\"\n    password = \"password\"\n\n    def setUp(self):\n        self.group, created = Group.objects.get_or_create(name='test')\n        duser, created = DjangoUser.objects.get_or_create(username=self.username)\n        duser.set_password(self.password)\n        duser.save()\n\n        self.perspective = Perspective(name='test')\n        self.perspective.set_default_user()\n        self.perspective.save()\n\n    def test_chat_get_new_messages(self):\n        \"\"\"Test get_new_messages\"\"\"\n        response = self.client.post(\n            '/chat', {'json': '{\"cmd\":\"Get\", \"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_connect(self):\n        \"\"\"Test connect\"\"\"\n        response = self.client.post(\n            '/chat', {'json': '{\"cmd\":\"Connect\", \"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_disconnect(self):\n        \"\"\"Test disconnect\"\"\"\n        response = self.client.post(\n            '/chat', {'json': '{\"cmd\":\"Disconnect\", \"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_add_new_message(self):\n        \"\"\"Test add_new_message\"\"\"\n        response = self.client.post(\n            '/chat', {\n                'json': '{\"cmd\":\"Message\",\"data\":{\"id\":\"test_b5e6d0470a5f4656c3bc77f879c3dbbc\",\"text\":\"test message\"},\"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_exit_from_conference(self):\n        \"\"\"Test exit_from_conference\"\"\"\n        response = self.client.post(\n            '/chat', {'json': '{\"cmd\":\"Exit\",\"data\":{\"id\":\"test_b5e6d0470a5f4656c3bc77f879c3dbbc\"},\"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_add_users_in_conference(self):\n        \"\"\"Test add_users_in_conference\"\"\"\n        response = self.client.post(\n            '/chat', {\n                'json': '{\"cmd\":\"Add\",\"data\":{\"id\":\"guest_006f721c4a59a44d969b9f73fb6360a5\",\"users\":[\"test\"]},\"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_chat_create_conference(self):\n        \"\"\"Test create_conference\"\"\"\n        response = self.client.post(\n            '/chat', {'json': '{\"cmd\":\"Create\",\"data\":{\"title\":[\"Admin\"],\"users\":[\"admin\"]},\"location\":\"#\"}'})\n        self.assertEqual(response.status_code, 200)\n"
}
{
    "repo_name": "Azure/azure-sdk-for-python",
    "ref": "refs/heads/main",
    "path": "sdk/appplatform/azure-mgmt-appplatform/azure/mgmt/appplatform/v2022_03_01_preview/aio/operations/_monitoring_settings_operations.py",
    "copies": "1",
    "content": "# pylint: disable=too-many-lines\n# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for license information.\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is regenerated.\n# --------------------------------------------------------------------------\nfrom typing import Any, Callable, Dict, Optional, TypeVar, Union\n\nfrom azure.core.exceptions import ClientAuthenticationError, HttpResponseError, ResourceExistsError, ResourceNotFoundError, map_error\nfrom azure.core.pipeline import PipelineResponse\nfrom azure.core.pipeline.transport import AsyncHttpResponse\nfrom azure.core.polling import AsyncLROPoller, AsyncNoPolling, AsyncPollingMethod\nfrom azure.core.rest import HttpRequest\nfrom azure.core.tracing.decorator_async import distributed_trace_async\nfrom azure.mgmt.core.exceptions import ARMErrorFormat\nfrom azure.mgmt.core.polling.async_arm_polling import AsyncARMPolling\n\nfrom ... import models as _models\nfrom ..._vendor import _convert_request\nfrom ...operations._monitoring_settings_operations import build_get_request, build_update_patch_request_initial, build_update_put_request_initial\nT = TypeVar('T')\nClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]\n\nclass MonitoringSettingsOperations:\n    \"\"\"MonitoringSettingsOperations async operations.\n\n    You should not instantiate this class directly. Instead, you should create a Client instance that\n    instantiates it for you and attaches it as an attribute.\n\n    :ivar models: Alias to model classes used in this operation group.\n    :type models: ~azure.mgmt.appplatform.v2022_03_01_preview.models\n    :param client: Client for service requests.\n    :param config: Configuration of service client.\n    :param serializer: An object model serializer.\n    :param deserializer: An object model deserializer.\n    \"\"\"\n\n    models = _models\n\n    def __init__(self, client, config, serializer, deserializer) -\u003e None:\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self._config = config\n\n    @distributed_trace_async\n    async def get(\n        self,\n        resource_group_name: str,\n        service_name: str,\n        **kwargs: Any\n    ) -\u003e \"_models.MonitoringSettingResource\":\n        \"\"\"Get the Monitoring Setting and its properties.\n\n        :param resource_group_name: The name of the resource group that contains the resource. You can\n         obtain this value from the Azure Resource Manager API or the portal.\n        :type resource_group_name: str\n        :param service_name: The name of the Service resource.\n        :type service_name: str\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :return: MonitoringSettingResource, or the result of cls(response)\n        :rtype: ~azure.mgmt.appplatform.v2022_03_01_preview.models.MonitoringSettingResource\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.MonitoringSettingResource\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n\n        api_version = kwargs.pop('api_version', \"2022-03-01-preview\")  # type: str\n\n        \n        request = build_get_request(\n            subscription_id=self._config.subscription_id,\n            resource_group_name=resource_group_name,\n            service_name=service_name,\n            api_version=api_version,\n            template_url=self.get.metadata['url'],\n        )\n        request = _convert_request(request)\n        request.url = self._client.format_url(request.url)\n\n        pipeline_response = await self._client._pipeline.run(  # pylint: disable=protected-access\n            request,\n            stream=False,\n            **kwargs\n        )\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            raise HttpResponseError(response=response, error_format=ARMErrorFormat)\n\n        deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n\n    get.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.AppPlatform/Spring/{serviceName}/monitoringSettings/default\"}  # type: ignore\n\n\n    async def _update_put_initial(\n        self,\n        resource_group_name: str,\n        service_name: str,\n        monitoring_setting_resource: \"_models.MonitoringSettingResource\",\n        **kwargs: Any\n    ) -\u003e \"_models.MonitoringSettingResource\":\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.MonitoringSettingResource\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n\n        api_version = kwargs.pop('api_version', \"2022-03-01-preview\")  # type: str\n        content_type = kwargs.pop('content_type', \"application/json\")  # type: Optional[str]\n\n        _json = self._serialize.body(monitoring_setting_resource, 'MonitoringSettingResource')\n\n        request = build_update_put_request_initial(\n            subscription_id=self._config.subscription_id,\n            resource_group_name=resource_group_name,\n            service_name=service_name,\n            api_version=api_version,\n            content_type=content_type,\n            json=_json,\n            template_url=self._update_put_initial.metadata['url'],\n        )\n        request = _convert_request(request)\n        request.url = self._client.format_url(request.url)\n\n        pipeline_response = await self._client._pipeline.run(  # pylint: disable=protected-access\n            request,\n            stream=False,\n            **kwargs\n        )\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200, 202]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            raise HttpResponseError(response=response, error_format=ARMErrorFormat)\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n\n        if response.status_code == 202:\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n\n    _update_put_initial.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.AppPlatform/Spring/{serviceName}/monitoringSettings/default\"}  # type: ignore\n\n\n    @distributed_trace_async\n    async def begin_update_put(\n        self,\n        resource_group_name: str,\n        service_name: str,\n        monitoring_setting_resource: \"_models.MonitoringSettingResource\",\n        **kwargs: Any\n    ) -\u003e AsyncLROPoller[\"_models.MonitoringSettingResource\"]:\n        \"\"\"Update the Monitoring Setting.\n\n        :param resource_group_name: The name of the resource group that contains the resource. You can\n         obtain this value from the Azure Resource Manager API or the portal.\n        :type resource_group_name: str\n        :param service_name: The name of the Service resource.\n        :type service_name: str\n        :param monitoring_setting_resource: Parameters for the update operation.\n        :type monitoring_setting_resource:\n         ~azure.mgmt.appplatform.v2022_03_01_preview.models.MonitoringSettingResource\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :keyword str continuation_token: A continuation token to restart a poller from a saved state.\n        :keyword polling: By default, your polling method will be AsyncARMPolling. Pass in False for\n         this operation to not poll, or pass in your own initialized polling object for a personal\n         polling strategy.\n        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod\n        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no\n         Retry-After header is present.\n        :return: An instance of AsyncLROPoller that returns either MonitoringSettingResource or the\n         result of cls(response)\n        :rtype:\n         ~azure.core.polling.AsyncLROPoller[~azure.mgmt.appplatform.v2022_03_01_preview.models.MonitoringSettingResource]\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        api_version = kwargs.pop('api_version', \"2022-03-01-preview\")  # type: str\n        content_type = kwargs.pop('content_type', \"application/json\")  # type: Optional[str]\n        polling = kwargs.pop('polling', True)  # type: Union[bool, AsyncPollingMethod]\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.MonitoringSettingResource\"]\n        lro_delay = kwargs.pop(\n            'polling_interval',\n            self._config.polling_interval\n        )\n        cont_token = kwargs.pop('continuation_token', None)  # type: Optional[str]\n        if cont_token is None:\n            raw_result = await self._update_put_initial(\n                resource_group_name=resource_group_name,\n                service_name=service_name,\n                monitoring_setting_resource=monitoring_setting_resource,\n                api_version=api_version,\n                content_type=content_type,\n                cls=lambda x,y,z: x,\n                **kwargs\n            )\n        kwargs.pop('error_map', None)\n\n        def get_long_running_output(pipeline_response):\n            response = pipeline_response.http_response\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n            if cls:\n                return cls(pipeline_response, deserialized, {})\n            return deserialized\n\n\n        if polling is True: polling_method = AsyncARMPolling(lro_delay, lro_options={'final-state-via': 'azure-async-operation'}, **kwargs)\n        elif polling is False: polling_method = AsyncNoPolling()\n        else: polling_method = polling\n        if cont_token:\n            return AsyncLROPoller.from_continuation_token(\n                polling_method=polling_method,\n                continuation_token=cont_token,\n                client=self._client,\n                deserialization_callback=get_long_running_output\n            )\n        return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)\n\n    begin_update_put.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.AppPlatform/Spring/{serviceName}/monitoringSettings/default\"}  # type: ignore\n\n    async def _update_patch_initial(\n        self,\n        resource_group_name: str,\n        service_name: str,\n        monitoring_setting_resource: \"_models.MonitoringSettingResource\",\n        **kwargs: Any\n    ) -\u003e \"_models.MonitoringSettingResource\":\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.MonitoringSettingResource\"]\n        error_map = {\n            401: ClientAuthenticationError, 404: ResourceNotFoundError, 409: ResourceExistsError\n        }\n        error_map.update(kwargs.pop('error_map', {}))\n\n        api_version = kwargs.pop('api_version', \"2022-03-01-preview\")  # type: str\n        content_type = kwargs.pop('content_type', \"application/json\")  # type: Optional[str]\n\n        _json = self._serialize.body(monitoring_setting_resource, 'MonitoringSettingResource')\n\n        request = build_update_patch_request_initial(\n            subscription_id=self._config.subscription_id,\n            resource_group_name=resource_group_name,\n            service_name=service_name,\n            api_version=api_version,\n            content_type=content_type,\n            json=_json,\n            template_url=self._update_patch_initial.metadata['url'],\n        )\n        request = _convert_request(request)\n        request.url = self._client.format_url(request.url)\n\n        pipeline_response = await self._client._pipeline.run(  # pylint: disable=protected-access\n            request,\n            stream=False,\n            **kwargs\n        )\n        response = pipeline_response.http_response\n\n        if response.status_code not in [200, 202]:\n            map_error(status_code=response.status_code, response=response, error_map=error_map)\n            raise HttpResponseError(response=response, error_format=ARMErrorFormat)\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n\n        if response.status_code == 202:\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n\n        if cls:\n            return cls(pipeline_response, deserialized, {})\n\n        return deserialized\n\n    _update_patch_initial.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.AppPlatform/Spring/{serviceName}/monitoringSettings/default\"}  # type: ignore\n\n\n    @distributed_trace_async\n    async def begin_update_patch(\n        self,\n        resource_group_name: str,\n        service_name: str,\n        monitoring_setting_resource: \"_models.MonitoringSettingResource\",\n        **kwargs: Any\n    ) -\u003e AsyncLROPoller[\"_models.MonitoringSettingResource\"]:\n        \"\"\"Update the Monitoring Setting.\n\n        :param resource_group_name: The name of the resource group that contains the resource. You can\n         obtain this value from the Azure Resource Manager API or the portal.\n        :type resource_group_name: str\n        :param service_name: The name of the Service resource.\n        :type service_name: str\n        :param monitoring_setting_resource: Parameters for the update operation.\n        :type monitoring_setting_resource:\n         ~azure.mgmt.appplatform.v2022_03_01_preview.models.MonitoringSettingResource\n        :keyword callable cls: A custom type or function that will be passed the direct response\n        :keyword str continuation_token: A continuation token to restart a poller from a saved state.\n        :keyword polling: By default, your polling method will be AsyncARMPolling. Pass in False for\n         this operation to not poll, or pass in your own initialized polling object for a personal\n         polling strategy.\n        :paramtype polling: bool or ~azure.core.polling.AsyncPollingMethod\n        :keyword int polling_interval: Default waiting time between two polls for LRO operations if no\n         Retry-After header is present.\n        :return: An instance of AsyncLROPoller that returns either MonitoringSettingResource or the\n         result of cls(response)\n        :rtype:\n         ~azure.core.polling.AsyncLROPoller[~azure.mgmt.appplatform.v2022_03_01_preview.models.MonitoringSettingResource]\n        :raises: ~azure.core.exceptions.HttpResponseError\n        \"\"\"\n        api_version = kwargs.pop('api_version', \"2022-03-01-preview\")  # type: str\n        content_type = kwargs.pop('content_type', \"application/json\")  # type: Optional[str]\n        polling = kwargs.pop('polling', True)  # type: Union[bool, AsyncPollingMethod]\n        cls = kwargs.pop('cls', None)  # type: ClsType[\"_models.MonitoringSettingResource\"]\n        lro_delay = kwargs.pop(\n            'polling_interval',\n            self._config.polling_interval\n        )\n        cont_token = kwargs.pop('continuation_token', None)  # type: Optional[str]\n        if cont_token is None:\n            raw_result = await self._update_patch_initial(\n                resource_group_name=resource_group_name,\n                service_name=service_name,\n                monitoring_setting_resource=monitoring_setting_resource,\n                api_version=api_version,\n                content_type=content_type,\n                cls=lambda x,y,z: x,\n                **kwargs\n            )\n        kwargs.pop('error_map', None)\n\n        def get_long_running_output(pipeline_response):\n            response = pipeline_response.http_response\n            deserialized = self._deserialize('MonitoringSettingResource', pipeline_response)\n            if cls:\n                return cls(pipeline_response, deserialized, {})\n            return deserialized\n\n\n        if polling is True: polling_method = AsyncARMPolling(lro_delay, lro_options={'final-state-via': 'azure-async-operation'}, **kwargs)\n        elif polling is False: polling_method = AsyncNoPolling()\n        else: polling_method = polling\n        if cont_token:\n            return AsyncLROPoller.from_continuation_token(\n                polling_method=polling_method,\n                continuation_token=cont_token,\n                client=self._client,\n                deserialization_callback=get_long_running_output\n            )\n        return AsyncLROPoller(self._client, raw_result, get_long_running_output, polling_method)\n\n    begin_update_patch.metadata = {'url': \"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.AppPlatform/Spring/{serviceName}/monitoringSettings/default\"}  # type: ignore\n"
}
{
    "repo_name": "fin/froide",
    "ref": "refs/heads/main",
    "path": "froide/document/migrations/0016_auto_20190806_2121.py",
    "copies": "1",
    "content": "# Generated by Django 2.1.9 on 2019-08-06 19:21\n\nfrom django.db import migrations, models\nimport taggit.managers\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        (\"document\", \"0015_auto_20180810_1910\"),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name=\"document\",\n            name=\"file_size\",\n            field=models.BigIntegerField(null=True),\n        ),\n        migrations.AlterField(\n            model_name=\"document\",\n            name=\"language\",\n            field=models.CharField(\n                blank=True,\n                choices=[(\"de\", \"German\"), (\"en\", \"English\")],\n                default=\"de\",\n                max_length=10,\n            ),\n        ),\n        migrations.AlterField(\n            model_name=\"document\",\n            name=\"tags\",\n            field=taggit.managers.TaggableManager(\n                blank=True,\n                help_text=\"A comma-separated list of tags.\",\n                related_name=\"+\",\n                through=\"filingcabinet.TaggedDocument\",\n                to=\"taggit.Tag\",\n                verbose_name=\"Tags\",\n            ),\n        ),\n    ]\n"
}
{
    "repo_name": "pyrolysis/kinetic-schemes",
    "ref": "refs/heads/main",
    "path": "functions/ranzi.py",
    "copies": "2",
    "content": "\"\"\"\nFunctions for Ranzi 2014 kinetic reaction schemes for biomass pyrolysis. See\ncomments in each function for more details.\n\nReference:\nRanzi, Corbetta, Pierucci, 2014. Chemical Engineering Science, 110, pp 2-12.\n\"\"\"\n\nimport numpy as np\n\ndef ranzicell(rhow, wt, T, dt, nt):\n    \"\"\"\n    Cellulose reactions CELL from Ranzi 2014 paper for biomass pyrolysis.\n\n    Parameters\n    ----------\n    rhow = wood density, kg/m^3\n    wt = weight percent of cellulose, %\n    T = temperature, K\n    dt = time step, s\n    nt = total number of time steps\n\n    Returns\n    -------\n    main/rhow = mass fraction of main group, (-)\n    prod/rhow = mass fraction of product group, (-)\n    \"\"\"\n\n    # vector for initial wood concentration, kg/m^3\n    pw = np.ones(nt)*rhow\n\n    # vectors to store main product concentrations, kg/m^3\n    cell = pw*(wt/100)      # initial cellulose conc. in wood\n    g1 = np.zeros(nt)       # G1\n    cella = np.zeros(nt)    # CELLA\n    lvg = np.zeros(nt)      # LVG\n    g4 = np.zeros(nt)       # G4\n\n    R = 1.987   # universal gas constant, kcal/kmol*K\n\n    # reaction rate constant for each reaction, 1/s\n    # A = pre-factor (1/s) and E = activation energy (kcal/kmol)\n    K1 = 4e7 * np.exp(-31000 / (R * T))         # CELL -\u003e G1\n    K2 = 4e13 * np.exp(-45000 / (R * T))        # CELL -\u003e CELLA\n    K3 = 1.8 * T * np.exp(-10000 / (R * T))     # CELLA -\u003e LVG\n    K4 = 0.5e9 * np.exp(-29000 / (R * T))       # CELLA -\u003e G4\n\n    # sum of moles in each group, mol\n    sumg1 = 11      # sum of G1\n    sumg4 = 4.08    # sum of G4\n\n    # calculate concentrations for main groups, kg/m^3\n    for i in range(1, nt):\n        r1 = K1 * cell[i-1]     # CELL -\u003e G1\n        r2 = K2 * cell[i-1]     # CELL -\u003e CELLA\n        r3 = K3 * cella[i-1]    # CELLA -\u003e LVG\n        r4 = K4 * cella[i-1]    # CELLA -\u003e G4\n        cell[i] = cell[i-1] - (r1+r2)*dt            # CELL\n        g1[i] = g1[i-1] + r1*dt                     # G1\n        cella[i] = cella[i-1] + r2*dt - (r3+r4)*dt  # CELLA\n        lvg[i] = lvg[i-1] + r3*dt                   # LVG\n        g4[i] = g4[i-1] + r4*dt                     # G4\n\n    # store main groups in array\n    main = np.array([cell, g1, cella, lvg, g4])\n\n    # total group concentration per total moles in that group, (kg/m^3) / mol\n    fg1 = g1/sumg1  # fraction of G1\n    fg4 = g4/sumg4  # fraction of G4\n\n    # array to store product concentrations as a density, kg/m^3\n    prod = np.zeros([21, nt])\n    prod[0] = 0.16*fg4              # CO\n    prod[1] = 0.21*fg4              # CO2\n    prod[2] = 0.4*fg4               # CH2O\n    prod[3] = 0.02*fg4              # HCOOH\n    prod[5] = 0.1*fg4               # CH4\n    prod[6] = 0.2*fg4               # Glyox\n    prod[8] = 0.1*fg4               # C2H4O\n    prod[9] = 0.8*fg4               # HAA\n    prod[11] = 0.3*fg4              # C3H6O\n    prod[14] = 0.25*fg4             # HMFU\n    prod[15] = lvg                  # LVG\n    prod[18] = 0.1*fg4              # H2\n    prod[19] = 5*fg1 + 0.83*fg4     # H2O\n    prod[20] = 6*fg1 + 0.61*fg4     # Char\n\n    # return arrays of main groups and products as mass fraction, (-)\n    return main/rhow, prod/rhow\n\n\ndef ranzihemi(rhow, wt, T, dt, nt):\n    \"\"\"\n    Hemicellulose reactions HCE from Ranzi 2014 paper for biomass pyrolysis.\n\n    Parameters\n    ----------\n    rhow = wood density, kg/m^3\n    wt = weight percent of hemicellulose, %\n    T = temperature, K\n    dt = time step, s\n    nt = total number of time steps\n\n    Returns\n    -------\n    main/rhow = mass fraction of main group, (-)\n    prod/rhow = mass fraction of product group, (-)\n    \"\"\"\n\n    # vector for initial wood concentration, kg/m^3\n    pw = np.ones(nt)*rhow\n\n    # vectors to store main product concentrations, kg/m^3\n    hce = pw*(wt/100)   # initial hemicellulose conc. in wood\n    g1 = np.zeros(nt)   # G1\n    g2 = np.zeros(nt)   # G2\n    g3 = np.zeros(nt)   # G3\n    g4 = np.zeros(nt)   # G4\n    xyl = np.zeros(nt)  # Xylan\n\n    R = 1.987   # universal gas constant, kcal/kmol*K\n\n    # reaction rate constant for each reaction, 1/s\n    # A = pre-factor (1/s) and E = activation energy (kcal/kmol)\n    K1 = 0.33e10 * np.exp(-31000 / (R * T))     # HCE -\u003e G1\n    K2 = 0.33e10 * np.exp(-33000 / (R * T))     # HCE2 -\u003e G2\n    K3 = 0.05 * T * np.exp(-8000 / (R * T))     # HCE1 -\u003e G3\n    K4 = 1e9 * np.exp(-32000 / (R * T))         # HCE1 -\u003e G4\n    K5 = 0.9 * T * np.exp(-11000 / (R * T))     # HCE1 -\u003e Xylan\n\n    # sum of moles in each group, mol\n    sumg2 = 4.625   # sum of G2\n    sumg3 = 4.875   # sum of G3\n    sumg4 = 4.775   # sum of G4\n\n    # calculate concentrations for main groups, kg/m^3\n    # where HCE1 as 0.4*g1/(0.4+0.6) and HCE2 as 0.6*g1/(0.4+0.6)\n    for i in range(1, nt):\n        r1 = K1 * hce[i-1]      # HCE -\u003e G1\n        r2 = K2 * 0.6*g1[i-1]   # HCE2 -\u003e G2\n        r3 = K3 * 0.4*g1[i-1]   # HCE1 -\u003e G3\n        r4 = K4 * 0.4*g1[i-1]   # HCE1 -\u003e G4\n        r5 = K5 * 0.4*g1[i-1]   # HCE1 -\u003e Xylan\n        hce[i] = hce[i-1] - r1*dt                   # HCE\n        g1[i] = g1[i-1] + r1*dt - (r2+r3+r4+r5)*dt  # G1\n        g2[i] = g2[i-1] + r2*dt                     # G2\n        g3[i] = g3[i-1] + r3*dt                     # G3\n        g4[i] = g4[i-1] + r4*dt                     # G4\n        xyl[i] = xyl[i-1] + r5*dt                   # Xylan\n\n    # store main groups in array\n    main = np.array([hce, g1, g2, g3, g4, xyl])\n\n    # total group concentration per total moles in that group, (kg/m^3)/mol\n    fg2 = g2/sumg2  # fraction of G2\n    fg3 = g3/sumg3  # fraction of G3\n    fg4 = g4/sumg4  # fraction of G4\n\n    # array to store product concentrations as a density, kg/m^3\n    prod = np.zeros([21, nt])\n    prod[0] = 0.175*fg2 + (0.3 + 0.15)*fg3 + 0.5*fg4                # CO\n    prod[1] = (0.275+0.4)*fg2 + (0.5+0.25)*fg3 + (0.5+0.275)*fg4    # CO2\n    prod[2] = (0.5+0.925)*fg2 + 1.7*fg3 + (0.8+0.4)*fg4             # CH2O\n    prod[3] = 0.025*fg2 + 0.05*fg3 + 0.025*fg4                      # HCOOH\n    prod[4] = 0.3*fg2 + (0.1+0.45)*fg4                              # CH3OH\n    prod[5] = 0.25*fg2 + 0.625*fg3 + 0.325*fg4                      # CH4\n    prod[7] = 0.275*fg2 + 0.375*fg3 + 0.25*fg4                      # C2H4\n    prod[9] = 0.2*fg2                                               # HAA\n    prod[10] = 0.1*fg2 + 0.125*fg4                                  # C2H5OH\n    prod[12] = xyl                                                  # Xylan\n    prod[18] = 0.125*fg4                                            # H2\n    prod[19] = 0.2*fg2 + 0.25*fg3 + 0.025*fg4                       # H2O\n    prod[20] = 1*fg2 + 0.675*fg3 + 0.875*fg4                        # Char\n\n    # return arrays of main groups and products as mass fraction, (-)\n    return main/rhow, prod/rhow\n\n\ndef ranziligc(rhow, wt, T, dt, nt):\n    \"\"\"\n    Lignin carbon rich reactions LIG-C from Ranzi 2014 paper for biomass pyrolysis.\n\n    Parameters\n    ----------\n    rhow = wood density, kg/m^3\n    wt = weight percent of lignin-c, %\n    T = temperature, K\n    dt = time step, s\n    nt = total number of time steps\n\n    Returns\n    -------\n    main/rhow = mass fraction of main group, (-)\n    prod/rhow = mass fraction of product group, (-)\n    \"\"\"\n\n    # vector for initial wood concentration, kg/m^3\n    pw = np.ones(nt)*rhow\n\n    # vectors to store main product concentrations, kg/m^3\n    ligc = pw*(wt/100/3)    # initial lignin in wood, assume 1/3 of total lignin\n    g1 = np.zeros(nt)\n    g2 = np.zeros(nt)\n\n    R = 1.987       # universal gas constant, kcal/kmol*K\n\n    # reaction rate constant for each reaction, 1/s\n    # A = pre-factor (1/s) and E = activation energy (kcal/kmol)\n    K1 = 1.33e15 * np.exp(-48500 / (R * T))     # LIG-C -\u003e G1\n    K2 = 1.6e6 * np.exp(-31500 / (R * T))       # LIG-CC -\u003e G2\n\n    # sum of moles in each group, mol\n    sumg1 = 9.49    # sum of G1\n    sumg2 = 11.35   # sum of G2\n\n    # calculate concentrations for main groups, kg/m^3\n    for i in range(1, nt):\n        r1 = K1 * ligc[i-1]             # LIG-C -\u003e G1\n        r2 = K2 * 0.35*g1[i-1]/sumg1    # LIG-CC -\u003e G2\n        ligc[i] = ligc[i-1] - r1*dt         # LIG-C\n        g1[i] = g1[i-1] + r1*dt - r2*dt     # G1\n        g2[i] = g2[i-1] + r2*dt             # G2\n\n    # store main groups in array\n    main = np.array([ligc, g1, g2])\n\n    # total group concentration per total moles in that group, (kg/m^3)/mol\n    fg1 = g1/sumg1  # fraction of G1\n    fg2 = g2/sumg2  # fraction of G2\n\n    # array to store product concentrations as a density, kg/m^3\n    prod = np.zeros([21, nt])\n    prod[0] = 0.32*fg1 + (0.4 + 0.4)*fg2    # CO\n    prod[2] = (0.3 + 0.7)*fg1 + 1*fg2       # CH2O\n    prod[5] = 0.495*fg1 + 0.65*fg2          # CH4\n    prod[7] = 0.41*fg1 + 0.6*fg2            # C2H4\n    prod[9] = 0.35*fg2                      # HAA\n    prod[13] = 0.08*fg1 + 0.2*fg2           # Phenol\n    prod[16] = 0.1*fg1 + 0.3*fg2            # Coumaryl\n    prod[19] = 1*fg1 + 0.7*fg2              # H2O\n    prod[20] = 5.735*fg1 + 6.75*fg2         # Char\n\n    # return arrays of main groups and products as mass fractions, (-)\n    return main/rhow, prod/rhow\n\n\ndef ranziligh(rhow, wt, T, dt, nt):\n    \"\"\"\n    Lignin hydrogen rich reactions LIG-H from Ranzi 2014 paper for biomass pyrolysis.\n\n    Parameters\n    ----------\n    rhow = wood density, kg/m^3\n    wt = weight percent of lignin-h, %\n    T = temperature, K\n    dt = time step, s\n    nt = total number of time steps\n\n    Returns\n    -------\n    main/rhow = mass fraction of main group, (-)\n    prod/rhow = mass fraction of product group, (-)\n    \"\"\"\n\n    # vector for initial wood concentration, kg/m^3\n    pw = np.ones(nt)*rhow\n\n    # vectors to store main product concentrations, kg/m^3\n    ligh = pw*(wt/100/3)    # initial lignin in wood, assume 1/3 of total lignin\n    g1 = np.zeros(nt)       # G1\n    g2 = np.zeros(nt)       # G2\n    g3 = np.zeros(nt)       # G3\n    g4 = np.zeros(nt)       # G4\n    g5 = np.zeros(nt)       # G4\n    fe2macr = np.zeros(nt)  # FE2MACR\n\n    R = 1.987       # universal gas constant, kcal/kmol*K\n\n    # reaction rate constant for each reaction, 1/s\n    # A = pre-factor (1/s) and E = activation energy (kcal/kmol)\n    K1 = 0.67e13 * np.exp(-37500 / (R * T))     # LIG-H -\u003e G1\n    K2 = 33 * np.exp(-15000 / (R * T))          # LIG-OH -\u003e G2\n    K3 = 0.5e8 * np.exp(-30000 / (R * T))       # LIG-OH -\u003e LIG\n    K4 = 0.083 * T * np.exp(-8000 / (R * T))    # LIG -\u003e G4\n    K5 = 0.4e9 * np.exp(-30000 / (R * T))       # LIG -\u003e G5\n    K6 = 2.4 * T * np.exp(-12000 / (R * T))     # LIG -\u003e FE2MACR\n\n    # sum of moles in each group, mol\n    sumg1 = 2       # sum of G1\n    sumg2 = 20.7    # sum of G2\n    sumg3 = 9.85    # sum of G3\n    sumg4 = 11.1    # sum of G4\n    sumg5 = 10.7    # sum of G5\n\n    # calculate concentrations for main groups, kg/m^3\n    for i in range(1, nt):\n        r1 = K1 * ligh[i-1]             # LIG-H -\u003e G1\n        r2 = K2 * 1*g1[i-1]/sumg1       # LIG-OH -\u003e G2\n        r3 = K3 * 1*g1[i-1]/sumg1       # LIG-OH -\u003e LIG\n        r4 = K4 * 1*g3[i-1]/sumg3       # LIG -\u003e G4\n        r5 = K5 * 1*g3[i-1]/sumg3       # LIG -\u003e G5\n        r6 = K6 * 1*g3[i-1]/sumg3       # LIG -\u003e FE2MACR\n        ligh[i] = ligh[i-1] - r1*dt                 # LIG-H\n        g1[i] = g1[i-1] + r1*dt - (r2+r3)*dt        # G1\n        g2[i] = g2[i-1] + r2*dt                     # G2\n        g3[i] = g3[i-1] + r3*dt - (r4+r5+r6)*dt     # G3\n        g4[i] = g4[i-1] + r4*dt                     # G4\n        g5[i] = g5[i-1] + r5*dt                     # G5\n        fe2macr[i] = fe2macr[i-1] + r6*dt           # FE2MACR\n\n    # store main groups in array\n    main = np.array([ligh, g1, g2, g3, g4, g5, fe2macr])\n\n    # total group concentration per total moles in that group, (kg/m^3)/mol\n    fg1 = g1/sumg1  # fraction of G1\n    fg2 = g2/sumg2  # fraction of G2\n    fg3 = g3/sumg3  # fraction of G3\n    fg4 = g4/sumg4  # fraction of G4\n    fg5 = g5/sumg5  # fraction of G5\n\n    # array to store product concentrations as a density, kg/m^3\n    prod = np.zeros([21, nt])\n    prod[0] = (0.5 + 1.6)*fg2 + (0.3 + 1)*fg3 + (0.4 + 0.2)*fg4 + (1 + 0.45)*fg5 # CO\n    prod[1] = 0.05*fg3                                              # CO2\n    prod[2] = 3.9*fg2 + 0.6*fg3 + (2 + 0.4)*fg4 + (0.2 + 0.5)*fg5   # CH2O\n    prod[3] = 0.05*fg3 + 0.05*fg5                                   # HCOOH\n    prod[4] = 0.5*fg2 + (0.5 + 0.5)*fg3 + 0.4*fg4 + 0.4*fg5         # CH3OH\n    prod[5] = (0.1 + 1.65)*fg2 + (0.1 + 0.35)*fg3 + (0.2 + 0.4)*fg4 + (0.2 + 0.4)*fg5 # CH4\n    prod[6] = 0                                                     # Glyox\n    prod[7] = 0.3*fg2 + 0.2*fg3 + 0.5*fg4 + 0.65*fg5                # C2H4\n    prod[8] = 0.2*fg5                                               # C2H4O\n    prod[9] = 0                                                     # HAA\n    prod[10] = 0                                                    # C2H5OH\n    prod[11] = 1*fg1 + 0.2*fg5                                      # C3H6O\n    prod[12] = 0                                                    # Xylan\n    prod[13] = 0                                                    # Phenol\n    prod[14] = 0                                                    # HMFU\n    prod[15] = 0                                                    # LVG\n    prod[16] = 0                                                    # Coumaryl\n    prod[17] = fe2macr                                              # FE2MACR\n    prod[18] = 0.5*fg2 + 0.15*fg3                                   # H2\n    prod[19] = 1.5*fg2 + 0.9*fg3 + 0.6*fg4 + 0.95*fg5               # H2O\n    prod[20] = 10.15*fg2 + 4.15*fg3 + 6*fg4 + 5.5*fg5               # Char\n\n    # return arrays of main groups and products as mass fractions, (-)\n    return main/rhow, prod/rhow\n\n\ndef ranziligo(rhow, wt, T, dt, nt):\n    \"\"\"\n    Lignin oxygen rich reactions LIG-O from Ranzi 2014 paper for biomass pyrolysis.\n\n    Parameters\n    ----------\n    rhow = wood density, kg/m^3\n    wt = weight percent of lignin-h, %\n    T = temperature, K\n    dt = time step, s\n    nt = total number of time steps\n\n    Returns\n    -------\n    main/rhow = mass fraction of main group, (-)\n    prod/rhow = mass fraction of product group, (-)\n    \"\"\"\n\n    # vector for initial wood concentration, kg/m^3\n    pw = np.ones(nt)*rhow\n\n    # vectors to store main product concentrations, kg/m^3\n    ligo = pw*(wt/100/3)    # initial lignin in wood, assume 1/3 of total lignin\n    g1 = np.zeros(nt)       # G1\n    g2 = np.zeros(nt)       # G2\n    g3 = np.zeros(nt)       # G3\n    g4 = np.zeros(nt)       # G4\n    g5 = np.zeros(nt)       # G4\n    fe2macr = np.zeros(nt)  # FE2MACR\n\n    R = 1.987       # universal gas constant, kcal/kmol*K\n\n    # reaction rate constant for each reaction, 1/s\n    # A = pre-factor (1/s) and E = activation energy (kcal/kmol)\n    K1 = 0.33e9 * np.exp(-25500 / (R * T))      # LIG-O -\u003e G1\n    K2 = 33 * np.exp(-15000 / (R * T))          # LIG-OH -\u003e G2\n    K3 = 0.5e8 * np.exp(-30000 / (R * T))       # LIG-OH -\u003e LIG\n    K4 = 0.083 * T * np.exp(-8000 / (R * T))    # LIG -\u003e G4\n    K5 = 0.4e9 * np.exp(-30000 / (R * T))       # LIG -\u003e G5\n    K6 = 2.4 * T * np.exp(-12000 / (R * T))     # LIG -\u003e FE2MACR\n\n    # sum of moles in each group, mol\n    sumg1 = 2       # sum of G1\n    sumg2 = 20.7    # sum of G2\n    sumg3 = 9.85    # sum of G3\n    sumg4 = 11.1    # sum of G4\n    sumg5 = 10.7    # sum of G5\n\n    # calculate concentrations for main groups, kg/m^3\n    for i in range(1, nt):\n        r1 = K1 * ligo[i-1]             # LIG-O -\u003e G1\n        r2 = K2 * 1*g1[i-1]/sumg1       # LIG-OH -\u003e G2\n        r3 = K3 * 1*g1[i-1]/sumg1       # LIG-OH -\u003e LIG\n        r4 = K4 * 1*g3[i-1]/sumg3       # LIG -\u003e G4\n        r5 = K5 * 1*g3[i-1]/sumg3       # LIG -\u003e G5\n        r6 = K6 * 1*g3[i-1]/sumg3       # LIG -\u003e FE2MACR\n        ligo[i] = ligo[i-1] - r1*dt                 # LIG-H\n        g1[i] = g1[i-1] + r1*dt - (r2+r3)*dt        # G1\n        g2[i] = g2[i-1] + r2*dt                     # G2\n        g3[i] = g3[i-1] + r3*dt - (r4+r5+r6)*dt     # G3\n        g4[i] = g4[i-1] + r4*dt                     # G4\n        g5[i] = g5[i-1] + r5*dt                     # G5\n        fe2macr[i] = fe2macr[i-1] + r6*dt           # FE2MACR\n\n    # store main groups in array\n    main = np.array([ligo, g1, g2, g3, g4, g5, fe2macr])\n\n    # total group concentration per total moles in that group, (kg/m^3)/mol\n    fg1 = g1/sumg1  # fraction of G1\n    fg2 = g2/sumg2  # fraction of G2\n    fg3 = g3/sumg3  # fraction of G3\n    fg4 = g4/sumg4  # fraction of G4\n    fg5 = g5/sumg5  # fraction of G5\n\n    # array to store product concentrations as a density, kg/m^3\n    prod = np.zeros([21, nt])\n    prod[0] = (0.5 + 1.6)*fg2 + (0.3 + 1)*fg3 + (0.4 + 0.2)*fg4 + (1 + 0.45)*fg5 # CO\n    prod[1] = 1*fg1 + 0.05*fg3                                      # CO2\n    prod[2] = 3.9*fg2 + 0.6*fg3 + (2 + 0.4)*fg4 + (0.2 + 0.5)*fg5   # CH2O\n    prod[3] = 0.05*fg3 + 0.05*fg5                                   # HCOOH\n    prod[4] = 0.5*fg2 + (0.5 + 0.5)*fg3 + 0.4*fg4 + 0.4*fg5         # CH3OH\n    prod[5] = (0.1 + 1.65)*fg2 + (0.1 + 0.35)*fg3 + (0.2 + 0.4)*fg4 + (0.2 + 0.4)*fg5 # CH4\n    prod[6] = 0                                                     # Glyox\n    prod[7] = 0.3*fg2 + 0.2*fg3 + 0.5*fg4 + 0.65*fg5                # C2H4\n    prod[8] = 0.2*fg5                                               # C2H4O\n    prod[9] = 0                                                     # HAA\n    prod[10] = 0                                                    # C2H5OH\n    prod[11] = 0.2*fg5                                              # C3H6O\n    prod[12] = 0                                                    # Xylan\n    prod[13] = 0                                                    # Phenol\n    prod[14] = 0                                                    # HMFU\n    prod[15] = 0                                                    # LVG\n    prod[16] = 0                                                    # Coumaryl\n    prod[17] = fe2macr                                              # FE2MACR\n    prod[18] = 0.5*fg2 + 0.15*fg3                                   # H2\n    prod[19] = 1.5*fg2 + 0.9*fg3 + 0.6*fg4 + 0.95*fg5               # H2O\n    prod[20] = 10.15*fg2 + 4.15*fg3 + 6*fg4 + 5.5*fg5               # Char\n\n    # return arrays of main groups and products as mass fractions, (-)\n    return main/rhow, prod/rhow\n"
}
{
    "repo_name": "egenerat/flight-manager",
    "ref": "refs/heads/main",
    "path": "django/contrib/gis/db/backends/oracle/introspection.py",
    "copies": "12",
    "content": "import cx_Oracle\r\nfrom django.db.backends.oracle.introspection import DatabaseIntrospection\r\n\r\nclass OracleIntrospection(DatabaseIntrospection):\r\n    # Associating any OBJECTVAR instances with GeometryField.  Of course,\r\n    # this won't work right on Oracle objects that aren't MDSYS.SDO_GEOMETRY,\r\n    # but it is the only object type supported within Django anyways.\r\n    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()\r\n    data_types_reverse[cx_Oracle.OBJECT] = 'GeometryField'\r\n\r\n    def get_geometry_type(self, table_name, geo_col):\r\n        cursor = self.connection.cursor()\r\n        try:\r\n            # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.\r\n            try:\r\n                cursor.execute('SELECT \"DIMINFO\", \"SRID\" FROM \"USER_SDO_GEOM_METADATA\" WHERE \"TABLE_NAME\"=%s AND \"COLUMN_NAME\"=%s',\r\n                               (table_name.upper(), geo_col.upper()))\r\n                row = cursor.fetchone()\r\n            except Exception, msg:\r\n                raise Exception('Could not find entry in USER_SDO_GEOM_METADATA corresponding to \"%s\".\"%s\"\\n'\r\n                                'Error message: %s.' % (table_name, geo_col, msg))\r\n\r\n            # TODO: Research way to find a more specific geometry field type for\r\n            # the column's contents.\r\n            field_type = 'GeometryField'\r\n\r\n            # Getting the field parameters.\r\n            field_params = {}\r\n            dim, srid = row\r\n            if srid != 4326:\r\n                field_params['srid'] = srid\r\n            # Length of object array ( SDO_DIM_ARRAY ) is number of dimensions.\r\n            dim = len(dim)\r\n            if dim != 2:\r\n                field_params['dim'] = dim\r\n        finally:\r\n            cursor.close()\r\n\r\n        return field_type, field_params\r\n"
}
{
    "repo_name": "coolshop-com/AltaPay",
    "ref": "refs/heads/master",
    "path": "altapay/transaction.py",
    "copies": "1",
    "content": "from __future__ import absolute_import, unicode_literals\n\nimport altapay.callback\nimport altapay.chargeback\nfrom altapay import exceptions\nfrom altapay.resource import Resource\n\n\nclass Transaction(Resource):\n    @classmethod\n    def find(cls, transaction_id, api):\n        \"\"\"\n        Find exactly one transaction by a transaction ID.\n\n        :arg transaction_id: ID of the transaction in AltaPay\n        :arg api: An API object which will be used for AltaPay communication.\n\n        :rtype: :py:class:`altapay.Transaction`\n        \"\"\"\n        response = api.post(\n            'API/payments', data={'transaction_id': transaction_id}\n        )['APIResponse']\n\n        try:\n            transaction = response['Body']['Transactions']['Transaction']\n        except KeyError:\n            raise exceptions.ResourceNotFoundError(\n                'No Transaction found matching transaction ID: {}'.format(\n                    transaction_id))\n\n        if isinstance(transaction, list):\n            raise exceptions.MultipleResourcesError(\n                'More than one Payment was found. Total found is: {}'.format(\n                    len(response['Body']['Transactions'])))\n\n        return cls(\n            response['@version'], response['Header'], transaction, api=api)\n\n    def capture(self, **kwargs):\n        \"\"\"\n        Capture a reservation on a transaction.\n\n        :arg kwargs: used for optional capture parameters, see the\n            AltaPay documentation for a full list.\n            Note that you will need to use lists and dictionaries to map the\n            URL structures from the AltaPay documentation into these kwargs.\n\n        :rtype: :py:class:`altapay.Callback` object.\n        \"\"\"\n        parameters = {\n            'transaction_id': self.transaction_id\n        }\n\n        parameters.update(kwargs)\n\n        response = self.api.post(\n            'API/captureReservation', data=parameters)['APIResponse']\n\n        return altapay.callback.Callback.from_xml_callback(response)\n\n    def refund(self, **kwargs):\n        \"\"\"\n        Refund full or partial payments on an order.\n\n        :arg kwargs: used for optional refund parameters, see the\n            AltaPay documentation for a full list.\n            Note that you will need to use lists and dictionaries to map the\n            URL structures from the AltaPay documentation into these kwargs.\n\n        :rtype: :py:class:`altapay.Callback` object.\n        \"\"\"\n        parameters = {\n            'transaction_id': self.transaction_id\n        }\n\n        parameters.update(kwargs)\n\n        response = self.api.post(\n            'API/refundCapturedReservation', data=parameters)['APIResponse']\n\n        return altapay.callback.Callback.from_xml_callback(response)\n\n    def charge_subscription(self, **kwargs):\n        \"\"\"\n        This will charge a subscription using a capture. Can be called many\n        times on a subscription.\n\n        If amount is not sent as an optinal parameter, the amount specified in\n        the original setup of the subscription will be used.\n\n        :arg kwargs: used for optional charge subscription parameters,\n            see the AltaPay documentation for a full list.\n            Note that you will need to use lists and dictionaries to map the\n            URL structures from the AltaPay documentation into these kwargs.\n\n        :rtype: :py:class:`altapay.Callback` object.\n        \"\"\"\n        parameters = {\n            'transaction_id': self.transaction_id\n        }\n\n        parameters.update(kwargs)\n\n        response = self.api.post(\n            'API/chargeSubscription', data=parameters)['APIResponse']\n\n        return altapay.callback.Callback.from_xml_callback(response)\n\n    def reserve_subscription_charge(self, **kwargs):\n        \"\"\"\n        This will create a reservation on a subscription. Can be called many\n        times on a subscription.\n\n        If amount is not sent as an optinal parameter, the amount specified in\n        the original setup of the subscription will be used.\n\n        :arg kwargs: used for optional reserve subscription parameters,\n            see the AltaPay documentation for a full list.\n            Note that you will need to use lists and dictionaries to map the\n            URL structures from the AltaPay documentation into these kwargs.\n\n        :rtype: :py:class:`altapay.Callback` object.\n        \"\"\"\n        parameters = {\n            'transaction_id': self.transaction_id\n        }\n\n        parameters.update(kwargs)\n\n        response = self.api.post(\n            'API/reserveSubscriptionCharge',\n            data=parameters)['APIResponse']\n\n        return altapay.callback.Callback.from_xml_callback(response)\n\n    def release(self):\n        \"\"\"\n        This will release the reservation on the transaction. This is useful\n        if you for whatever reason do not want to capture the payment.\n\n        Refer to the AltaPay documentation for edge cases surround this method.\n\n        :rtype: :py:class:`altapay.Callback` object.\n        \"\"\"\n        parameters = {\n            'transaction_id': self.transaction_id\n        }\n\n        response = self.api.post(\n            'API/releaseReservation', data=parameters)['APIResponse']\n\n        return altapay.callback.Callback.from_xml_callback(response)\n\n    def chargebacks(self):\n        data = self.chargeback_events['chargeback_event']\n\n        if not isinstance(data, list):\n            data = [data]\n\n        # Use dummy ChargebackEvent objects for making response pythonic\n        return [\n            altapay.chargeback.ChargebackEvent(body=chargeback)\n            for chargeback in data\n        ]\n"
}
{
    "repo_name": "micktwomey/acaversity",
    "ref": "refs/heads/master",
    "path": "acaversity/settings.py",
    "copies": "1",
    "content": "\"\"\"\nDjango settings for acaversity project.\n\nGenerated by 'django-admin startproject' using Django 1.8.2.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.8/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.8/ref/settings/\n\"\"\"\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.8/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '*pnfb)v2u-6kxe3nu=5w*vku9gkf\u00262$^d37\u0026oc\u0026-l@oqs0n71='\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = (\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n)\n\nROOT_URLCONF = 'acaversity.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'acaversity.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.8/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.8/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.8/howto/static-files/\n\nSTATIC_URL = '/static/'\n"
}
{
    "repo_name": "ActiveState/code",
    "ref": "refs/heads/master",
    "path": "recipes/Python/578895_Simultaneous__paned_grids/recipe-578895.py",
    "copies": "1",
    "content": "# Version 0.1\n# Author: Miguel Martinez Lopez\n# Uncomment the next line to see my email\n# print \"Email: \", \"61706c69636163696f6e616d656469646140676d61696c2e636f6d\".decode(\"hex\")\n\n\nfrom Tkinter import *\nfrom collections import defaultdict\nimport itertools\n\n\nclass connected_Panedgrids(object):\n\n\tdef __init__(self, numRows=0, numColumns=0):\n\t\tif type(numRows) is not int:\n\t\t\traise Exception(\"The number of shared rows is not an int.\")\n\n\t\tif type(numColumns) is not int:\n\t\t\traise Exception(\"The number of shared columns is not an int.\")\n\n\t\tself.numberOfSharedRows = numRows\n\t\tself.numberOfSharedColumns = numColumns\n\n\t\tself.panedwindow_conexion = SimultaneousPanels()\n\n\t\tself.__connected_widgets = {}\n\n\tdef create_widget(self, master, shared_rows=[], shared_columns=[], shared_panedwindows=[], **kwargs):\n\t\tpanedgrid_widget = PanedGrid(master, **kwargs)\n\n\t\tself.add_widget(panedgrid_widget, shared_rows, shared_columns, shared_panedwindows)\n\t\treturn panedgrid_widget\n\n\tdef add_widget(self, panedgrid_widget, shared_rows=[], shared_columns=[], shared_panedwindows=[]):\n\n\t\tif shared_rows == 'all':\n\t\t\tshared_rows = range(self.numberOfSharedRows)\n\n\t\tif shared_columns == 'all':\n\t\t\tshared_columns = range(self.numberOfSharedColumns)\n\n\t\twidget_bundle = {\t'panedgrid_widget': panedgrid_widget,\n\t\t\t\t\t\t\t'shared_rows' : shared_rows,\n\t\t\t\t\t\t\t'shared_columns': shared_columns,\n\t\t\t\t\t\t\t'shared_panedwindows': shared_panedwindows\n\t\t\t\t\t\t}\n\n\t\tself.__connected_widgets[str(panedgrid_widget)] = widget_bundle\n\n\tdef delete_widget(self, widget_reference):\n\t\tdel self.__connected_widgets[str(panedgrid_widget)]\n\n\tdef update(self):\n\t\tself.panedwindow_conexion.clear()\n\n\t\theight_list = [0]*self.numberOfSharedRows\n\t\twidth_list = [0]*self.numberOfSharedColumns\n\n\t\tfor bundle_INFO in self.__connected_widgets.values():\n\t\t\tpanedgrid_widget = bundle_INFO['panedgrid_widget']\n\t\t\tshared_panedwindows = bundle_INFO['shared_panedwindows']\n\t\t\tshared_rows = bundle_INFO['shared_rows']\n\t\t\tshared_columns = bundle_INFO['shared_columns']\n\n\t\t\tif not panedgrid_widget.builded:\n\t\t\t\tpanedgrid_widget.build()\n\n\n\t\t\tif shared_panedwindows == 'all':\n\t\t\t\tshared_panedwindows = range(len(panedgrid_widget.panedwindows))\n\n\t\t\tfor conexion_index, panedwindow_index in enumerate(shared_panedwindows):\n\t\t\t\tinternal_panedwindow = panedgrid_widget.panedwindows[panedwindow_index]\n\t\t\t\tself.panedwindow_conexion.add_widget(internal_panedwindow, tag=conexion_index)\n\n\t\t\t\tpane_indices = range(*internal_panedwindow.index_range)\n\n\t\t\t\tif panedgrid_widget.orient == HORIZONTAL:\n\t\t\t\t\tshared_columns.extend(pane_indices)\n\t\t\t\telse:\n\t\t\t\t\tshared_rows.extend(pane_indices)\n\n\n\t\t\tshared_rows = sorted(set(shared_rows))\n\t\t\tshared_columns = sorted(set(shared_columns))\n\n\t\t\tif self.numberOfSharedRows != len(shared_rows):\n\t\t\t\traise Exception(\"The number of shared rows %s is not equal to %s.\" % (len(shared_rows), self.numberOfSharedRows) )\n\t\t\tif self.numberOfSharedColumns != len(shared_columns):\n\t\t\t\traise Exception(\"The number of shared columns %s is not equal to %s.\" % (len(shared_columns), self.numberOfSharedColumns) )\n\n\t\t\tbundle_INFO['shared_rows'] = shared_rows\n\t\t\tbundle_INFO['shared_columns'] = shared_columns\n\n\t\t\tfor position_in_height_list, row_index in enumerate(shared_rows):\n\t\t\t\theight_list[position_in_height_list] = max(\n\t\t\t\t\t\t\t\t\t\t\t\theight_list[position_in_height_list],\n\t\t\t\t\t\t\t\t\t\t\t\tpanedgrid_widget.get_row_height( row_index )\n\t\t\t\t\t\t\t\t\t\t\t\t)\n\n\n\t\t\tfor position_in_width_list, column_index in enumerate(shared_columns):\n\t\t\t\twidth_list[position_in_width_list] = max(\n\t\t\t\t\t\t\t\t\t\t\t\twidth_list[position_in_width_list],\n\t\t\t\t\t\t\t\t\t\t\t\tpanedgrid_widget.get_column_width( column_index )\n\t\t\t\t\t\t\t\t\t\t\t\t)\n\n\t\tfor bundle_INFO in self.__connected_widgets.values():\n\n\t\t\tfor positionInList, rowIndex in enumerate(bundle_INFO['shared_rows']):\n\t\t\t\tbundle_INFO['panedgrid_widget'].rowconfigure(rowIndex, height = height_list[positionInList])\n\n\t\t\tfor positionInList, columnIndex in enumerate(bundle_INFO['shared_columns']):\n\t\t\t\tbundle_INFO['panedgrid_widget'].columnconfigure(columnIndex, width = width_list[positionInList])\n\n\n\nclass SimultaneousPanels(PanedWindow):\n\n \tdef __init__(self):\n\t\tself.collectionOfPanedWindows = {}\n\n\tdef create_widget(self,master, tag= '_default', **kargs):\n\t\twidget = PanedWindow(master, **kargs)\n\t\tself.add_widget(widget,tag)\n\n\t\treturn widget\n\n\tdef add_widget(self, widget, tag):\n\t\twidget.other_paned_windows = []\n\n\t\tif tag in self.collectionOfPanedWindows:\n\t\t\tfor pwindow in self.collectionOfPanedWindows[tag]:\n\t\t\t\twidget.other_paned_windows.append(pwindow)\n\t\t\t\tpwindow.other_paned_windows.append(widget)\n\n\t\t\tself.collectionOfPanedWindows[tag].append(widget)\n\t\telse:\n\t\t\tself.collectionOfPanedWindows[tag] = [widget]\n\n\t\twidget.bindtags( ('SimultaneousPanels',)+ widget.bindtags() )\n\t\twidget.bind_class('SimultaneousPanels', '\u003cButton-1\u003e', self.sash_mark)\n\t\twidget.bind_class('SimultaneousPanels', '\u003cB1-Motion\u003e', self.sash_dragto)\n\n\tdef sash_mark(self,event):\n\t\tthis_widget = event.widget\n\n\t\tidentity = this_widget.identify(event.x, event.y)\n\n\t\tif len(identity) ==2:\n\t\t\tindex = identity[0]\n\t\t\tthis_widget.activedSash=index\n\t\telse:\n\t\t\tthis_widget.activedSash = None\n\n\tdef sash_dragto(self,event):\n\t\tthis_widget = event.widget\n\t\tactivedSash = this_widget.activedSash\n\n\t\tif activedSash != None:\n\t\t\tfor pwindow in this_widget.other_paned_windows:\n\t\t\t\tpwindow.sash_place(activedSash, event.x, event.y)\n\n\tdef clear(self):\n\t\tfor list_of_panels in self.collectionOfPanedWindows.values():\n\t\t\tfor panel in list_of_panels:\n\t\t\t\tdel panel.other_paned_windows\n\t\t\t\tself.delete_bindtag(panel)\n\t\tself.collectionOfPanedWindows = {}\n\n\tdef delete_tag(self, tag):\n\t\tfor widget in self.collectionOfPanedWindows[tag]:\n\t\t\tdel widget.other_paned_windows\n\t\t\tself.delete_bindtag(widget)\n\n\t\tdel self.collectionOfPanedWindows[tag]\n\n\tdef delete_widget(self, widget, tag):\n\t\tfor panel in self.collectionOfPanedWindows[tag]:\n\t\t\tpanel.other_paned_windows.remove(widget)\n\t\tself.delete_bindtag(widget)\n\t\tdel widget.other_paned_windows\n\n\tdef delete_bindtag(self, widget):\n\t\tnew_bindtags = list(widget.bindtags())\n\t\tnew_bindtags.remove('SimultaneousPanels')\n\t\twidget.bindtags(tuple(new_bindtags))\n\n\t\t\nclass PanedGrid(Frame):\n\tdef __init__(self, master, orient=HORIZONTAL, minsize=20, columnWidths = {}, rowHeights={}, fixedPanes = []):\n\t\tFrame.__init__(self,master)\n\n\t\tif orient in (HORIZONTAL, VERTICAL):\n\t\t\tself.orient = orient\n\t\telse:\n\t\t\traise Exception(\"orient must be 'horizontal' or 'vertical, not '%s'.\" % orient)\n\n\t\tif type(minsize) is int:\n\t\t\tself.minsize = minsize\n\t\telse:\n\t\t\traise Exception(\"Minsize must be an integer.\")\n\n\t\tself.fixedPanes = set(fixedPanes)\n\n\t\tself.__packSideBetweenPanes = LEFT if self.orient == HORIZONTAL else TOP\n\t\tself.__packSideInsidePane = TOP if self.orient == HORIZONTAL else LEFT\n\t\tself.__cellFill = X if self.orient == HORIZONTAL else Y\n\n\t\tself.userDefinedWidths = columnWidths\n\t\tself.userDefinedHeights = rowHeights\n\n\t\tself.__panes = {}\n\n\t\tself.builded = False\n\n\t\tself.clear_table()\n\n\n\tdef clear_table(self):\n\t\tif not self.builded:\n\t\t\tself.__gridOfWidgets = {}\n\n\t\t\tself.__rows = defaultdict(set)\n\t\t\tself.__columns = defaultdict(set)\n\n\t\t\tself.__rowHeights = {}\n\t\t\tself.__columnWidths = {}\n\n\t\t\tself.userDefinedWidths = {}\n\t\t\tself.userDefinedHeights = {}\n\n\t\telse:\n\t\t\traise Exception(\"You can't clear the table one this is builded.\")\n\n\tdef rowIndices(self):\n\t\treturn self.__rows.keys()\n\n\tdef columnIndices(self):\n\t\treturn self.__columns.keys()\n\n\tdef numberOfRows(self):\n\t\treturn len(self.__rows)\n\n\tdef numberOfColumns(self):\n\t\treturn len(self.__columns)\n\n\tdef update_cell(self, coordinates, widget):\n\t\tif not self.builded:\n\t\t\traise Exception(\"First you must to build the panedgrid.\")\n\n\t\tif coordinates in self.__gridOfWidgets:\n\t\t\tmaster = self.__gridOfWidgets[coordinates].pack_info()['in']\n\t\t\tself.__gridOfWidgets[coordinates].destroy()\n\t\t\tself.__gridOfWidgets[coordinates] = widget\n\t\t\twidget.pack(in_=master, expand=YES, fill=BOTH)\n\t\telse:\n\t\t\traise Exception(\"There is no widget with coordiantes: %s.\" % coordinates)\n\n\n\tdef build(self):\n\t\tif self.builded:\n\t\t\traise Exception(\"You have just builded the grid before.\")\n\n\t\tif not self.__gridOfWidgets: return\n\n\t\tif self.calculate_dimensions:\n\t\t\tself.__calculate_list_of_cell_widths_and_heights()\n\n\t\tself.panedwindows = []\n\n\t\tlistOfCellCoordinates = list(itertools.product(self.__rows, self.__columns))\n\n\t\tif self.orient == HORIZONTAL:\n\t\t\tlistOfCellCoordinates.sort(key = lambda item: (item[1], item[0]))\n\t\telse:\n\t\t\tlistOfCellCoordinates.sort()\n\n\t\t# We set up the first pane index\n\t\tpane_index = self.__columns[0] if self.orient == HORIZONTAL else self.__rows[0]\n\n\t\tposition_of_main_index = 1 if self.orient == HORIZONTAL else 0\n\n\t\tnewPanedWindow = True\n\n\t\tfor cell_coordinates in listOfCellCoordinates:\n\n\t\t\tmain_index = cell_coordinates[position_of_main_index]\n\t\t\tif pane_index != main_index:\n\t\t\t\t# Creating a new pane\n\t\t\t\tpane_index = main_index\n\n\t\t\t\tif pane_index in self.fixedPanes:\n\t\t\t\t\tframeOfPane = Frame(self)\n\t\t\t\t\tframeOfPane.pack(side=self.__packSideBetweenPanes)\n\t\t\t\t\tnewPanedWindow = True\n\t\t\t\telse:\n\t\t\t\t\tif newPanedWindow:\n\t\t\t\t\t\tpaneMaster = PanedWindow(self,orient=self.orient, bd=0, sashwidth=3)\n\t\t\t\t\t\tpaneMaster.index_range = (pane_index, pane_index + 1 )\n\n\t\t\t\t\t\tpaneMaster.pack(side=self.__packSideBetweenPanes)\n\n\t\t\t\t\t\tself.panedwindows.append(paneMaster)\n\t\t\t\t\t\tnewPanedWindow = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tbeginning_of_range = paneMaster.index_range[0]\n\t\t\t\t\t\tending_of_range = pane_index + 1\n\t\t\t\t\t\tpaneMaster.index_range = (beginning_of_range, ending_of_range)\n\n\t\t\t\t\tframeOfPane = Frame(paneMaster)\n\t\t\t\t\tframeOfPane.pack()\n\t\t\t\t\tpaneMaster.add(frameOfPane)\n\n\t\t\t\t\tpaneMaster.paneconfigure(frameOfPane, minsize=self.minsize)\n\n\t\t\t\tsizeOfPane = self.__columnWidths[pane_index] if self.orient == HORIZONTAL else self.__rowHeights[pane_index]\n\n\t\t\t\tif self.orient == HORIZONTAL:\n\t\t\t\t\tframeOfPane.configure(width=sizeOfPane, height=self.tableHeight)\n\t\t\t\telse:\n\t\t\t\t\tframeOfPane.configure(width=self.tableWidth, height=sizeOfPane)\n\n\t\t\t\tframeOfPane.pack_propagate(False)\n\n\t\t\t\tself.__panes[pane_index] = frameOfPane\n\n\t\t\tcellFrame = Frame(frameOfPane, bd=1, relief=SUNKEN)\n\t\t\tcellFrame.pack(side=self.__packSideInsidePane, expand=YES, fill=self.__cellFill)\n\t\t\tcellFrame.pack_propagate(False)\n\n\t\t\twidget = self.__gridOfWidgets[cell_coordinates]\n\n\t\t\tif widget:\n\t\t\t\twidget.lift()\n\t\t\t\twidget.pack(in_=cellFrame, expand=YES, fill=BOTH)\n\n\t\t\t\tif self.orient == HORIZONTAL:\n\t\t\t\t\tcellFrame.configure( height=self.__rowHeights[ cell_coordinates[0] ] )\n\t\t\t\telse:\n\t\t\t\t\tcellFrame.configure( width=self.__columnWidths[ cell_coordinates[1] ] )\n\n\t\tself.builded = True\n\n\tdef __calculate_list_of_cell_widths_and_heights(self):\n\t\tself.__rowHeights = self.userDefinedHeights.copy()\n\t\tself.__columnWidths = self.userDefinedWidths.copy()\n\n\t\tfor coordinate, widget in self.__gridOfWidgets.items():\n\t\t\ti , j = coordinate\n\t\t\tif i not in self.userDefinedHeights:\n\t\t\t\tself.__rowHeights[i] = max(self.__rowHeights.get(i,0),widget.winfo_reqheight())\n\n\t\t\tif j not in self.userDefinedWidths:\n\t\t\t\tself.__columnWidths[j] = max(self.__columnWidths.get(j,0),widget.winfo_reqwidth())\n\n\t\tself.calculate_dimensions = False\n\n\t@property\n\tdef tableHeight(self):\n\t\treturn sum(self.__rowHeights.values())\n\n\t@property\n\tdef tableWidth(self):\n\t\treturn sum(self.__columnWidths.values())\n\n\n\tdef rowconfigure(self,row, height=None, fixed = None):\n\t\tif row not in self.__rows:\n\t\t\traise Exception(\"Row %d doesn't exists.\" % column)\n\n\t\tif height is not None:\n\n\t\t\tif type(height) is not int: raise Exception(\"The height must be an integer:%s\"%height)\n\n\t\t\tself.userDefinedHeights[row] = height\n\n\t\t\tif self.orient == VERTICAL:\n\t\t\t\tself.__panes[row].configure(height=height)\n\t\t\telse:\n\t\t\t\tfor column in self.__rows[row]:\n\t\t\t\t\tcellFrame = self.__gridOfWidgets[row,column].pack_info()['in']\n\t\t\t\t\tcellFrame.configure(height=height)\n\n\t\tif fixed is not None:\n\t\t\tif self.orient == HORIZONTAL:\n\t\t\t\traise Exception(\"You can't set fixed property on a row because 'orient' is VERTICAL.\")\n\n\t\t\tself.fix_pane(index, fixed)\n\n\tdef columnconfigure(self,column, width=None, fixed = None):\n\t\tif column not in self.__columns:\n\t\t\traise Exception(\"Column %d doesn't exists.\" % column)\n\n\t\tif width is not None:\n\t\t\tif type(width) is not int: raise Exception(\"The width must be an integer:%s\"%width)\n\n\t\t\tself.userDefinedWidths[column] = width\n\n\t\t\tif self.orient == HORIZONTAL:\n\t\t\t\tself.__panes[column].configure(width=width)\n\t\t\telse:\n\t\t\t\tfor row in self.__columns[column]:\n\t\t\t\t\tcellFrame = self.__gridOfWidgets[row,column].pack_info()['in']\n\t\t\t\t\tcellFrame.configure(width=width)\n\n\t\tif fixed is not None:\n\t\t\tif self.orient == VERTICAL:\n\t\t\t\traise Exception(\"You can't set fixed property on a column because 'orient' is HORIZONTAL.\")\n\n\t\t\tself.fix_pane(index, fixed)\n\n\tdef fix_pane(index, fixed=True):\n\t\tif self.builded:\n\t\t\traise Exception(\"You can't modify the fixed of the panes once you builded the grid.\")\n\n\t\tif fixed == True:\n\t\t\tself.fixedPanes.add(index)\n\t\telif fixed == False:\n\t\t\tself.fixedPanes.discard(index)\n\t\telse:\n\t\t\traise Exception(\"fixed must be 'True' or 'False'.\")\n\n\tdef fix_all_panes(self):\n\t\tif self.builded:\n\t\t\traise Exception(\"You can't modify the fixed of the panes once you builded the grid.\")\n\n\t\tfixedPanes_list = self.__columns.keys() if self.orient == HORIZONTAL else self.__rows.keys()\n\t\tself.fixedPanes = set(fixedPanes_list)\n\n\tdef realease_all_panes(self):\n\t\tif self.builded:\n\t\t\traise Exception(\"You can't modify the fixed of the panes once you builded the grid.\")\n\n\t\tself.fixedPanes = set()\n\n\tdef is_a_fixed_pane(self, index ):\n\t\tpaneWidget = self.__panes[index]\n\n\t\tparent = self.nametowidget( paneWidgetid.winfo_parent() )\n\n\t\tif parent.__class__.name == 'PanedWindow':\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get_row_height(self, row):\n\t\tif self.calculate_dimensions:\n\t\t\tself.__calculate_list_of_cell_widths_and_heights()\n\n\t\tif self.orient == HORIZONTAL:\n\t\t\tif self.builded:\n\t\t\t\treturn self.__panes[row].winfo_height()\n\t\t\telse:\n\t\t\t\treturn self.__rowHeights[row]\n\t\telse:\n\t\t\treturn self.__rowHeights[row]\n\n\tdef get_column_width(self, column):\n\t\tif self.calculate_dimensions:\n\t\t\tself.__calculate_list_of_cell_widths_and_heights()\n\n\t\tif self.orient == HORIZONTAL:\n\t\t\treturn self.__columnWidths[column]\n\t\telse:\n\t\t\tif self.builded:\n\t\t\t\treturn self.__panes[column].winfo_width()\n\t\t\telse:\n\t\t\t\treturn self.__columnWidths[column]\n\n\tdef check_coordinates(self, coordinates):\n\t\tif type(coordinates) is int:\n\t\t\ti = 0\n\t\t\tj = coordinates\n \t\telse:\n\t\t\ttry:\n\t\t\t\ti, j = coordinates\n\t\t\t\tif type(i) is not int or type(j) is not int:\n\t\t\t\t\traise Exception(\"Invalid coordinate\")\n\t\t\texcept:\n\t\t\t\traise Exception(\"Invalid coordinate\")\n\n\t\treturn (i,j)\n\n\tdef __getitem__(self, coordinates):\n\t\tcoordinates = self.check_coordinates(coordinates)\n\n\t\treturn self.__gridOfWidgets.get(coordinates)\n\n\n\tdef __setitem__(self,coordinates, widget):\n\t\tif self.builded:\n\t\t\traise Exception(\"The panedgrid was builded. Use cell_update() instead.\")\n\n\t\ti , j = self.check_coordinates(coordinates)\n\n\t\tself.__gridOfWidgets[i,j] = widget\n\n\t\tself.__rows[i].add(j)\n\t\tself.__columns[j].add(i)\n\n\t\tself.calculate_dimensions = True\n\n\tdef __delitem__(self, coordinates ):\n\t\tif self.builded:\n\t\t\traise Exception(\"The panedgrid was builded. Use cell_update() instead.\")\n\n\t\ti, j = self.check_coordinates(coordinates)\n\n\t\tdel self.__gridOfWidgets[i,j]\n\n\t\tself.__rows[i].discard(j)\n\t\tif not self.__rows[i]: del self.__rows[i]\n\n\t\tself.__columns[j].discard(i)\n\t\tif not self.__columns[j]: del self.__columns[j]\n\n\t\tself.calculate_dimensions = True\n\n\tdef grid(self, *args, **kargs):\n\t\tif not self.builded: self.build()\n\t\tFrame.grid(self,*args, **kargs)\n\n\tdef pack(self, *args, **kargs):\n\t\tif not self.builded: self.build()\n\t\tFrame.pack(self,*args, **kargs)\n\n\n\t\t\t\t\n\t\t\t\t\ndef test():\n\timport random\n\n\troot = Tk()\n\n\tdef table_of_random_widgets(master, numRows,numColumns, **kwargs):\n\t\timport ttk\n\n\t\tdemo = testConnection.create_widget(master, shared_columns ='all', shared_panedwindows='all', **kwargs)\n\n\t\ttkinterWidgets =  (\n\t\t\t\t\t(Label, {'text':'This is a label'}),\n\t\t\t\t\t(Label, {'text':'This is another label', 'bg':'yellow'}),\n\t\t\t\t\t(Checkbutton,{}),\n\t\t\t\t\t(Button, {'text':'Click me'}),\n\t\t\t\t\t(ttk.Combobox, {'values':('item1', 'item2','item3','item4')})\n\t\t\t\t\t)\n\n\t\tfor i in range(numRows):\n\t\t\tfor j in range(numColumns):\n\t\t\t\twidgetClass, args = random.choice(tkinterWidgets)\n\t\t\t\twidget = widgetClass(demo,**args)\n\n\t\t\t\tdemo[i,j] =  widget\n\n\t\treturn demo\n\ttestConnection = connected_Panedgrids(0,6)\n\n\ttable1 = table_of_random_widgets(root, numRows=4,numColumns=6, fixedPanes=[2,5])\n\ttable2 = table_of_random_widgets(root, numRows=4,numColumns=6, fixedPanes=[2,5])\n\n\ttestConnection.update()\n\n\tLabel(root, text= \"Two different paned grids connected. Columns 2 and 5 are fixed.\").pack(anchor=NW,pady=7, padx=12)\n\ttable1.pack(anchor=NW,pady=2, padx=12)\n\n\temptySpace = Frame(root, height =50)\n\temptySpace.pack()\n\n\ttable2.pack(anchor=NW,pady=2, padx=12)\n\n\temptySpace = Frame(root, height =20)\n\temptySpace.pack()\n\n\n\troot.mainloop()\n\n\nif __name__ == '__main__':\n\ttest()\n"
}
{
    "repo_name": "lukeolson/pyewald",
    "ref": "refs/heads/master",
    "path": "setup.py",
    "copies": "1",
    "content": "from distutils.core import setup\n\nDESCRIPTION = 'Ewald Summation in Python'\nLONG_DESCRIPTION = \"\"\"pyewald: Ewald Summation in Python\n\nCode details are found at http://github.com/lukeolson/pyewald\n\"\"\"\nNAME = 'pyewald'\nAUTHOR = 'Luke Olson'\nAUTHOR_EMAIL = 'luke.olson@gmail.com'\nMAINTAINER = 'Luke Olson'\nMAINTAINER_EMAIL = 'luke.olson@gmail.com'\nURL = 'http://github.com/lukeolson/pyewald'\nDOWNLOAD_URL = 'http://github.com/lukeolson/pyewald'\nLICENSE = 'MIT'\n\nsetup(name=NAME,\n      version='0.1',\n      description=DESCRIPTION,\n      long_description=LONG_DESCRIPTION,\n      author=AUTHOR,\n      author_email=AUTHOR_EMAIL,\n      maintainer=MAINTAINER,\n      maintainer_email=MAINTAINER_EMAIL,\n      url=URL,\n      download_url=DOWNLOAD_URL,\n      license=LICENSE,\n      packages=['pyewald',\n                'pyewald.tests'],\n      )\n"
}
{
    "repo_name": "azharhappy/pymysql",
    "ref": "refs/heads/master",
    "path": "GetValues.py",
    "copies": "1",
    "content": "import pymysql\ncon=pymysql.connect(host='localhost',port=3306,user='root',passwd='root',db='test') \ncursor=con.cursor()\nsql=\"SELECT * FROM `student`\"\ncursor.execute(sql)\nresults=cursor.fetchall()\nfor i in results:\n    print(i)\n"
}
{
    "repo_name": "chrisdjscott/Atoman",
    "ref": "refs/heads/master",
    "path": "atoman/rendering/renderers/voronoiRenderer.py",
    "copies": "1",
    "content": "\n\"\"\"\nModule for rendering Voronoi cells\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nimport logging\n\nimport vtk\nimport numpy as np\n\nfrom . import baseRenderer\nfrom . import povrayWriters\nfrom .. import utils\nfrom ...algebra import vectors\n\n\nclass VoronoiRenderer(baseRenderer.BaseRenderer):\n    \"\"\"\n    Render Voronoi cells.\n    \n    \"\"\"\n    def __init__(self):\n        super(VoronoiRenderer, self).__init__()\n        self._logger = logging.getLogger(__name__)\n    \n    def render(self, inputState, visibleAtoms, scalarsArray, lut, voro, voronoiOptions, colouringOptions):\n        \"\"\"\n        Render Voronoi cells for visible atoms\n        \n        \"\"\"\n        self._logger.debug(\"Rendering Voronoi cells (%d visible atoms)\", len(visibleAtoms))\n        \n        # object for combining poly datas\n        appendPolyData = vtk.vtkAppendPolyData()\n        \n        # loop over the visible atoms\n        for visIndex, index in enumerate(visibleAtoms):\n            # check we are working with the same atom!\n            inp_pos = inputState.atomPos(index)\n            out_pos = voro.getInputAtomPos(index)\n            sep = vectors.separation(inp_pos, out_pos, inputState.cellDims, np.ones(3, np.int32))\n            if sep \u003e 1e-4:\n                raise RuntimeError(\"Voronoi ordering is different\")\n            \n            # faces\n            faces = voro.atomFaces(index)\n            if faces is None:\n                continue\n            \n            # scalar value for this atom\n            scalar = scalarsArray[visIndex]\n            \n            # points (vertices)\n            points = vtk.vtkPoints()\n            scalars = vtk.vtkFloatArray()\n            for point in voro.atomVertices(index):\n                points.InsertNextPoint(point)\n                scalars.InsertNextValue(scalar)\n            \n            # make polygons\n            facePolygons = vtk.vtkCellArray()\n            for face in faces:\n                polygon = vtk.vtkPolygon()\n                \n                # set number of vertices\n                polygon.GetPointIds().SetNumberOfIds(len(face))\n                \n                # add vertices (indexes)\n                for i, index in enumerate(face):\n                    polygon.GetPointIds().SetId(i, index)\n                \n                # add the polygon to the set\n                facePolygons.InsertNextCell(polygon)\n            \n            # polydata object\n            regionPolyData = vtk.vtkPolyData()\n            regionPolyData.SetPoints(points)\n            regionPolyData.SetPolys(facePolygons)\n            regionPolyData.GetPointData().SetScalars(scalars)\n            \n            # append the poly data\n            if vtk.vtkVersion.GetVTKMajorVersion() \u003c= 5:\n                appendPolyData.AddInputConnection(regionPolyData.GetProducerPort())\n            else:\n                appendPolyData.AddInputData(regionPolyData)\n        appendPolyData.Update()\n        \n        # remove any duplicate points\n        cleanFilter = vtk.vtkCleanPolyData()\n        cleanFilter.SetInputConnection(appendPolyData.GetOutputPort())\n        cleanFilter.Update()\n        \n        # mapper\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputConnection(cleanFilter.GetOutputPort())\n        mapper.SetLookupTable(lut)\n        utils.setMapperScalarRange(mapper, colouringOptions, len(inputState.specieList))\n        \n        # actor\n        actor = vtk.vtkActor()\n        actor.SetMapper(mapper)\n        actor.GetProperty().SetOpacity(voronoiOptions.opacity)\n        \n        # store data\n        self._actor = utils.ActorObject(actor)\n        self._data[\"LUT\"] = lut\n        self._data[\"Voronoi\"] = voro\n        self._data[\"Scalars\"] = scalarsArray\n        self._data[\"Visible atoms\"] = visibleAtoms\n        self._data[\"Lattice\"] = inputState\n        self._data[\"Opacity\"] = voronoiOptions.opacity\n    \n    def writePovray(self, filename):\n        \"\"\"Write voronoi cells to POV-Ray file.\"\"\"\n        self._logger.debug(\"Writing Voronoi cells POV-Ray file\")\n        \n        # povray writer\n        writer = povrayWriters.PovrayVoronoiWriter()\n        writer.write(filename, self._data[\"Visible atoms\"], self._data[\"Lattice\"], self._data[\"Scalars\"],\n                     self._data[\"LUT\"], self._data[\"Voronoi\"], self._data[\"Opacity\"])\n"
}
{
    "repo_name": "plorry/mortality_api",
    "ref": "refs/heads/master",
    "path": "mortality/actuaries/api/renderers.py",
    "copies": "1",
    "content": "from rest_framework.renderers import JSONRenderer\r\nfrom actuaries.constants import Attribution\r\n\r\n\r\nclass AttributionRenderer(JSONRenderer):\r\n    \"\"\"\r\n    Include attribution with each response\r\n    \"\"\"\r\n    def render(self, data, accepted_media_type=None, renderer_context=None):\r\n        response_data = data\r\n        response_data.update({'attribution': Attribution.ATTRIBUTION})\r\n\r\n        response = super(AttributionRenderer, self).render(response_data, accepted_media_type, renderer_context)\r\n        return response\r\n"
}
{
    "repo_name": "publicscience/hive",
    "ref": "refs/heads/master",
    "path": "app/routes/oauth/github.py",
    "copies": "1",
    "content": "from app import app\nfrom . import requires_login\nfrom flask import redirect, url_for, flash, session, render_template, request\nfrom rauth.service import OAuth2Service\n\ngithub = OAuth2Service(\n    name='github',\n    base_url='https://api.github.com/user',\n    authorize_url='https://github.com/login/oauth/authorize',\n    access_token_url='https://github.com/login/oauth/access_token',\n    client_id=app.config['GITHUB_CLIENT_ID'],\n    client_secret=app.config['GITHUB_CLIENT_SECRET']\n)\n\n@app.route('/github_login')\n@requires_login\ndef github_login():\n    \"\"\"\n    Authenticate with Github.\n    \"\"\"\n    redirect_uri = url_for('github_authorized', _external=True)\n    return redirect(github.get_authorize_url(redirect_uri=redirect_uri, scope='user,repo'))\n\n@app.route('/github')\n@requires_login\ndef github_info():\n    \"\"\"\n    Provide some info about linking\n    a Github account.\n    \"\"\"\n    return render_template('github.html')\n\n# After authentication.\n@app.route(app.config['GITHUB_REDIRECT_URI'])\ndef github_authorized():\n    \"\"\"\n    Post-authentication redirect route.\n    Adds access token to the session.\n    \"\"\"\n\n    if not 'code' in request.args:\n        flash(u'Whoops, you denied us access to your Github account')\n        return redirect(url_for('github_login'))\n\n    redirect_uri = url_for('github_authorized', _external=True)\n    data = dict(code=request.args['code'], redirect_uri=redirect_uri, scope='user,repo')\n    session_ = github.get_auth_session(data=data)\n\n    # Store access token in session.\n    session['github_access_token'] = session_.access_token\n\n    from app.models.user import current_user\n    current_user = current_user()\n    current_user.github_id = session_.get('/user').json()['id']\n    current_user.github_access = session_.access_token\n    current_user.save()\n\n    # Redirect\n    return redirect('/')\n\ndef api(token=None):\n    if token is None:\n        token = session['github_access_token']\n    return github.get_session(token=token)\n"
}
{
    "repo_name": "dArignac/scapegoat",
    "ref": "refs/heads/master",
    "path": "docs/conf.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\n#\n# complexity documentation build configuration file, created by\n# sphinx-quickstart on Tue Jul  9 22:26:36 2013.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\ncwd = os.getcwd()\nparent = os.path.dirname(cwd)\nsys.path.append(parent)\n\nimport scapegoat\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'scaepgoat'\ncopyright = u'2015, Alexander Herrmann'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = scapegoat.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = scapegoat.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'default'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"\u003cproject\u003e v\u003crelease\u003e documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a \u003clink\u003e tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'scapegoatdoc'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  ('index', 'scapegoat.tex', u'scapegoat Documentation',\n   u'Alexander Herrmann', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'scapegoat', u'scapegoat Documentation',\n     [u'Alexander Herrmann'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'scapegoat', u'scapegoat Documentation',\n   u'Alexander Herrmann', 'scapegoat', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n"
}
{
    "repo_name": "lluxury/P_U_S_A",
    "ref": "refs/heads/master",
    "path": "9_package_management/distutils/setup.py",
    "copies": "1",
    "content": "from setuptools import setup\nsetup(\n    name = \"distutils_example\",\n    description = \"A completly Useless Script That Prints\",\n    author = \"yann\",\n    author_email = \"xx@xx\",\n    version = \"0.1\",\n    url = \"htt这://www.pyatl.org\"\n)\n"
}
{
    "repo_name": "jinjin123/devops2.0",
    "ref": "refs/heads/master",
    "path": "devops/ops/migrations/0027_auto_20171115_1015.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\n# Generated by Django 1.9 on 2017-11-15 10:15\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('ops', '0026_auto_20171115_1014'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='hostinfo',\n            name='pwd',\n            field=models.CharField(default='N', max_length=10, null=True),\n        ),\n    ]\n"
}
{
    "repo_name": "MegabytePhreak/rdl",
    "ref": "refs/heads/master",
    "path": "rdlcompiler/systemrdl/test/rdl/parse.py",
    "copies": "1",
    "content": "__author__ = 'MegabytePhreak'\n\nimport rdlcompiler.systemrdl.parser as parser\n\n\ndef test_enum():\n    p = parser.RdlParser()\n\n    p.parse('enum myenum { True = 1\\'b0; False = 1\\'b1 { name=\"FALSE\"; descn=\"The opposite of \\nTRUE\"; }; };')\n"
}
{
    "repo_name": "weso/CWR-DataApi",
    "ref": "refs/heads/master",
    "path": "tests/grammar/factory/record/test_message.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\nimport unittest\n\nfrom pyparsing import ParseException\n\nfrom tests.utils.grammar import get_record_grammar\n\n\"\"\"\nCWR Message grammar tests.\n\nThe following cases are tested:\n\"\"\"\n\n__author__ = 'Bernardo Martínez Garrido'\n__license__ = 'MIT'\n__status__ = 'Development'\n\n\nclass TestMessageGrammar(unittest.TestCase):\n    \"\"\"\n    Tests that the Message grammar decodes correctly formatted strings\n    \"\"\"\n\n    def setUp(self):\n        self.grammar = get_record_grammar('message')\n\n    def test_valid_full(self):\n        \"\"\"\n        Tests that NWN grammar decodes correctly formatted record prefixes.\n\n        This test contains all the optional fields.\n        \"\"\"\n        record = 'MSG0000123400000023F00001235AGRE123MESSAGE                                                                                                                                               '\n\n        result = self.grammar.parseString(record)[0]\n\n        self.assertEqual('MSG', result.record_type)\n        self.assertEqual(1234, result.transaction_sequence_n)\n        self.assertEqual(23, result.record_sequence_n)\n        self.assertEqual('F', result.message_type)\n        self.assertEqual(1235, result.original_record_sequence_n)\n        self.assertEqual('AGR', result.message_record_type)\n        self.assertEqual('E', result.message_level)\n        self.assertEqual(123, result.validation_n)\n        self.assertEqual('MESSAGE', result.message_text)\n\n\nclass TestMessageGrammarException(unittest.TestCase):\n    def setUp(self):\n        self.grammar = get_record_grammar('message')\n\n    def test_empty(self):\n        \"\"\"\n        Tests that a exception is thrown when the the works number is zero.\n        \"\"\"\n        record = ''\n\n        self.assertRaises(ParseException, self.grammar.parseString, record)\n\n    def test_invalid(self):\n        record = 'This is an invalid string'\n\n        self.assertRaises(ParseException, self.grammar.parseString, record)\n"
}
{
    "repo_name": "1tush/reviewboard",
    "ref": "refs/heads/master",
    "path": "reviewboard/accounts/signals.py",
    "copies": "1",
    "content": "from __future__ import unicode_literals\n\nfrom django.dispatch import Signal\n\n\nuser_registered = Signal(providing_args=[\"user\"])\n"
}
{
    "repo_name": "LCBRU/reporter",
    "ref": "refs/heads/master",
    "path": "reporter/uhl_reports/civicrm/practice_missing_code.py",
    "copies": "1",
    "content": "#!/usr/bin/env python3\r\n\r\nfrom reporter.core import SqlReport, Schedule\r\nfrom reporter.uhl_reports.civicrm import get_contact_link\r\nfrom reporter.emailing import RECIPIENT_IT_DQ\r\n\r\n\r\nclass CivicrmPracticeMissingCode(SqlReport):\r\n    def __init__(self):\r\n        super().__init__(\r\n            introduction=(\"The following GP Practices do not have \"\r\n                          \"codes in CiviCRM\"),\r\n            recipients=[RECIPIENT_IT_DQ],\r\n            sql='''\r\n                SELECT\r\n                    con.display_name,\r\n                    con.id\r\n                FROM STG_CiviCRM.dbo.civicrm_contact con\r\n                LEFT JOIN STG_CiviCRM.dbo.civicrm_value_gp_surgery_data_3 gp\r\n                    ON gp.entity_id = con.id\r\n                WHERE con.contact_type = 'Organization'\r\n                    AND con.contact_sub_type LIKE '%GP_Surgery%'\r\n                    AND con.is_deleted = 0\r\n                    AND LEN(RTRIM(LTRIM(COALESCE(gp.practice_code_7, '')))) = 0\r\n                ;\r\n                ''',\r\n                schedule=Schedule.daily\r\n        )\r\n\r\n    def get_report_line(self, row):\r\n        return '- {}\\r\\n'.format(\r\n            get_contact_link(\r\n                row['display_name'], row['id']))\r\n"
}
{
    "repo_name": "robjstan/python-enzymegraph",
    "ref": "refs/heads/master",
    "path": "example/enzymegraph-example.py",
    "copies": "1",
    "content": "\n# coding: utf-8\n\n# # python-enzymegraph\n# A Python package for generating models of enzymes under the quasi-steady-state assumption (QSSA).\n# \n# ## Requirements\n# Currently tested with only Python 3.4.\n# \n# Requires [sympy](https://github.com/sympy/sympy).\n# \n# ## Installation\n# \tpip install git+git://github.com/robjstan/python-enzymegraph.git\n# \n# ## References\n# \tGunawardena, J.\n#     A linear framework for time-scale separation in nonlinear biochemical systems.\n#     PLoS ONE (2012)\n# \n# \tGabow, H. N. \u0026 Myers, E. W.\n#     Finding all spanning trees of directed and undirected graphs.\n#     SIAM Journal on Computing (1978)\n#     \n# ## Example\n# Biochemical example is the Michaelis-Menten function with product inhibition.\n# $$E + S \\underset{k_2}{\\overset{k_1}{\\rightleftharpoons}}\n#   ES \\overset{k_\\text{cat}}{\\rightleftharpoons}\n#   EP \\underset{k_4}{\\overset{k_3}{\\rightleftharpoons}}\n#   E + P$$\n\n# ### Python setup\n\n# In[1]:\n\nfrom enzymegraph import *\n\n\n# In[2]:\n\nfrom sympy import *\nfrom numpy import linspace\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\n\n\n# ### Symbolic setup\n# \n# First we need to initialise the varaibles that describe the biochemical species and complexes.\n# \n#     e  = free enzyme\n#     s  = substrate\n#     p  = product\n#     es = enzyme bound to substrate\n#     ep = enzyme bound to product\n\n# In[3]:\n\ne, s, p, es, ep = symbols('e s p es ep', positive = True)\n\n\n# Then we need to describe the edges between these species/complexes, labelled with their rates, including the uptake of substrate and product.\n\n# In[4]:\n\nk1, k2, k3, k4, kcat = symbols('k_1 k_2 k_3, k_4 k_cat', positive = True)\n\nedges = {(e, es): k1*s, (es, e): k2,\n         (e, ep): k3*p, (ep, e): k4,\n         (es,ep): kcat, }\n\n\n#### Creating the graph object\n\n# #### *enzymegraph.enzymegraph(...)*\n# \n# The class constructor *enzymegraph.enzymegraph(...)* will take as its first argument either:\n# * a dictionary with keys=edges and values=edge labels\n# * or, a list of edges (edge labels assumed to be equal to 1)\n\n# In[5]:\n\ngraph = enzymegraph(edges)\ngraph\n\n\n# In[6]:\n\nedges = list(edges.keys())\n\nenzymegraph(edges)\n\n\n# #### *enzymegraph.from_matrix(...)*\n# The class constructor *enzymegraph.from_matrix(...)* will take as its first argument a matrix of edges, with an optional second argument giving vertex names.\n\n# In[7]:\n\nm = [[ 0,k1*s, k3*p],\n     [k2,   0, kcat],\n     [k4,   0,    0]]\nv = [e, es, ep]\n\nenzymegraph.from_matrix(m, v).edges\n\n\n#### ODE outputs\n\n# #### *graph.ode_model()*\n# The class method *enzymegraph.ode_model()* returns the equivalent mass-action (ODE) model of the system.\n\n# In[8]:\n\nfor var, ode in graph.ode_model().items():\n    print(\"d%s/dt = %s\" % (var, ode))\n\n\n# #### *graph.ode_function()*\n# The method *enzymegraph.ode_function()* returns the equivalent mass-action (ODE) model of the system as a python function (suitable for simulation using *scipy.odeint(...)*.\n# \n# This requires three extra parameters to be given:\n# * param_dict, a dictionary giving specific values for the system parameters\n# * variables, a list of the model variables in a desired order\n# * extra_odes, a dictionary of additional ODEs that complete the system (e.g. equations that describe the production of substrate and product)\n\n# In[9]:\n\nparam_dict = {k1:1, k2:0.1, k3:2, k4:0.5, kcat:1}\nvariables = [e, s, p, es, ep]\nextra_odes = {s: k2*es - k1*e*s, p: k4*ep - k3*e*p}\node_func = graph.ode_function(param_dict, variables, extra_odes)\node_func\n\n\n# In[10]:\n\nconcs = zip(*odeint(ode_func, [1,1,0,0,0], linspace(0,10,100)))\n\nfor conc, var in zip(concs, variables):\n    plt.plot(conc, label=var)\nplt.xlabel(\"time\")\nplt.ylabel(\"concentration\")\nplt.legend()\nplt.show()\n\n\n#### Spanning trees\n\n# #### *graph.spanning_trees()*\n# The method *enzymegraph.spanning_trees()* enumerates all the possible spanning trees of the graph, returning these as *enzymegraph* objects.\n\n# In[11]:\n\nfor span_tree in graph.spanning_trees():\n    print(span_tree.edges)\n\n\n#### TikZ output\n\n# #### *graph.graph_as_tikz(...)*\n# \n# The method *enzymegraph.graph_as_tikz(...)* outputs the graph as TikZ commands, suitable for inclusion in a LaTeX document.\n# \n# This requires extra parameters to be given:\n# * vertex_pos, a dictionary giving Tikz position arguments for each of the complexes\n# * edge_styles (optional), a dictionary giving a Tikz line argument for any of the edges\n# * relabel (optional), whether to relabel the output TikZ nodes (for instance if their names are otherwise complicated (and so may cause TikZ errors)\n# * vertex_text_formatter (optional), a lambda that takes the symbolic label of the vertex and returns a formatted string.\n# * edge_text_formatter (optional), a lambda that takes the symbolic label of the edge and returns a formatted string.\n\n# In[12]:\n\nvertex_pos = {e:  \"(0,0)\",\n              es: \"(240:2)\",\n              ep: \"(300:2)\",}\nedge_styles = {(e, es): \"to[out=210,in=90]\",\n               (e, ep): \"to[out=330,in=90]\",}\n\nprint(graph.graph_as_tikz(vertex_pos, edge_styles, relabel=False))\n\n\n# #### *graph.spanning_trees_as_tikz(...)* (beta)\n# The method *enzymegraph.spanning_trees_as_tikz(...)* outputs all the spanning trees of the system as TikZ commands, suitable for inclusion in a LaTeX document.\n# \n# This requires extra parameters, as described under *enzymegraph.graph_as_tikz(...)*.\n\n# In[13]:\n\nprint(graph.spanning_trees_as_tikz(vertex_pos, edge_styles, relabel=False))\n\n\n#### Basis element\n\n# #### *graph.basis_element()*\n# The method *enzymegraph.basis_element()* outputs a dictionary of the basis element of the graph.\n\n# In[14]:\n\nfor var, basis_elem in graph.basis_element().items():\n    print(\"%s = %s\" % (var, basis_elem))\n\n\n# #### *graph.qssa_replacements(...)*\n# The method *enzymegraph.qssa_replacements(...)* outputs each the quasi-steady-state solution for each intermediate, given in terms of the total enzyme (the parameter for which should be given as the first term).\n\n# In[15]:\n\net = symbols('e_t', positive = True)\nqssa_reps = graph.qssa_replacements(et)\n\nfor var, rep in qssa_reps.items():\n    print(\"%s = %s\" % (var, rep))\n\n\n# These replacements can be used to substitute into the original ODE system.\n# \n# e.g. $ \\frac{dp}{dt} = k_4 ep - k_3 p e $\n\n# In[16]:\n\ndpdt = k4*ep - k3*p*e\n\nprint(\"dp/dt = %s\" % dpdt.subs(qssa_reps))\n\n\n# In[16]:\n\n\n\n"
}
{
    "repo_name": "Daerdemandt/dota2league-tracker",
    "ref": "refs/heads/master",
    "path": "dota2league-tracker/config.py",
    "copies": "1",
    "content": "from konf import Konf\nfrom good import Schema, Optional, Extra, Reject, Invalid, Any, Fallback\nfrom collections import Counter\nfrom webhooker import template_schema\n\n#TODO: use messages to explain what's wrong in the config\nis_valid_server_conf = Schema({\n    'port' : Any(int, Fallback(5000)),\n    'debug' : Any(bool, Fallback(True))\n})\n\nis_valid_hooklist = Schema({\n    Extra : template_schema\n})\n\ndef parse(filename):\n    k = Konf(filename)\n\n    # syntax:\n    config = {\n        'server' : k('server', is_valid_server_conf),\n        'steam api key' : k('steam api key', str),\n        'hooks' : k('hooks', is_valid_hooklist)\n    }\n\n    # semantics:\n\n    return config\n\n"
}
{
    "repo_name": "ArnaudCalmettes/gator",
    "ref": "refs/heads/master",
    "path": "test/aiojob/unit/test_job.py",
    "copies": "1",
    "content": "import asyncio\nfrom datetime import timedelta\nfrom unittest import mock\n\nimport pytest\n\nfrom gator.aiojob import Job\n\n\ndef test_job_unique_jid():\n    jobs = [Job() for _ in range(1000)]\n    jids = {job.jid for job in jobs}\n    assert len(jobs) == len(jids)\n\n\n@pytest.fixture\ndef job():\n    return Job()\n\n\n@pytest.fixture\ndef pending_job(job):\n    return job\n\n\ndef test_pending_job_start(pending_job):\n    pending_job.start()\n    assert pending_job.state() == 'RUNNING'\n\n\ndef test_pending_job_done(pending_job):\n    assert not pending_job.done()\n\n\ndef test_pending_job_cancelled(pending_job):\n    assert not pending_job.cancelled()\n\n\ndef test_pending_job_state(pending_job):\n    assert pending_job.state() == 'PENDING'\n\n\ndef test_pending_job_result(pending_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        pending_job.result()\n\n\ndef test_pending_job_exception(pending_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        pending_job.exception()\n\n\ndef test_pending_job_cancel(pending_job):\n    assert pending_job.cancel()\n    assert pending_job.cancelled()\n    assert pending_job.state() == 'CANCELLED'\n\n\ndef test_pending_job_set_result(pending_job):\n    pending_job.set_result(True)\n    assert pending_job.done()\n    assert pending_job.state() == 'DONE'\n    assert pending_job.duration() == timedelta(0)\n\n\ndef test_pending_job_set_exception(pending_job):\n    pending_job.set_exception(Exception())\n    assert pending_job.exception()\n\n\ndef test_pending_job_duration(pending_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        pending_job.duration()\n\n\n@pytest.fixture\ndef running_job(job):\n    job.start()\n    return job\n\n\ndef test_running_job_start(running_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        running_job.start()\n\n\ndef test_running_job_done(running_job):\n    assert not running_job.done()\n\n\ndef test_running_job_cancelled(running_job):\n    assert not running_job.cancelled()\n\n\ndef test_running_job_state(running_job):\n    assert running_job.state() == 'RUNNING'\n\n\ndef test_running_job_result(running_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        running_job.result()\n\n\ndef test_running_job_exception(running_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        running_job.exception()\n\n\ndef test_running_job_cancel(running_job):\n    assert running_job.cancel()\n\n\ndef test_running_job_set_result(running_job):\n    running_job.set_result(True)\n    assert running_job.done()\n\n\ndef test_running_job_set_exception(running_job):\n    running_job.set_exception(Exception())\n    assert running_job.exception()\n\n\ndef test_running_job_duration(running_job):\n    assert running_job.duration()\n\n\n@pytest.fixture\ndef cancelled_job(job):\n    job.start()\n    job.cancel()\n    return job\n\n\ndef test_cancelled_job_done(cancelled_job):\n    assert cancelled_job.done()\n\n\ndef test_cancelled_job_cancelled(cancelled_job):\n    assert cancelled_job.cancelled()\n\n\ndef test_cancelled_job_state(cancelled_job):\n    assert cancelled_job.state() == 'CANCELLED'\n\n\ndef test_cancelled_job_result(cancelled_job):\n    with pytest.raises(asyncio.CancelledError):\n        cancelled_job.result()\n\n\ndef test_cancelled_job_exception(cancelled_job):\n    with pytest.raises(asyncio.CancelledError):\n        cancelled_job.exception()\n\n\ndef test_cancelled_job_cancel(cancelled_job):\n    assert not cancelled_job.cancel()\n\n\ndef test_cancelled_job_set_result(cancelled_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        cancelled_job.set_result(True)\n\n\ndef test_cancelled_job_set_exception(cancelled_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        cancelled_job.set_exception(Exception())\n\n\ndef test_cancelled_job_duration(cancelled_job):\n    try:\n        cancelled_job.duration()\n    except asyncio.InvalidStateError:\n        pytest.fail(\"Cancelled jobs must have a duration\")\n\n\n@pytest.fixture\ndef done_job(job):\n    job.start()\n    job.set_result(True)\n    return job\n\n\ndef test_done_job_done(done_job):\n    assert done_job.done()\n\n\ndef test_done_job_cancelled(done_job):\n    assert not done_job.cancelled()\n\n\ndef test_done_job_state(done_job):\n    assert done_job.state() == 'DONE'\n\n\ndef test_done_job_result(done_job):\n    assert done_job.result()\n\n\ndef test_done_job_exception(done_job):\n    assert done_job.exception() is None\n\n\ndef test_done_job_cancel(done_job):\n    assert not done_job.cancel()\n\n\ndef test_done_job_set_result(done_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        done_job.set_result(False)\n\n\ndef test_done_job_set_exception(done_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        done_job.set_exception(Exception())\n\n\ndef test_done_job_duration(done_job):\n    try:\n        done_job.duration()\n    except asyncio.InvalidStateError:\n        pytest.fail(\"Finished jobs must have a duration\")\n\n\nclass DummyError(Exception):\n    pass\n\n\n@pytest.fixture\ndef error_job():\n    job = Job()\n    job.start()\n    job.set_exception(DummyError)\n    yield job\n    job.exception()\n\n\ndef test_error_job_done(error_job):\n    assert error_job.done()\n\n\ndef test_error_job_cancelled(error_job):\n    assert not error_job.cancelled()\n\n\ndef test_error_job_state(error_job):\n    assert error_job.state() == 'ERROR'\n\n\ndef test_error_job_result(error_job):\n    with pytest.raises(DummyError):\n        error_job.result()\n\n\ndef test_error_job_exception(error_job):\n    assert isinstance(error_job.exception(), DummyError)\n\n\ndef test_error_job_cancel(error_job):\n    assert not error_job.cancel()\n\n\ndef test_error_job_set_result(error_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        error_job.set_result('foo')\n\n\ndef test_error_job_set_exception(error_job):\n    with pytest.raises(asyncio.InvalidStateError):\n        error_job.set_exception(Exception())\n\n\ndef test_error_job_duration(error_job):\n    try:\n        error_job.duration()\n    except asyncio.InvalidStateError:\n        pytest.fail(\"Jobs in error state must have a duration\")\n\n\ndef test_job_is_hashable(job):\n    assert hash(job)\n\n\ndef test_job_comparison():\n    job1, job2 = Job(), Job()\n    assert job1 == job1\n    assert job1 != job2\n\n\n@pytest.mark.asyncio\nasync def test_job_await_done(event_loop):\n    job = Job(loop=event_loop)\n\n    callback = mock.MagicMock(job)\n    job.add_done_callback(callback)\n\n    async def set_result(job):\n        job.set_result(True)\n\n    asyncio.ensure_future(set_result(job), loop=event_loop)\n    await asyncio.wait_for(job, 5, loop=event_loop)\n    assert job.state() == 'DONE'\n    assert callback.called\n    assert callback.call_args_list == [mock.call(job)]\n\n\n@pytest.mark.asyncio\nasync def test_job_await_cancelled(event_loop):\n    job = Job(loop=event_loop)\n\n    callback = mock.MagicMock(job)\n    job.add_done_callback(callback)\n\n    async def cancel(job):\n        job.cancel()\n\n    asyncio.ensure_future(cancel(job), loop=event_loop)\n    with pytest.raises(asyncio.CancelledError):\n        await asyncio.wait_for(job, 5, loop=event_loop)\n    assert job.state() == 'CANCELLED'\n    assert callback.called\n    assert callback.call_args_list == [mock.call(job)]\n\n\n@pytest.mark.asyncio\nasync def test_job_await_error(event_loop):\n    job = Job(loop=event_loop)\n\n    callback = mock.MagicMock(job)\n    job.add_done_callback(callback)\n\n    class SomeException(Exception):\n        pass\n\n    async def set_exception(job):\n        job.set_exception(SomeException())\n\n    asyncio.ensure_future(set_exception(job), loop=event_loop)\n    with pytest.raises(SomeException):\n        await asyncio.wait_for(job, 5, loop=event_loop)\n    assert job.state() == 'ERROR'\n    assert callback.called\n    assert callback.call_args_list == [mock.call(job)]\n"
}
{
    "repo_name": "mnunezdm/cazasteroides",
    "ref": "refs/heads/master",
    "path": "karmaserver/modules/selection/provider/filter.py",
    "copies": "1",
    "content": "''' Filter of the EFES Algorithm '''\nimport math\nfrom karmaserver.data.models.observation import State\n\n\nclass ObservationFilterAbstract: # pragma: no cover\n    ''' Abstract class for observation Filter '''\n    def observations(self, observation_list, karma_level):\n        ''' Gets the observations for the karma level passed '''\n        raise NotImplementedError('Abstract class, this method should have been implemented')\n\n\nclass ObservationFilter(ObservationFilterAbstract):\n    ''' Implementation of the Filter class '''\n    def __init__(self, number_of_karma_levels, number_of_filter_levels):\n        self.number_of_filter_levels = number_of_filter_levels\n        self.karma_per_filter = number_of_karma_levels / number_of_filter_levels\n\n    def observations(self, observation_list, karma_level):\n        filter_level = self.__get_filter_level(karma_level)\n        filtered_observations = self.__filter_observations(observation_list)\n        return self.__get__specific_observations(filtered_observations, filter_level), filter_level\n\n    def __get__specific_observations(self, filtered_observations, filter_level):\n        magic_number = math.ceil(filter_level / 2) - 1\n        if filter_level % 2 == 0:\n            return filtered_observations[magic_number] + filtered_observations[magic_number + 1]\n        return filtered_observations[magic_number]\n\n    def __get_filter_level(self, karma_level):\n        filter_level = math.ceil(karma_level / self.karma_per_filter)\n        if filter_level \u003e self.number_of_filter_levels:\n            return self.number_of_filter_levels\n        if filter_level == 0:\n            return 1\n        return filter_level\n\n    def __chunkify(self, list_, number_of_groups):\n        return [list_[i::number_of_groups] for i in range(number_of_groups)]\n\n    def __filter_observations(self, observation_list):\n        approved = [observation for observation in observation_list\n                    if observation.state == State.APPROVED]\n        pending = [observation for observation in observation_list\n                   if observation.state == State.PENDING]\n        disputed = [observation for observation in observation_list\n                    if observation.state == State.DISPUTED]\n        pending_split = self.__chunkify(pending, 2)\n        result = [approved, pending_split[0], disputed + pending_split[1]]\n        return result\n        "
}
{
    "repo_name": "wappulehti-apy/diilikone-api",
    "ref": "refs/heads/master",
    "path": "diilikone/models/__init__.py",
    "copies": "1",
    "content": "from .deal import Deal  # noqa\nfrom .deal_group import DealGroup  # noqa\nfrom .group_provision import GroupProvision  # noqa\nfrom .individual_provision import IndividualProvision  # noqa\nfrom .product_type import ProductType  # noqa\nfrom .user import User  # noqa\n"
}
{
    "repo_name": "getopenmono/buildsystem",
    "ref": "refs/heads/master",
    "path": "reboot.py",
    "copies": "1",
    "content": "import serial;\nimport serial.tools.list_ports\nimport re\nimport sys\nimport time\n\nportName = None\n\nif (sys.platform.startswith(\"darwin\")):\n    ports = serial.tools.list_ports.comports()\n    cnt = []\n    for pn in ports:\n        #m = re.search(\"/dev/cu.usbmodem([0-9]+)\", pn[0])\n        m = re.search('USB VID:PID=([0-9ABCDEF]{3,4}):([0-9ABCDEF]{4})', pn[2], re.I)\n        if m != None:\n            if (m.group(1).upper() == \"4B4\" or m.group(1).upper() == \"04B4\") and m.group(2).upper() == \"F232\":\n                cnt.append(pn[0])\n    \n    if len(cnt) \u003e 0:\n        cnt.sort()\n        portName = cnt[len(cnt)-1]\n        print \"using port: \"+portName\n\nif portName != None:\n    port = serial.Serial(portName)\n    port.setDTR(True)\n    time.sleep(0.1)\n    port.setDTR(False)\nelse:\n    print \"Could not find any suited serial port\""
}
{
    "repo_name": "danmackinlay/branching_process",
    "ref": "refs/heads/master",
    "path": "dodo.py",
    "copies": "1",
    "content": "#! /usr/bin/env python\n\n\"\"\"\nSee http://pydoit.org/tasks.html\n\"\"\"\nfrom doit.action import CmdAction\nfrom doit.task import clean_targets\n\nimport sys\n\n\ndef gui_open_action(pth):\n    action = None\n    if sys.platform.startswith('linux'):\n        action = [\"xdg-open\", str(pth)]\n    elif sys.platform.startswith('darwin'):\n        action = [\"open\", str(pth)]\n    elif sys.platform.startswith('win'):\n        action = [\"start\", str(pth)]\n    return action\n\n\ndef _task_html(pth):\n    \"\"\"\n    see http://nbconvert.readthedocs.io/en/latest/usage.html\n    \"\"\"\n    return dict(\n        file_dep=[\n            'docs/{pth}.ipynb'.format(pth=pth),\n            'docs/html.tpl'.format(pth=pth),\n        ],\n        #    + [ str(p) for p in pathlib.Path('docs/ext_media').glob('*')],\n        targets=[\n            'docs/{pth}.html'.format(pth=pth),\n            # 'docs/refs.bib'.format(pth=pth)\n        ],\n        actions=[\n            # 'mkdir -p docs',\n            # 'ln -f docs/refs.bib docs/'.format(pth=pth),\n            'jupyter nbconvert --to html '\n            # '--template=docs/{pth}_html.tpl '\n            '--TemplateExporter.exclude_output_prompt=True '\n            '--FilesWriter.build_directory=docs/ '\n            'docs/{pth}.ipynb'.format(pth=pth),\n        ],\n        clean=[\n            # 'rm -rf docs/{pth}_files',\n            clean_targets,\n        ],\n    )\n\n\ndef _task_latex(pth):\n    \"\"\"\n    see http://nbconvert.readthedocs.io/en/latest/usage.html\n    \"\"\"\n    return dict(\n        file_dep=[\n            'docs/{pth}.ipynb'.format(pth=pth),\n            'docs/{pth}_print.tplx'.format(pth=pth),\n            'docs/refs.bib'.format(pth=pth),\n            # 'docs/ext_media/',\n        ],\n        targets=[\n            '_paper_output/{pth}.tex'.format(pth=pth),\n            '_paper_output/refs.bib'.format(pth=pth)\n        ],\n        actions=[\n            'mkdir -p _paper_output',\n            'rm -rf _paper_output/{pth}_files',\n            'ln -f docs/refs.bib _paper_output'.format(pth=pth),\n            'jupyter nbconvert --to latex --template=docs/{pth}_print.tplx '\n            '--FilesWriter.build_directory=_paper_output/ '\n            '--TemplateExporter.exclude_output_prompt=True '\n            'docs/{pth}.ipynb'.format(pth=pth),\n        ],\n        clean=[\n            'rm -rf _paper_output/{pth}_files',\n            clean_targets,\n        ],\n    )\n\n\ndef _task_pdf(pth):\n    \"\"\"\n    \"\"\"\n    return dict(\n        file_dep=[\n            '_paper_output/refs.bib'.format(pth=pth),\n            '_paper_output/{pth}.tex'.format(pth=pth)\n            ],\n        targets=[\n            '_paper_output/{pth}.pdf'.format(pth=pth),\n            '_paper_output/{pth}.aux'.format(pth=pth),\n            '_paper_output/{pth}.dvi'.format(pth=pth),\n            '_paper_output/{pth}.bcf'.format(pth=pth),\n            '_paper_output/{pth}.blg'.format(pth=pth),\n            '_paper_output/{pth}.bbl'.format(pth=pth),\n            '_paper_output/{pth}.run.xml'.format(pth=pth),\n            '_paper_output/texput.log',\n            '_paper_output/q.log',\n        ],\n        actions=[\n            CmdAction(\n                'pdflatex -halt-on-error -interaction=batchmode '\n                '{pth}'.format(pth=pth),\n                cwd='_paper_output'),\n            CmdAction(\n                'bibtex '\n                '{pth}'.format(pth=pth),\n                cwd='_paper_output'),\n            CmdAction(\n                'pdflatex -halt-on-error -interaction=batchmode '\n                '{pth}'.format(pth=pth),\n                cwd='_paper_output'),\n            CmdAction(\n                'pdflatex -halt-on-error -interaction=batchmode '\n                '{pth}'.format(pth=pth),\n                cwd='_paper_output'),\n        ],\n        verbosity=1,\n        clean=True,\n    )\n\n\ndef _task_view_pdf(pth):\n    \"\"\"\n    \"\"\"\n    return dict(\n        file_dep=['_paper_output/{pth}.pdf'.format(pth=pth)],\n        targets=[],\n        actions=[\n            gui_open_action('_paper_output/{pth}.pdf'.format(pth=pth)),\n        ],\n    )\n\n\ndef _task_zdravko(srcpth, destpth):\n    \"\"\"\n    \"\"\"\n    return dict(\n        file_dep=[\n            '_paper_output/{srcpth}.pdf'.format(srcpth=srcpth),\n            '_paper_output/{srcpth}.tex'.format(srcpth=srcpth),\n            '_paper_output/refs.bib'\n        ],\n        actions=[\n            'mkdir -p ~/Dropbox/dan-zdravko-stuff/tex/{destpth}/'.format(\n                destpth=destpth\n            ),\n            CmdAction(\n                'rsync -av {srcpth}_files '\n                'refs.bib {srcpth}.tex '\n                '{srcpth}.pdf'\n                ' ~/Dropbox/dan-zdravko-stuff/tex/{destpth}/'.format(\n                    srcpth=srcpth,\n                    destpth=destpth\n                ),\n                cwd='_paper_output'\n            ),\n        ],\n        verbosity=2\n    )\n\n\ndef task_latex_chapter_sparse_hawkes():\n    return _task_latex('chapter_sparse_hawkes')\n\n\ndef task_pdf_chapter_sparse_hawkes():\n    return _task_pdf('chapter_sparse_hawkes')\n\n\ndef task_view_pdf_chapter_sparse_hawkes():\n    return _task_view_pdf('chapter_sparse_hawkes')\n\n\ndef task_zdravko_chapter_sparse_hawkes():\n    return _task_zdravko('chapter_sparse_hawkes', 'sparse_hawkes')\n\n\ndef task_html_chapter_sparse_hawkes():\n    return _task_html('chapter_sparse_hawkes')\n\n\ndef task_html_intro_to_cts_hawkes():\n    return _task_html('intro_to_cts_hawkes')\n"
}
{
    "repo_name": "dcalacci/hubway-vis",
    "ref": "refs/heads/master",
    "path": "scripts/station_time_series.py",
    "copies": "1",
    "content": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pandas_utils\n\nstations = pd.read_csv('../data/stations_10_12_to_11_13.csv', \n                       index_col=0)\n\ntrips = pd.read_csv('../data/hubwaydata_10_12_to_11_13.csv',\n                    index_col=0,\n                    parse_dates=['start_date', 'end_date'])\n\n# sigma.js requres the node IDs to be strings.\nstations = stations.set_index(stations.index.map(str))\ntrips.start_station = trips.start_station.map(str)\ntrips.end_station = trips.end_station.map(str)\n\n\ndef get_trips_for_date(d):\n    start_trips = trips[trips.start_date == d].start_station\n    end_trips = trips[trips.end_date == d].end_station\n    return (list(start_trips),\n            list(end_trips))\n\n\nstart_date = sorted(trips.start_date)[0]\nend_date = sorted(trips.end_date)[-1]\ntimerange = pd.date_range(start_date, end_date, freq='min')\n\n\nstarting_ending = timerange.map(get_trips_for_date)\n\n# separate into two lists\nstarting, ending = zip(*starting_ending)\n\nres = pd.DataFrame({'date': timerange,\n\t\t\t\t\t'starting': starting,\n\t\t\t\t\t'ending': ending})\n\nres.to_csv('trip_timeseries.csv')"
}
{
    "repo_name": "ivanlyon/exercises",
    "ref": "refs/heads/master",
    "path": "test/test_k_multiplicationgame.py",
    "copies": "1",
    "content": "import io\nimport unittest\nfrom unittest.mock import patch\nfrom kattis import k_multiplicationgame\n\n###############################################################################\n\nclass SampleInput(unittest.TestCase):\n    '''Problem statement sample inputs and outputs'''\n\n    def test_sample_input(self):\n        '''Run and assert problem statement sample input and output.'''\n        inputs = []\n        inputs.append('10')\n        inputs.append('10 Alice')\n        inputs.append('20 Bob')\n        inputs.append('30 Alice')\n        inputs.append('40 Bob')\n        inputs.append('50 Alice')\n        inputs.append('60 Bob')\n        inputs.append('70 Alice')\n        inputs.append('80 Bob')\n        inputs.append('90 Alice')\n        inputs.append('100 Bob')\n        inputs = '\\n'.join(inputs) + '\\n'\n\n        outputs = []\n        outputs.append('Bob')\n        outputs.append('Bob')\n        outputs.append('tie')\n        outputs.append('tie')\n        outputs.append('Alice')\n        outputs.append('tie')\n        outputs.append('tie')\n        outputs.append('tie')\n        outputs.append('tie')\n        outputs.append('Alice')\n        outputs = '\\n'.join(outputs) + '\\n'\n\n        with patch('sys.stdin', io.StringIO(inputs)) as stdin,\\\n             patch('sys.stdout', new_callable=io.StringIO) as stdout:\n            k_multiplicationgame.main()\n            self.assertEqual(stdout.getvalue(), outputs)\n            self.assertEqual(stdin.read(), '')\n\n###############################################################################\n\nif __name__ == '__main__':\n    unittest.main()\n"
}
{
    "repo_name": "sauli6692/ibc-server",
    "ref": "refs/heads/master",
    "path": "rte/views/harvest.py",
    "copies": "1",
    "content": "from ..models import Harvest\nfrom ..serializers import HarvestSerializer\nfrom pmm.views import PersonViewSet, PersonFilter\n\n\nclass HarvestFilter(PersonFilter):\n    class Meta(PersonFilter.Meta):\n        model = Harvest\n        fields = PersonFilter.Meta.fields + ('route', 'discarded',)\n\n\nclass HarvestViewSet(PersonViewSet):\n    queryset = Harvest.objects.all()\n    filter_class = HarvestFilter\n    serializer_class = HarvestSerializer\n    search_fields = PersonViewSet.search_fields + ('route__name', 'discarded_reason',)\n"
}
{
    "repo_name": "moin18/utilspie",
    "ref": "refs/heads/master",
    "path": "utilspie/exceptions.py",
    "copies": "1",
    "content": "\nclass MethodCallNotAllowed(Exception):\n    pass\n\n"
}
{
    "repo_name": "fsxfreak/esys-pbi",
    "ref": "refs/heads/master",
    "path": "src/pupil/pupil_src/shared_modules/remote_recorder.py",
    "copies": "1",
    "content": "'''\n(*)~---------------------------------------------------------------------------\nPupil - eye tracking platform\nCopyright (C) 2012-2017  Pupil Labs\n\nDistributed under the terms of the GNU\nLesser General Public License (LGPL v3.0).\nSee COPYING and COPYING.LESSER for license details.\n---------------------------------------------------------------------------~(*)\n'''\n\nfrom time import strftime, localtime\nfrom pyglui import ui\nfrom plugin import Plugin\n\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass Remote_Recorder(Plugin):\n\n    order = .3\n    uniqueness = 'by_class'\n\n    def __init__(self, g_pool, session_name='Unnamed session'):\n        super().__init__(g_pool)\n        self.session_name = session_name\n        self.running = False\n        self.menu_toggle = None\n        self.quickbar_toggle = None\n        self.menu = None\n\n    def toggle_recording(self, *args, **kwargs):\n        if self.running:\n            self.stop()\n        else:\n            self.start()\n\n    def start(self):\n        if not self.running:\n            del self.menu[-1]\n            self.menu_toggle = ui.Button('Stop Recording', self.toggle_recording)\n            self.menu.append(self.menu_toggle)\n            unique_session_name = strftime(\"%Y%m%d%H%M%S\", localtime())\n            self.notify_all({\n                'subject': 'remote_recording.should_start',\n                'session_name': '{}/{}'.format(self.session_name, unique_session_name)})\n            self.running = True\n\n    def stop(self):\n        if self.running:\n            del self.menu[-1]\n            self.menu_toggle = ui.Button('Start Recording', self.toggle_recording)\n            self.menu.append(self.menu_toggle)\n            self.notify_all({'subject': 'remote_recording.should_stop'})\n            self.running = False\n\n    def on_notify(self, notification):\n        subject = notification['subject']\n        if subject == 'ndsi.host_recording.stopped' and self.running:\n            source = notification['source']\n            logger.warning('Recording on {} was stopped remotely. Stopping whole recording.'.format(source))\n            self.stop()\n\n    def init_gui(self):\n        self.menu = ui.Growing_Menu('Remote Recorder')\n        self.g_pool.sidebar.append(self.menu)\n        self.menu.append(ui.Button('Close', self.close))\n        self.menu.append(ui.Info_Text('Starts a recording session on each connected Pupil Mobile source.'))\n        self.menu.append(ui.Text_Input('session_name', self))\n        self.menu_toggle = ui.Button('Start Recording', self.toggle_recording)\n        # ↴: Unicode: U+21B4, UTF-8: E2 86 B4\n        self.quickbar_toggle = ui.Thumb('running', self, setter=self.toggle_recording,\n                                        label=chr(0xf03d), label_font='fontawesome',\n                                        label_offset_size=-30, hotkey='e')\n        self.quickbar_toggle.on_color[:] = (1, .0, .0, .8)\n\n        self.menu.append(self.menu_toggle)\n        self.g_pool.quickbar.append(self.quickbar_toggle)\n\n    def deinit_gui(self):\n        if self.menu:\n            self.g_pool.sidebar.remove(self.menu)\n            self.g_pool.quickbar.remove(self.quickbar_toggle)\n            self.menu = None\n            self.menu_toggle = None\n            self.quickbar_toggle = None\n\n    def close(self):\n        self.alive = False\n\n    def cleanup(self):\n        self.stop()\n        self.deinit_gui()\n\n    def get_init_dict(self):\n        return {'session_name': self.session_name}\n"
}
{
    "repo_name": "scottrice/Ice",
    "ref": "refs/heads/master",
    "path": "ice/paths.py",
    "copies": "1",
    "content": "# encoding: utf-8\n\nimport appdirs\nimport os\n\ndef application_data_directory():\n  # Parameters are 'App Name' and 'App Author'\n  # TODO: Get these values from the same place as setup.py\n  return appdirs.user_data_dir(\"Ice\", \"Scott Rice\")\n\ndef data_file_path(filename):\n  return os.path.join(application_data_directory(), filename)\n\ndef archive_path():\n  return data_file_path('archive.json')\n\ndef log_file_location():\n  return data_file_path('ice.log')\n\ndef default_roms_directory():\n  return os.path.join(os.path.expanduser('~'), 'ROMs')\n"
}
{
    "repo_name": "achm6174/kaggle-physics-tau",
    "ref": "refs/heads/master",
    "path": "semi_strong_model/semi_strong_ensemble.py",
    "copies": "1",
    "content": "\"\"\"\r\n@author: achm\r\n\r\nFinding best ensemble of strong models using grid search, based on evaluation sets\r\n\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\r\nimport sys\r\nimport cPickle\r\nimport time\r\nimport copy\r\nfrom sklearn.metrics import log_loss\r\nimport glob\r\nimport os\r\nfrom sklearn.preprocessing import StandardScaler\r\nimport xgboost as xgb\r\n\r\nsys.path.append('../input')\r\nimport evaluation\r\n\r\nsys.setrecursionlimit(100000000)\r\n\r\n\r\ntrain_table = {}\r\ntrain_eval_table = {}\r\ntrain_eval_score = {}\r\n\r\n# Load training and train evaluation data and store it into dict\r\nprint(\"Load the training/training_eval data using pandas\")\r\nfor i in range(0,5):\r\n    print i\r\n    train_table[i] = pd.read_csv(\"./input/training_%i.csv\" %i)\r\n    train_eval_table[i] = pd.read_csv(\"./input/training_eval_%i.csv\" %i)\r\n    train_eval_score[i] = 0\r\n\r\n# Indicate which features to use\r\nfeatures = list(train_table[0].columns[1:-5])\r\n\r\n# Load test data\r\nprint(\"Load test data using pandas\")\r\ntest = pd.read_csv('../input/test.csv')\r\n\r\nprint time.localtime()\r\nks_cutoff =0.09\r\n\r\nfor i in range(0,5):\r\n    best_xs = []\r\n    print \"### %i ###\" %i\r\n    # read Keras model\r\n    search_dir = \"./model/final_keras_%i/new_\" %i\r\n    files = filter(os.path.isfile, glob.glob(search_dir + \"*\"))\r\n    files.sort(key=lambda x: os.path.getmtime(x))\r\n\r\n    print files[-1]\r\n    fh = open(files[-1],\"rb\")\r\n    keras_model = cPickle.load(fh)\r\n    fh.close()\r\n\r\n    scaler = StandardScaler()\r\n    scaler.fit(np.array(train_table[i][features]))\r\n\r\n    keras_pred = keras_model.predict(scaler.transform(np.array(train_eval_table[i][features])), batch_size=256)[:,1]\r\n    keras_test = keras_model.predict(scaler.transform(np.array(test[features])), batch_size=256)[:,1]\r\n\r\n    # read xgboost model\r\n    search_dir = \"./model/final_gbm_%i/new_\" %i\r\n    files = filter(os.path.isfile, glob.glob(search_dir + \"*\"))\r\n    files.sort(key=lambda x: os.path.getmtime(x))\r\n\r\n    print files[-1]\r\n    fh = open(files[-1],\"rb\")\r\n    gbm_model = cPickle.load(fh)\r\n    fh.close()\r\n\r\n    scaler = StandardScaler()\r\n    scaler.fit(np.array(train_table[i][features]))\r\n\r\n    gbm_pred = gbm_model.predict(xgb.DMatrix(scaler.transform(np.array(train_eval_table[i][features]))))\r\n    gbm_test = gbm_model.predict(xgb.DMatrix(scaler.transform(np.array(test[features]))))\r\n\r\n\r\n    # Grid search to compute best score\r\n    def multichoose(n,k):\r\n        if k \u003c 0 or n \u003c 0: return \"Error\"\r\n        if not k: return [[0]*n]\r\n        if not n: return []\r\n        if n == 1: return [[k]]\r\n        return [[0]+val for val in multichoose(n-1,k)] + \\\r\n            [[val[0]+1]+val[1:] for val in multichoose(n,k-1)]\r\n\r\n    n = 2\r\n    k = 1000\r\n    for xs in multichoose(n,k):\r\n        #print xs\r\n        preds = (xs[0]*keras_pred + xs[1]*gbm_pred)/float(k)\r\n        score = evaluation.roc_auc_truncated(train_eval_table[i]['signal'], preds)\r\n        if score\u003e=train_eval_score[i]:\r\n            train_eval_score[i] = score\r\n            print score\r\n            best_xs = xs\r\n\r\n    print train_eval_score[i]\r\n    print best_xs\r\n    test[\"prediction_%i\" %i] = (best_xs[0]*keras_test + best_xs[1]*gbm_test)/float(k)\r\n\r\n    with open('./output/semi_strong_submission_%i.csv' %i, 'w') as f:\r\n        f.write('id,prediction\\n')\r\n        for ID, p in zip(test['id'], test[\"prediction_%i\" %i]):\r\n            f.write('%s,%.8f\\n' % (ID, p))\r\n\r\n    # Save best combination weight\r\n    temp_file_name = \"./output/best_xs_%i\" %i\r\n    fh = open(temp_file_name, \"wb\")\r\n    cPickle.dump(best_xs,fh)\r\n    fh.close()\r\n\r\n    # Save best score\r\n    temp_file_name = \"./output/best_score_%i\" %i\r\n    fh = open(temp_file_name, \"wb\")\r\n    cPickle.dump(train_eval_score[i],fh)\r\n    fh.close()\r\n\r\nensemble_prediction = (test[\"prediction_0\"] + test[\"prediction_1\"] + test[\"prediction_2\"] + test[\"prediction_3\"] + test[\"prediction_4\"])/5.\r\nwith open('./output/semi_strong_ensemble_submission.csv', 'w') as f:\r\n    f.write('id,prediction\\n')\r\n    for ID, p in zip(test['id'], ensemble_prediction):\r\n        f.write('%s,%.8f\\n' % (ID, p))\r\n"
}
{
    "repo_name": "TheVGLC/TheVGLC",
    "ref": "refs/heads/master",
    "path": "WADParser/WADRasterizer.py",
    "copies": "1",
    "content": "'''\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the \"Software\"),\nto deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\nNO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\nOTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\nUSE OR OTHER DEALINGS IN THE SOFTWARE.\n'''\nimport struct\nfrom linedef import decode\nimport re\nbit2object = {1:\"Player start #1\", 2:\"Player start #2\", 3:\"Player start #3\", 4:\"Player start #4\", 5:\"BlueCard\", 6:\"YellowCard\", 7:\"SpiderMastermind\", 8:\"Backpack\", 9:\"ShotgunGuy\", 10:\"GibbedMarine\", 11:\"Deathmatch start\", 12:\"GibbedMarineExtra\", 13:\"RedCard\", 14:\"TeleportDest\", 15:\"DeadMarine\", 16:\"Cyberdemon\", 17:\"CellPack\", 18:\"DeadZombieMan\", 19:\"DeadShotgunGuy\", 20:\"DeadDoomImp\", 21:\"DeadDemon\", 22:\"DeadCacodemon\", 23:\"DeadLostSoul\", 24:\"Gibs\", 25:\"DeadStick\", 26:\"LiveStick\", 27:\"HeadOnAStick\", 28:\"HeadsOnAStick\", 29:\"HeadCandles\", 30:\"TallGreenColumn\", 31:\"ShortGreenColumn\", 32:\"TallRedColumn\", 33:\"ShortRedColumn\", 34:\"Candlestick\", 35:\"Candelabra\", 36:\"HeartColumn\", 37:\"SkullColumn\", 38:\"RedSkull\", 39:\"YellowSkull\", 40:\"BlueSkull\", 41:\"EvilEye\", 42:\"FloatingSkull\", 43:\"TorchTree\", 44:\"BlueTorch\", 45:\"GreenTorch\", 46:\"RedTorch\", 47:\"Stalagtite\", 48:\"TechPillar\", 49:\"BloodyTwitch\", 50:\"Meat2\", 51:\"Meat3\", 52:\"Meat4\", 53:\"Meat5\", 54:\"BigTree\", 55:\"ShortBlueTorch\", 56:\"ShortGreenTorch\", 57:\"ShortRedTorch\", 58:\"Spectre\", 59:\"NonsolidMeat2\", 60:\"NonsolidMeat4\", 61:\"NonsolidMeat3\", 62:\"NonsolidMeat5\", 63:\"NonsolidTwitch\", 118:\"ZBridge\", 888:\"MBFHelperDog\", 1400:\"Sound sequence thing (0)\", 1401:\"Sound sequence thing (1)\", 1402:\"Sound sequence thing (2)\", 1403:\"Sound sequence thing (3)\", 1404:\"Sound sequence thing (4)\", 1405:\"Sound sequence thing (5)\", 1406:\"Sound sequence thing (6)\", 1407:\"Sound sequence thing (7)\", 1408:\"Sound sequence thing (8)\", 1409:\"Sound sequence thing (9)\", 1411:\"Sound sequence thing (arg)\", 1500:\"Floor slope\", 1501:\"Ceiling slope\", 1504:\"Floor vertex height\", 1505:\"Ceiling vertex height\", 2001:\"Shotgun\", 2002:\"Chaingun\", 2003:\"RocketLauncher\", 2004:\"PlasmaRifle\", 2005:\"Chainsaw\", 2006:\"BFG9000\", 2007:\"Clip\", 2008:\"Shell\", 2010:\"RocketAmmo\", 2011:\"Stimpack\", 2012:\"Medikit\", 2013:\"Soulsphere\", 2014:\"HealthBonus\", 2015:\"ArmorBonus\", 2018:\"GreenArmor\", 2019:\"BlueArmor\", 2022:\"InvulnerabilitySphere\", 2023:\"Berserk\", 2024:\"BlurSphere\", 2025:\"RadSuit\", 2026:\"Allmap\", 2028:\"Column\", 2035:\"ExplosiveBarrel\", 2045:\"Infrared\", 2046:\"RocketBox\", 2047:\"Cell\", 2048:\"ClipBox\", 2049:\"ShellBox\", 3001:\"DoomImp\", 3002:\"Demon\", 3003:\"BaronOfHell\", 3004:\"ZombieMan\", 3005:\"Cacodemon\", 3006:\"LostSoul\", 4001:\"Player start #5\", 4002:\"Player start #6\", 4003:\"Player start #7\", 4004:\"Player start #8\", 5001:\"PointPusher\", 5002:\"PointPuller\", 5004:\"FS_Mapspot\", 5006:\"SkyCamCompat\", 5010:\"Pistol\", 5050:\"Stalagmite\", 5061:\"InvisibleBridge32\", 5064:\"InvisibleBridge16\", 5065:\"InvisibleBridge8\", 9001:\"MapSpot\", 9013:\"MapSpotGravity\", 9024:\"PatrolPoint\", 9025:\"SecurityCamera\", 9026:\"Spark\", 9027:\"RedParticleFountain\", 9028:\"GreenParticleFountain\", 9029:\"BlueParticleFountain\", 9030:\"YellowParticleFountain\", 9031:\"PurpleParticleFountain\", 9032:\"BlackParticleFountain\", 9033:\"WhiteParticleFountain\", 9037:\"BetaSkull\", 9038:\"ColorSetter\", 9039:\"FadeSetter\", 9040:\"MapMarker\", 9041:\"SectorFlagSetter\", 9043:\"TeleportDest3\", 9044:\"TeleportDest2\", 9045:\"WaterZone\", 9046:\"SecretTrigger\", 9047:\"PatrolSpecial\", 9048:\"SoundEnvironment\", 9052:\"StealthBaron\", 9053:\"StealthCacodemon\", 9055:\"StealthDemon\", 9057:\"StealthDoomImp\", 9060:\"StealthShotgunGuy\", 9061:\"StealthZombieMan\", 9070:\"InterpolationPoint\", 9071:\"PathFollower\", 9072:\"MovingCamera\", 9073:\"AimingCamera\", 9074:\"ActorMover\", 9075:\"InterpolationSpecial\", 9076:\"HateTarget\", 9077:\"UpperStackLookOnly\", 9078:\"LowerStackLookOnly\", 9080:\"SkyViewpoint\", 9081:\"SkyPicker\", 9082:\"SectorSilencer\", 9083:\"SkyCamCompat\", 9100:\"ScriptedMarine\", 9101:\"MarineFist\", 9102:\"MarineBerserk\", 9103:\"MarineChainsaw\", 9104:\"MarinePistol\", 9105:\"MarineShotgun\", 9106:\"MarineSSG\", 9107:\"MarineChaingun\", 9108:\"MarineRocket\", 9109:\"MarinePlasma\", 9110:\"MarineRailgun\", 9111:\"MarineBFG\", 9200:\"Decal\", 9300:\"PolyObject anchor\", 9301:\"PolyObject start spot (harmless)\", 9302:\"PolyObject start spot (crushing)\", 9303:\"PolyObject start spot (harmful)\", 9500:\"Floor line\", 9501:\"Ceiling line\", 9502:\"Floor tilt\", 9503:\"Ceiling tilt\", 9510:\"Copy floor slope\", 9511:\"Copy ceiling slope\", 9982:\"SecActEyesAboveC\", 9983:\"SecActEyesBelowC\", 9988:\"CustomSprite\", 9989:\"SecActHitFakeFloor\", 9990:\"InvisibleBridge\", 9991:\"CustomBridge\", 9992:\"SecActEyesSurface\", 9993:\"SecActEyesDive\", 9994:\"SecActUseWall\", 9995:\"SecActUse\", 9996:\"SecActHitCeil\", 9997:\"SecActExit\", 9998:\"SecActEnter\", 9999:\"SecActHitFloor\", 14001:\"AmbientSound\", 14002:\"AmbientSound\", 14003:\"AmbientSound\", 14004:\"AmbientSound\", 14005:\"AmbientSound\", 14006:\"AmbientSound\", 14007:\"AmbientSound\", 14008:\"AmbientSound\", 14009:\"AmbientSound\", 14010:\"AmbientSound\", 14011:\"AmbientSound\", 14012:\"AmbientSound\", 14013:\"AmbientSound\", 14014:\"AmbientSound\", 14015:\"AmbientSound\", 14016:\"AmbientSound\", 14017:\"AmbientSound\", 14018:\"AmbientSound\", 14019:\"AmbientSound\", 14020:\"AmbientSound\", 14021:\"AmbientSound\", 14022:\"AmbientSound\", 14023:\"AmbientSound\", 14024:\"AmbientSound\", 14025:\"AmbientSound\", 14026:\"AmbientSound\", 14027:\"AmbientSound\", 14028:\"AmbientSound\", 14029:\"AmbientSound\", 14030:\"AmbientSound\", 14031:\"AmbientSound\", 14032:\"AmbientSound\", 14033:\"AmbientSound\", 14034:\"AmbientSound\", 14035:\"AmbientSound\", 14036:\"AmbientSound\", 14037:\"AmbientSound\", 14038:\"AmbientSound\", 14039:\"AmbientSound\", 14040:\"AmbientSound\", 14041:\"AmbientSound\", 14042:\"AmbientSound\", 14043:\"AmbientSound\", 14044:\"AmbientSound\", 14045:\"AmbientSound\", 14046:\"AmbientSound\", 14047:\"AmbientSound\", 14048:\"AmbientSound\", 14049:\"AmbientSound\", 14050:\"AmbientSound\", 14051:\"AmbientSound\", 14052:\"AmbientSound\", 14053:\"AmbientSound\", 14054:\"AmbientSound\", 14055:\"AmbientSound\", 14056:\"AmbientSound\", 14057:\"AmbientSound\", 14058:\"AmbientSound\", 14059:\"AmbientSound\", 14060:\"AmbientSound\", 14061:\"AmbientSound\", 14062:\"AmbientSound\", 14063:\"AmbientSound\", 14064:\"AmbientSound\", 14065:\"AmbientSound\", 14066:\"SoundSequence\", 14067:\"AmbientSoundNoGravity\", 14101:\"MusicChanger\", 14102:\"MusicChanger\", 14103:\"MusicChanger\", 14104:\"MusicChanger\", 14105:\"MusicChanger\", 14106:\"MusicChanger\", 14107:\"MusicChanger\", 14108:\"MusicChanger\", 14109:\"MusicChanger\", 14110:\"MusicChanger\", 14111:\"MusicChanger\", 14112:\"MusicChanger\", 14113:\"MusicChanger\", 14114:\"MusicChanger\", 14115:\"MusicChanger\", 14116:\"MusicChanger\", 14117:\"MusicChanger\", 14118:\"MusicChanger\", 14119:\"MusicChanger\", 14120:\"MusicChanger\", 14121:\"MusicChanger\", 14122:\"MusicChanger\", 14123:\"MusicChanger\", 14124:\"MusicChanger\", 14125:\"MusicChanger\", 14126:\"MusicChanger\", 14127:\"MusicChanger\", 14128:\"MusicChanger\", 14129:\"MusicChanger\", 14130:\"MusicChanger\", 14131:\"MusicChanger\", 14132:\"MusicChanger\", 14133:\"MusicChanger\", 14134:\"MusicChanger\", 14135:\"MusicChanger\", 14136:\"MusicChanger\", 14137:\"MusicChanger\", 14138:\"MusicChanger\", 14139:\"MusicChanger\", 14140:\"MusicChanger\", 14141:\"MusicChanger\", 14142:\"MusicChanger\", 14143:\"MusicChanger\", 14144:\"MusicChanger\", 14145:\"MusicChanger\", 14146:\"MusicChanger\", 14147:\"MusicChanger\", 14148:\"MusicChanger\", 14149:\"MusicChanger\", 14150:\"MusicChanger\", 14151:\"MusicChanger\", 14152:\"MusicChanger\", 14153:\"MusicChanger\", 14154:\"MusicChanger\", 14155:\"MusicChanger\", 14156:\"MusicChanger\", 14157:\"MusicChanger\", 14158:\"MusicChanger\", 14159:\"MusicChanger\", 14160:\"MusicChanger\", 14161:\"MusicChanger\", 14162:\"MusicChanger\", 14163:\"MusicChanger\", 14164:\"MusicChanger\", 14165:\"MusicChanger\", 32000:\"DoomBuilderCamera\",64:'Archvile',65:'ChaingunGuy',66:'Revenant',67:'Fatso',68:'Arachnotron',69:'HellKnight',70:'BurningBarrel',71:'PainElemental',72:'CommanderKeen',73:'HangNoGuts',74:'HangBNoBrain',75:'HangTLookingDown',76:'HangTSkull',77:'HangTLookingUp',78:'HangTNoBrain',79:'ColonGibs',80:'SmallBloodPool',81:'BrainStem',82:'SuperShotgun',83:'Megasphere',84:'WolfensteinSS',85:'TechLamp',86:'TechLamp2:',87:'BossTarget',88:'BossBrain',89:'BossEye',9050:'StealthArachnotron',9051:'StealthArchvile',9054:'StealthChaingunGuy',9056:'StealthHellKnight',9058:'StealthFatso',9059:'StealthRevenant',}\n\n\nenemy=['SpiderMastermind','ShotgunGuy','Cyberdemon','DoomImp','Demon','BaronOfHell','ZombieMan','Cacodemon','LostSoul','StealthBaron','StealthCacodemon','StealthDemon','StealthDoomImp','StealthShotgunGuy','StealthZombieMan','MarineFist','MarineBerserk','MarineChainsaw','MarinePistol','MarineShotgun','MarineSSG','MarineChaingun','MarineRocket','MarinePlasma','MarineRailgun','MarineBFG','ScriptedMarine','Archvile','ChaingunGuy','Revenant','Fatso','Arachnotron','HellKnight','BossTarget','BossBrain','BossEye','StealthArachnotron','StealthArchvile','StealthChaingunGuy','StealthHellKnight','StealthFatso','StealthRevenant','PainElemental','Spectre']\n\nweapon= ['Shotgun','Chaingun','RocketLauncher','PlasmaRifle','Chainsaw','BFG9000','Pistol','SuperShotgun']\n\n\n\n\nammo = ['Clip','Shell','RocketAmmo','RocketBox','Cell','ClipBox','ShellBox','CellPack']\n\nhealth = ['Stimpack','Medikit','Soulsphere','HealthBonus','ArmorBonus','GreenArmor','BlueArmor','InvulnerabilitySphere','Berserk','BlurSphere','RadSuit','Allmap','Infrared','Megasphere']\n\nenvironmental = ['ExplosiveBarrel']\n\nid2char = {0:'-',\n           1:'X',\n           2:'.',\n           3:',',\n           4:'E',\n           5:'W',\n           6:'A',\n           7:'H',\n           8:'B',\n           9:'K',\n           10:'\u003c',\n           11:'T',\n           12:':',\n           13:'L',\n           14:'t',\n           15:'+',\n           16:'\u003e',            \n            }\n\nclass Wad(object):\n    \"\"\"Encapsulates the data found inside a WAD file\"\"\"\n\n    def __init__(self, wadFile):\n        \"\"\"Each WAD files contains definitions for global attributes as well as map level attributes\"\"\"\n        self.levels = []\n        self.wad_format = 'DOOM' #Assume DOOM format unless 'BEHAVIOR' \n        with open(wadFile, \"rb\") as f:\n            header_size = 12\n            self.wad_type = f.read(4)[0]\n            self.num_lumps = struct.unpack(\"\u003cI\", f.read(4))[0]\n            data = f.read(struct.unpack(\"\u003cI\", f.read(4))[0] - header_size)\n\n            current_level = Level(None) #The first few records of a WAD are not associated with a level\n\n            lump = f.read(16) #Each offset is is part of a packet 16 bytes\n            while len(lump) == 16:\n                filepos = struct.unpack(\"\u003cI\", lump[0:4])[0] - header_size\n                size = struct.unpack(\"\u003cI\", lump[4:8])[0]\n                name = lump[8:16].decode('UTF-8').rstrip('\\0')\n                #print(name)\n                if(re.match('E\\dM\\d|MAP\\d\\d', name)):\n                    #Level nodes are named things like E1M1 or MAP01\n                    if(current_level.is_valid()):\n                        self.levels.append(current_level)\n                    \n                    current_level = Level(name)\n                elif name == 'BEHAVIOR':\n                    #This node only appears in Hexen formated WADs\n                    self.wad_format = 'HEXEN'\n                else:\n                    current_level.lumps[name] = data[filepos:filepos+size]\n\n                lump = f.read(16)\n            if(current_level.is_valid()):\n                self.levels.append(current_level)\n\n        for level in self.levels:\n            level.load(self.wad_format)\n\nclass Level(object):\n    \"\"\"Represents a level inside a WAD which is a collection of lumps\"\"\"\n    def __init__(self, name):\n        self.name = name\n        self.lumps = dict()\n        self.vertices = []\n        self.lower_left = None\n        self.upper_right = None\n        self.shift = None\n        self.lines = []\n        self.objects = []\n\n    def is_valid(self):\n        return self.name is not None and 'VERTEXES' in self.lumps and 'LINEDEFS' in self.lumps\n\n    def normalize(self, point, padding=5):\n        return (int(self.shift[0]+point[0]+padding),int(self.shift[1]+point[1]+padding))\n\n    def load(self, wad_format):\n        for vertex in packets_of_size(4, self.lumps['VERTEXES']):\n            x,y = struct.unpack('\u003chh', vertex[0:4])\n            self.vertices.append((x,y))\n\n        self.lower_left = (min((v[0] for v in self.vertices)), min((v[1] for v in self.vertices)))\n        self.upper_right = (max((v[0] for v in self.vertices)), max((v[1] for v in self.vertices)))\n\n        self.shift = (0-self.lower_left[0],0-self.lower_left[1])\n        self.midpt = ( (self.upper_right[0]+self.lower_left[0])*0.5+self.shift[0], (self.upper_right[1]+self.lower_left[1])*0.5+self.shift[1])\n        \n        packet_size = 16 if wad_format is 'HEXEN' else 14\n        for data in packets_of_size(packet_size, self.lumps['LINEDEFS']):\n            self.lines.append(Line(data))\n        \n        \n        packet_size = 20 if wad_format is 'HEXEN' else 10\n        for data in packets_of_size(packet_size, self.lumps['THINGS']):\n            x,y,angle,type,spawn_flag = struct.unpack('\u003chhHHH',data)\n            type = bit2object[type]\n            self.objects.append( (x,y,angle,type,spawn_flag))\n     \n    def rotate(self,pt,midpt,angle):\n        import math\n        cosTheta = math.cos(angle*math.pi/180.0)\n        sinTheta = math.sin(angle*math.pi/180.0)\n        px = pt[0] - midpt[0]\n        py = pt[1] - midpt[1]\n        px_ = px*cosTheta - py*sinTheta\n        py_ = px*sinTheta + py*cosTheta\n        return (px_+midpt[0],py_+midpt[1])\n    def get_line(self,start, end):\n        \"\"\"Bresenham's Line Algorithm\n        Produces a list of tuples from start and end\n     \n        \u003e\u003e\u003e points1 = get_line((0, 0), (3, 4))\n        \u003e\u003e\u003e points2 = get_line((3, 4), (0, 0))\n        \u003e\u003e\u003e assert(set(points1) == set(points2))\n        \u003e\u003e\u003e print points1\n        [(0, 0), (1, 1), (1, 2), (2, 3), (3, 4)]\n        \u003e\u003e\u003e print points2\n        [(3, 4), (2, 3), (1, 2), (1, 1), (0, 0)]\n        \"\"\"\n        # Setup initial conditions\n        x1, y1 = start\n        x2, y2 = end\n        dx = x2 - x1\n        dy = y2 - y1\n     \n        # Determine how steep the line is\n        is_steep = abs(dy) \u003e abs(dx)\n     \n        # Rotate line\n        if is_steep:\n            x1, y1 = y1, x1\n            x2, y2 = y2, x2\n     \n        # Swap start and end points if necessary and store swap state\n        swapped = False\n        if x1 \u003e x2:\n            x1, x2 = x2, x1\n            y1, y2 = y2, y1\n            swapped = True\n     \n        # Recalculate differentials\n        dx = x2 - x1\n        dy = y2 - y1\n     \n        # Calculate error\n        error = int(dx / 2.0)\n        ystep = 1 if y1 \u003c y2 else -1\n     \n        # Iterate over bounding box generating points between start and end\n        y = y1\n        points = []\n        for x in range(x1, x2 + 1):\n            coord = (y, x) if is_steep else (x, y)\n            points.append(coord)\n            error -= abs(dy)\n            if error \u003c 0:\n                y += ystep\n                error += dx\n     \n        # Reverse the list if the coordinates were swapped\n        if swapped:\n            points.reverse()\n        return points\n    def fill(self, data, x_start, y_start,find=0,replace=2):\n\n        stack = [(x_start, y_start)]\n        seen = set()\n        while stack:\n            x, y = stack.pop(0)\n            if data[x, y] == find:\n                data[x, y] = replace\n                if x \u003e 0:\n                    if ((x-1,y) not in seen):\n                        stack.append((x - 1, y))\n                        seen.add(stack[-1])\n                if x \u003c (data.shape[0] - 1):\n                    if ((x+1,y) not in seen):\n                        stack.append((x + 1, y))\n                        seen.add(stack[-1])\n                if y \u003e 0:\n                    if ((x,y-1) not in seen):\n                        stack.append((x, y - 1))\n                        seen.add(stack[-1])\n                if y \u003c (data.shape[1] - 1):\n                    if ((x,y+1) not in seen):\n                        stack.append((x, y + 1))\n                        seen.add(stack[-1])\n    def scale(self,pt,scaleval):\n        return (int(pt[0]*scaleval),int(pt[1]*scaleval))\n    def rasterize(self,scaling=1.0/32.0):\n        view_box_size = self.scale(self.normalize(self.upper_right, 10),scaling)\n        if view_box_size[0] \u003e view_box_size[1]:\n            canvas_size = (1024, int(1024*(float(view_box_size[1])/view_box_size[0])))\n        else:\n            canvas_size = (int(1024*(float(view_box_size[0])/view_box_size[1])), 1024)\n        import numpy as np\n        output = np.zeros([view_box_size[0]+1,view_box_size[1]+1])\n        for line in self.lines:\n            \n            if line.is_one_sided():\n                sys.stdout.flush()\n                a = self.scale(self.normalize(self.vertices[line.a]),scaling)\n                b = self.scale(self.normalize(self.vertices[line.b]),scaling)\n                pts = self.get_line(a,b)\n                for pt in pts:\n                    if line.locked:\n                        output[pt[0],pt[1]] = 13\n                    elif line.teleport:\n                        output[pt[0],pt[1]] = 14\n                    elif line.door:\n                        output[pt[0],pt[1]] = 15\n                    elif line.exit:\n                        output[pt[0],pt[1]] = 16                    \n                    else:\n                        output[pt[0],pt[1]] = 1\n        for obj in self.objects:\n            x,y,angle,type,spawn = obj\n            (x,y) = self.scale(self.normalize((x,y)),scaling)\n            self.fill(output, x, y,find=0,replace=2)\n        \n        for line in self.lines:\n            if not line.is_one_sided():\n                a =  self.scale(self.normalize(self.vertices[line.a]),scaling)\n                b =  self.scale(self.normalize(self.vertices[line.b]),scaling)\n                pts = self.get_line(a,b)\n                for pt in pts:\n                    if line.locked:\n                        output[pt[0],pt[1]] = 13\n                    elif line.teleport:\n                        output[pt[0],pt[1]] = 14\n                    elif line.door:\n                        output[pt[0],pt[1]] = 15\n                    elif line.exit:\n                        output[pt[0],pt[1]] = 16                    \n                    else:\n                        output[pt[0],pt[1]] = 3\n        \n        for line in self.lines:\n            \n            if line.is_one_sided():\n                sys.stdout.flush()\n                a = self.scale(self.normalize(self.vertices[line.a]),scaling)\n                b = self.scale(self.normalize(self.vertices[line.b]),scaling)\n                pts = self.get_line(a,b)\n                for pt in pts:\n                    if line.locked:\n                        output[pt[0],pt[1]] = 13\n                    elif line.teleport:\n                        output[pt[0],pt[1]] = 14\n                    elif line.door:\n                        output[pt[0],pt[1]] = 15\n                    elif line.exit:\n                        output[pt[0],pt[1]] = 16                    \n                    else:\n                        output[pt[0],pt[1]] = 1\n        for obj in self.objects:\n            x,y,angle,type,spawn = obj\n            (x,y) = self.scale(self.normalize((x,y)),scaling)\n            if type in enemy:\n                output[x,y] = 4\n            elif type in weapon:\n                output[x,y] = 5\n            \n            elif type in ammo:\n                output[x,y] = 6\n            \n            elif type in health:\n                output[x,y] = 7\n            \n            elif type in environmental:\n                output[x,y] = 8              \n            \n            elif 'Card' in type or ('Skull' in type and 'Floating' not in type):\n                output[x,y] = 9         \n            \n            elif 'Player start' in type:\n                output[x,y] = 10\n            elif 'TeleportDest' in type:\n                output[x,y] = 11                \n            else :\n                output[x,y] = 12\n        #import matplotlib.pyplot as plt\n        with open(self.name+'.txt','wb') as outfile:\n            for yy in range(view_box_size[0]+1):\n                str = ''\n                for xx in range(view_box_size[1]+1):\n                    str += id2char[output[yy,xx]]\n                outfile.write(str + '\\n')\n        \n            \n        '''\n        imgplot = plt.imshow(output)\n        plt.show()\n        '''\n      \n\nclass Line(object):\n    \"\"\"Represents a Linedef inside a WAD\"\"\"\n    def __init__(self,data):\n        self.a, self.b, self.line_flags, self.line_type, self.sector,self.left_side, self.right_side  = struct.unpack('\u003chhhhhhh', data[0:14])\n        self.line_type = decode(self.line_type)\n        self.door =  'DOOR' in self.line_type\n        self.exit =  'EXIT' in self.line_type\n        self.teleport =  'TELEPORT' in self.line_type\n        self.locked = 'BLU' in self.line_type or 'RED' in self.line_type or 'YEL' in self.line_type\n        \n\n    def is_one_sided(self):\n        return self.left_side == -1 or self.right_side == -1\n\ndef packets_of_size(n, data):\n    size = len(data)\n    index = 0\n    while index \u003c size:\n        yield data[index : index+n]\n        index = index + n\n    return\n\nif __name__ == \"__main__\":\n    import sys\n\n    if len(sys.argv) \u003e 1:\n        wad = Wad(sys.argv[1])\n        for level in wad.levels:\n            level.rasterize()\n    else:\n        print('You need to pass a WAD file as the only argument')"
}
{
    "repo_name": "spantons/attacks-pages-collector",
    "ref": "refs/heads/master",
    "path": "collectors/nothink.py",
    "copies": "1",
    "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport socket\nimport re\nimport requests\nimport ipwhois\nfrom pprint import pprint\n\n\ndef get_url(url):\n    try:\n        res = requests.get(url)\n    except requests.exceptions.ConnectionError:\n        raise requests.exceptions.ConnectionError(\"DNS lookup failures\")\n    else:\n        if res.status_code != 200:\n            raise requests.exceptions.ConnectionError(\n                \"the {}, answer with {} error\".format(url, res.status_code))\n\n        return res\n\n\ndef get_host(ip):\n    attempts = 5\n    host = \"undefined\"\n    while attempts:\n        try:\n            data = socket.gethostbyaddr(ip)\n            host = data[0]\n            break\n        except socket.herror:\n            attempts -= 1\n\n    return host\n\n\ndef get_who_is_and_country(ip):\n    try:\n        ip_obj = ipwhois.IPWhois(ip)\n        who_is = ip_obj.lookup(retry_count=5)\n        return str(who_is), who_is['asn_country_code']\n    except ipwhois.exceptions.IPDefinedError:\n        return \"Private-Use Networks\", \"undefined\"\n    except ipwhois.exceptions.WhoisLookupError:\n        return \"undefined\", \"undefined\"\n\n\ndef gather():\n    base_url = \"http://www.nothink.org/blacklist/blacklist_malware_http.txt\"\n    attack_type = \"Malware IPs\"\n\n    res = get_url(base_url)\n    for line in res.iter_lines():\n        ip = re.findall(r'[0-9]+(?:\\.[0-9]+){3}', line)\n        if len(ip) == 0:\n            continue\n\n        ip_address = ip[0]\n        host = get_host(ip_address)\n        who_is, country = get_who_is_and_country(ip_address)\n\n        doc = {\n            'IP': ip_address,\n            'SourceInfo': base_url,\n            'Type': attack_type,\n            'Country': country,\n            'Domain': host,\n            'URL': host,\n            'WhoIsInfo': who_is,\n        }\n\n        pprint(doc)\n\nif __name__ == '__main__':\n    gather()\n"
}
{
    "repo_name": "cloudboss/friend",
    "ref": "refs/heads/master",
    "path": "friend/net.py",
    "copies": "1",
    "content": "# Copyright 2018 Joseph Wright \u003cjoseph@cloudboss.co\u003e\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\nimport random\n\nimport ipaddress\n\n\ndef random_ipv4(cidr='10.0.0.0/8'):\n    \"\"\"\n    Return a random IPv4 address from the given CIDR block.\n\n    :key str cidr: CIDR block\n    :returns: An IPv4 address from the given CIDR block\n    :rtype: ipaddress.IPv4Address\n    \"\"\"\n    try:\n        u_cidr = unicode(cidr)\n    except NameError:\n        u_cidr = cidr\n    network = ipaddress.ip_network(u_cidr)\n    start = int(network.network_address) + 1\n    end = int(network.broadcast_address)\n    randint = random.randrange(start, end)\n    return ipaddress.ip_address(randint)\n"
}
{
    "repo_name": "on-three/weeabot",
    "ref": "refs/heads/master",
    "path": "weeabot/youtube.py",
    "copies": "1",
    "content": "# vim: set ts=2 expandtab:\n# -*- coding: utf-8 -*-\n\"\"\"\n\nModule: youtube.py\nDesc: fetch and play youtub videos\nAuthor: on_three\nEmail: on.three.email@gmail.com\nDATE: Sat, Dec 13, 2014\n  \n\"\"\"\nimport string\nimport re\nimport os\nimport psutil\nfrom twisted.python import log\nfrom twisted.internet.task import LoopingCall\nfrom twisted.internet.task import deferLater\nfrom twisted.internet import reactor\n\n#allow \"mod\" like control\nfrom whitelist import is_mod\nfrom whitelist import is_whitelisted\nfrom irc import splitnick\nfrom util import kill_proc_tree\nfrom util import activate_window_by_pid\n\n#save data via REST to website\nfrom web import Youtubes as yt\n\n#to kill subprocesses\nimport win32api\nimport win32gui\nimport win32con\nimport time\n\n#try to interact with sling to mute it when playing\nfrom volume import volume_event\n\nfrom irc import foreground\nfrom irc import background\nfrom irc import style\n\ndef get_youtubes_status():\n  if len(Video.QUEUE):\n    return foreground(u'yellow') + background(u'green') + u' ▶PLAYING ({q} queued) '.format(q=unicode(len(Video.QUEUE)))+ style(u'normal')\n  elif Video.SUBPROCESS and Video.SUBPROCESS.poll() is None:\n    return foreground(u'yellow') + background(u'green') + u' PLAYING ' + style(u'normal')\n  elif Youtube._enabled:\n    return foreground(u'white') + background(u'green') + u' ON ' + style(u'normal')\n  else:\n    return foreground(u'black') + background(u'red') + u' OFF ' + style(u'normal')\n\nclass Url(object):\n  def __init__(self, url, mute, pip):\n    self._url = url\n    self._mute = mute\n    self._pip = pip\n\ndef play_video():\n  #is there a video playing?\n  if Video.SUBPROCESS and Video.SUBPROCESS.poll() is None:\n    #video still playing. don't initiate a new one\n    #activate_window_by_pid(Video.SUBPROCESS.pid)\n    return\n  \n  if len(Video.QUEUE) \u003e 0:\n    url=Video.QUEUE.pop(0)\n    #call = Youtube.MPLAYER_COMMAND.format(x=pos.x, y=pos.y, width=p.w, url=url)\n    #call = Youtube.MPV_COMMAND.format(x=pos.x, y=pos.y, width=pos.w, height=pos.h, url=url)\n    #call = Youtube.MPV_COMMAND.format(x=pos.x, y=pos.y, width=pos.w, height=pos.h, url=url)\n    #call = Youtube.SMPLAYER_COMMAND.format(x=pos.x, y=pos.y, width=pos.w, height=pos.h, url=url)\n    #call = Youtube.MPSYT_COMMAND.format(url=url._url);\n    call = Youtube.MPV_COMMAND.format(url=url._url)\n    if url._pip:\n      call = Youtube.MPV_PIP_COMMAND.format(url=url._url)\n    log.msg(call.encode('utf-8'))\n    #also turn on mute if specified and needed\n    if url._mute and not Youtube.SLING_MUTE_STATE:\n      Youtube.SLING_MUTE_STATE = True\n      volume_event(\"MUTE_SLING\")\n    Video.SUBPROCESS = psutil.Popen(call, shell=True)\n    #schedule a window activation for 2 seconds after we create it (fucking windows...)\n    activate_window_by_pid(pid=Video.SUBPROCESS.pid)\n    #deferLater(reactor, 2.5, activate_window_by_pid, pid=Video.SUBPROCESS.pid)\n    #deferLater(reactor, 5, activate_window_by_pid, pid=Video.SUBPROCESS.pid)\n  else:\n    #try to unmute sling if it needs it\n    if Youtube.SLING_MUTE_STATE:\n      Youtube.SLING_MUTE_STATE = False\n      volume_event(\"UNMUTE_SLING\")\n    Video.SUBPROCESS = None\n  \nclass Video(object):\n  QUEUE = []\n  STARTER = None\n  SUBPROCESS = None\n  \n  def __init__(self):\n    Video.STARTER = LoopingCall(play_video)\n    Video.STARTER.start(1.0);\n  \n  def play(self, url, mute=False, pip=False):\n    Video.QUEUE.append(Url(url, mute, pip))\n    \n  def next(self):\n    if Video.SUBPROCESS:\n      kill_proc_tree(Video.SUBPROCESS.pid)\n    Video.SUBPROCESS = None\n  \n  def wipe(self):\n    log.msg('wiping...')\n    del Video.QUEUE[:]\n    if Video.SUBPROCESS:\n      kill_proc_tree(Video.SUBPROCESS.pid)\n    Video.SUBPROCESS = None\n    #try to unmute if it needs it\n    if Youtube.SLING_MUTE_STATE:\n      Youtube.SLING_MUTE_STATE = False\n      volume_event(\"UNMUTE_SLING\")\n\nclass Youtube(object):\n  '''\n  show a webm via simple system call\n  '''\n  REGEX = ur'^\\.(?:youtube|y|mpv) +(?P\u003curl\u003ehttp[s]?://[\\S]+)( +(?P\u003cparam\u003e(?:mute|m|nomute|n|pip|mini)))?'\n  ON_REGEX = ur'^\\.(?:youtube|y|mpv) on$'\n  OFF_REGEX = ur'^\\.(?:youtube|y|mpv) off$'\n  WIPE_REGEX = ur'^\\.(?:youtube|y|mpv) wipe all$'\n  NEXT_REGEX = ur'^\\.(?:youtube|y|mpv) (?:wipe|next)$'\n  KILL_REGEX = ur'^\\.kill$'\n  TOP_REGEX = ur'^\\.(?:youtube|y|mpv)$'\n  #VLC_COMMAND = u'\"/cygdrive/c/Program Files (x86)/VideoLAN/VLC/vlc.exe\" -I dummy --play-and-exit --no-video-deco --no-embedded-video --height={height} --video-x={x} --video-y={y} {url}'\n  #MPLAYER_COMMAND = u' ~/mplayer-svn-37292-x86_64/mplayer.exe -cache-min 50 -noborder -xy {width} -geometry {x}:{y} {url}'\n  #SMPLAYER_COMMAND = u'\"/cygdrive/c/Program Files (x86)/SMPlayer/smplayer.exe\" −ontop -close-at-end -size {width} {height} -pos {x} {y} {url}'\n  #MPSYT_COMMAND = u'/usr/bin/mpsyt playurl {url}';\n  #MPV_COMMAND = u'mpv.exe --cache=1024 --ontop --no-border --geometry=1280x720+600+120 {url}'\n  #--no-cache\n  #MPV_COMMAND = u'C:\\\\Users\\\\onthree\\\\mpv-x86_64-20150505\\\\mpv.exe --cache-file=TMP --ytdl --hr-seek=no --ontop --no-border --geometry=1280x720+600+120 {url}'\n  #MPV_COMMAND = u'C:\\\\msys64\\\\mingw64\\\\bin\\\\mpv.com --cache-file=TMP --msg-level=all=v --ytdl --hr-seek=no --ontop --no-border --geometry=1280x720+600+120 {url}'\n  #MPV_COMMAND = u'youtube-dl \"{url}\" -o - | mpv --msg-level=all=v --hr-seek=no --ontop --no-border --geometry=1280x720+600+120 -'\n  #pip pos at 512x288+1350+540\n  #MPV_PIP_COMMAND = u'C:\\\\Users\\\\onthree\\\\mpv-x86_64-20150505\\\\mpv.exe --cache-file=TMP --ytdl --hr-seek=no --ontop --no-border --geometry=512x288+650+540 {url}'\n  #MPV_PIP_COMMAND = u'C:\\\\msys64\\\\mingw64\\\\bin\\\\mpv.com --cache-file=TMP --ytdl --hr-seek=no --ontop --no-border --geometry=512x288+650+540 {url}'\n  MPV_COMMAND = u'mpv.exe --network-timeout=1 --cache-file=TMP --ytdl --ontop --no-border --geometry=1280x720+600+120 \"{url}\"'\n  MPV_PIP_COMMAND = u'mpv.exe --network-timeout=1 --cache-file=TMP --ytdl --ontop --no-border --geometry=512x288+650+540 \"{url}\"'\n\n  #Try to keep track whether we should mute/unmute the sling\n  #Better to keep track here as it's bound to be fucked anyway\n  SLING_MUTE_STATE = False\n  _enabled = True\n  \n  def __init__(self, parent):\n    '''\n    constructor\n    '''\n    self._parent = parent\n    Youtube._enabled = True\n    self._video = Video()\n\n  def is_msg_of_interest(self, user, channel, msg):\n    '''\n    PLUGIN API REQUIRED\n    Is the rx'd irc message of interest to this plugin?\n    '''\n    if re.search(Youtube.REGEX, msg) or re.match(Youtube.ON_REGEX, msg) or \\\n      re.match(Youtube.OFF_REGEX, msg) or re.match(Youtube.WIPE_REGEX, msg) or\\\n      re.match(Youtube.NEXT_REGEX, msg) or re.match(Youtube.KILL_REGEX, msg) or\\\n      re.match(Youtube.TOP_REGEX, msg):\n      return True\n    else:\n      return False\n\n  def handle_msg(self, user, channel, msg):\n    '''\n    PLUGIN API REQUIRED\n    Handle message and return nothing\n    '''\n    if not is_whitelisted(splitnick(user)):\n      return\n    \n    if re.match(Youtube.ON_REGEX, msg) and is_mod(splitnick(user)):\n      return self.on()\n\n    if re.match(Youtube.OFF_REGEX, msg) and is_mod(splitnick(user)):\n      return self.off()\n\n    if re.match(Youtube.WIPE_REGEX, msg) and is_whitelisted(splitnick(user)):\n      return self.wipe()\n      \n    if re.match(Youtube.NEXT_REGEX, msg) and is_whitelisted(splitnick(user)):\n      return self.next()\n      \n    if re.match(Youtube.KILL_REGEX, msg) and is_mod(splitnick(user)):\n      return self.kill()\n      \n    if re.match(Youtube.TOP_REGEX, msg):\n      self.top()\n      return\n\n    m = re.search(Youtube.REGEX, msg)    \n    #got a command along with the .c or .channel statement\n    url = m.groupdict()['url']\n    mute = True\n    pip = False\n    if m.groupdict()['param'] and (m.groupdict()['param']=='nomute' or m.groupdict()['param']=='n'):\n      mute = False\n    if m.groupdict()['param'] and (m.groupdict()['param']=='pip' or m.groupdict()['param']=='mini'):\n      pip = True\n    self.show(channel, user, url, mute, pip)\n\n  def on(self):\n    Youtube._enabled = True\n    log.msg('Youtube on')\n\n  def off(self):\n    Youtube._enabled = False\n    #also wipe all Youtubes\n    self.wipe()\n    log.msg('Youtube_off')\n\n  def wipe(self):\n    self._video.wipe()\n  \n  def next(self):\n    self._video.next()\n\n  def show(self, channel, nick, url, mute, pip):\n    '''\n    show video at given URL.\n    '''\n    #hack to show https as http\n    #url = url.replace(u'https://', u'http://')\n    if not Youtube._enabled:\n      log.msg('Not showing webm as they are turned off.')\n      return\n    yt.save_youtube(channel, nick, url)\n    self._video.play(url, mute, pip)\n    \n  def kill(self):\n    '''kill all mpv instances as a last resort\n    '''\n    log.msg(\"killing all instances of mpv\")\n    os.system('taskkill /f /im mpv.exe')\n    \n  def top(self):\n    '''if wer'e playing a video, select it bringing it to the top\n    '''\n    if Video.SUBPROCESS:\n      activate_window_by_pid(pid=Video.SUBPROCESS.pid)\n"
}
{
    "repo_name": "trentspi/PX8",
    "ref": "refs/heads/master",
    "path": "games/ski/ski.py",
    "copies": "1",
    "content": "import math\nimport random\nfrom collections import deque\n\nclass Config(object):\n    def __init__(self):\n        self.speed = 0.7\n        self.dist = 0.0\n        self.stop = False\n\n    def update(self):\n        if not self.stop:\n            self.speed += 0.003\n            self.dist += self.speed\n\nclass Logo(object):\n    def __init__(self):\n        self.x = 34\n        self.y = -150\n        self.y_dest = 50\n        self.y_dist = 0\n\nclass SnowParticle(object):\n    def __init__(self):\n        pass\n\n    def draw(self):\n        circfill(self.x, self.y, self.r, 7)\n\n\nclass Trail(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nclass Player(object):\n    def __init__(self, config):\n        self.config = config\n        self.frame_offset = 0\n        self.x = 30\n        self.y = 40\n        self.dead = False\n\n        self.trail = []\n        self.add_trail()\n\n        self.current_state = \"center\"\n\n        self.state = {\n            \"center\": [10],\n            \"right\": [11, 27],\n            \"left\": [12, 28],\n            \"dead\": [18, 19, 20],\n        }\n\n    def add_trail(self):\n        self.trail.append(Trail(self.x, self.y))\n\n    def trail_update(self):\n        for trail in self.trail:\n            trail.y -= self.config.speed\n\n    def trail_draw(self):\n        for i in range(0, len(self.trail)):\n            n = i + 1\n            a = self.trail[i]\n            if (i + 1) \u003e= len(self.trail):\n                b = self\n            else:\n                b = self.trail[i+1]\n\n            if b:\n                line(a.x + 2, a.y + 8, b.x + 2, b.y + 8, 6)\n                line(a.x + 5, a.y + 8, b.x + 5, b.y + 8, 6)\n\n    def set_dead(self):\n        if not self.dead:\n            self.dead = True\n            self.current_state = \"dead\"\n            self.y += 4\n\n    def update(self, timer):\n        self.trail_update()\n\n        if not self.dead:\n            if btn(0):\n                self.x = self.x - 1.5\n                self.current_state = \"left\"\n                self.frame_offset = 0\n                self.add_trail()\n            elif btn(1):\n                self.x = self.x + 1.5\n                self.current_state = \"right\"\n                self.frame_offset = 0\n                self.add_trail()\n            else:\n                self.current_state = \"center\"\n                self.frame_offset = 0\n                if btnp(2):\n                    if self.config.speed \u003e= 0.1:\n                        self.config.speed -= 0.1\n                if btnp(3):\n                    self.config.speed += 0.1\n\n        if timer % 5:\n            self.frame_offset = (self.frame_offset + 1) % len(self.state[self.current_state])\n\n        if self.x \u003c 0:\n            self.x = 0\n        if self.x \u003e 120:\n            self.x = 120\n\n    def draw(self):\n        self.trail_draw()\n\n        if not self.dead:\n            spr(3 + self.state[self.current_state][0], self.x, self.y + 3)\n\n        spr(self.state[self.current_state][self.frame_offset], self.x, self.y)\n\n\nclass Background(object):\n    def __init__(self, config):\n        self.dots = []\n        self.config = config\n\n        for i in range(0, 20):\n            self.dots.append([random.randint(0, 128), random.randint(0, 128)])\n\n    def update(self):\n        for dot in self.dots:\n            dot[1] -= self.config.speed\n            if dot[1] \u003c 0:\n                dot[0] = random.randint(0, 128)\n                dot[1] = 127\n\n    def draw(self):\n        for dot in self.dots:\n            rectfill(dot[0], dot[1], dot[0], dot[1], 6)\n\n\nclass Tree(object):\n    def __init__(self, config):\n        self.config = config\n        self.f = random.randint(0, 4) + 5\n        self.x = random.randint(0, 18) * 8\n        self.y = 127 + random.randint(0, 8) * 8\n        self.col = 0\n        if self.f == 5:\n            self.col = 3\n        elif self.f == 6:\n            self.col = 5\n\n    def update(self):\n        self.y -= self.config.speed\n\n    def draw(self):\n        spr(self.f, self.x, self.y)\n\n\nclass Peoples(object):\n    def __init__(self, config, nb):\n        self.config = config\n\n        self.l = []\n        for i in range(0, nb):\n            self.l.append(People(config))\n\n    def update(self):\n        idx = 0\n        l = []\n        for people in self.l:\n            if people.y \u003c -10:\n                l.append(idx)\n                self.l.append(People(self.config))\n            idx += 1\n\n        deque((list.pop(self.l, i) for i in sorted(l, reverse=True)), maxlen=0)\n\n        for people in self.l:\n            people.update()\n\n\n    def draw(self):\n        for people in self.l:\n            people.draw()\n\n    def collides(self, players):\n        for player in players:\n            for people in self.l:\n                if collides(people, player):\n                    print(\"COLLIDE BETWEEN \", people, player)\n\nclass People(object):\n    def __init__(self, config):\n        self.config = config\n        self.dead = False\n        self.x = ( random.randint(0, 7) * 8 ) + 28\n        self.y = 127 + (random.randint(0, 8)) * 8\n        self.f_start = 21\n        self.intern_speed = -1\n\n        self.f_offset = 0\n        if random.randint(0, 3) \u003c 1:\n            self.f_offset = 16\n\n        self.f = self.f_start\n        self.anim_tick = 0\n\n    def update(self):\n        global timer\n\n        self.y -= self.config.speed + self.intern_speed\n\n        self.anim_tick += 0.5\n        if timer % 7 == 0:\n            if self.f == 21:\n                self.f = 22\n            else:\n                self.f = 21\n\n    def draw(self):\n        spr(3, self.x, self.y + 2)\n        spr(self.f + self.f_offset, self.x, self.y)\n\nclass Yeti(object):\n    def __init__(self, config):\n        self.config = config\n\n        self.anim_tick = 0\n        self.f = 24\n        self.y = -70\n        self.x = 60\n        self.vy = 0.3\n        self.howl = False\n\nclass Particle(object):\n    def __init__(self, config, x, y, col):\n        self.config = config\n        self.x = x\n        self.y = y\n        self.col = col\n        self.dx = random.randint(0, 2) -1\n        self.dy = random.randint(0, 2) -1\n        self.vx = random.randint(0, 4) +1\n        self.vy = random.randint(0, 4) +1\n\n    def update(self):\n        self.vx -= 0.2\n        self.vy -= 0.2\n\n        self.x += (self.dx * self.vx)\n        self.y += (self.dy * self.vx)\n\n    def draw(self):\n        rectfill(self.x, self.y, self.x + 1, self.y + 1, self.col)\n\n\nclass Particles(object):\n    def __init__(self, config):\n        self.config = config\n        self.l = []\n\n    def add(self, x, y, col, num):\n        for _ in range(0, num):\n            self.l.append(Particle(self.config, x, y, col))\n\n    def update(self):\n        idx = 0\n        to_del = []\n        for particle in self.l:\n            if particle.vx \u003c 0 or particle.vy \u003c 0:\n                to_del.append(idx)\n\n        deque((list.pop(self.l, i) for i in sorted(to_del, reverse=True)), maxlen=0)\n\n        for particle in self.l:\n            particle.update()\n\n    def draw(self):\n        for particle in self.l:\n            particle.draw()\n\nconfig = Config()\nlogo = Logo()\nbackground = Background(config)\n\nsnow_particles = []\nplayers = [Player(config)]\npeoples = Peoples(config, 3)\nparticles = Particles(config)\n\ntimer = 0\nshakescreen = 0\n\ntrees = []\nfor i in range(0, 1):\n    trees.append(Tree(config))\n\nstate = 'splash'\n\ndef _init():\n    print(\"SKI _INIT\")\n\ndef tween(current, dest, speed):\n    fps = 60\n    return dest * fps / speed + current\n\ndef logo_update():\n    global logo\n    logo.y_dist = logo.y_dest - logo.y\n    logo.y = tween(logo.y, logo.y_dist, 900)\n\ndef collides(a, b):\n    bx1 = b.x +2\n    bx2 = b.x +6\n    by1 = b.y +5\n    by2 = b.y +8\n\n    return not ((a.y+8\u003cby1) or (a.y\u003eby2) or\t(a.x+8\u003cbx1) or (a.x\u003ebx2))\n\ndef _update():\n    global config, state, players, background, timer, trees, shakescreen, particles\n\n    timer += 1\n    if state == 'splash':\n        logo_update()\n        if btnp(2):\n            fade_out()\n            state = 'main'\n    else:\n        background.update()\n        particles.update()\n        peoples.update()\n        players[0].update(timer)\n\n        config.update()\n\n        idx = 0\n        to_del = []\n        for tree in trees:\n            if (tree.y \u003c -10):\n                to_del.append(idx)\n                trees.append(Tree(config))\n            idx += 1\n\n        deque((list.pop(trees, i) for i in sorted(to_del, reverse=True)), maxlen=0)\n\n        for tree in trees:\n            tree.update()\n\n        for tree in trees:\n            if collides(tree, players[0]):\n                if not players[0].dead:\n                    shakescreen = 50\n                players[0].set_dead()\n                config.speed = 0\n                config.stop = True\n                particles.add(tree.x + 4, tree.y + 4, tree.col, 10)\n\n                tree.y = -150\n\n        peoples.collides(players)\n\n    if timer % 100 == 0:\n            trees.append(Tree(config))\n\ndef fade_out(fa=0.2):\n    fa=max(min(1,fa),0)\n    fn=8\n    pn=15\n    fc=1/fn\n    fi=math.floor(fa/fc)+1\n    fades = [\n            [1,1,1,1,0,0,0,0],\n            [2,2,2,1,1,0,0,0],\n            [3,3,4,5,2,1,1,0],\n            [4,4,2,2,1,1,1,0],\n            [5,5,2,2,1,1,1,0],\n            [6,6,13,5,2,1,1,0],\n            [7,7,6,13,5,2,1,0],\n            [8,8,9,4,5,2,1,0],\n            [9,9,4,5,2,1,1,0],\n            [10,15,9,4,5,2,1,0],\n            [11,11,3,4,5,2,1,0],\n            [12,12,13,5,5,2,1,0],\n            [13,13,5,5,2,1,1,0],\n            [14,9,9,4,5,2,1,0],\n            [15,14,9,4,5,2,1,0],\n    ]\n\n    for n in range(1, pn):\n        pal(n,fades[n][fi],0)\n\ndef logo_draw():\n    global logo\n    w = 8\n    h = 4\n    start = 67\n    remap = 115\n\n    for x in range(0, w):\n        for y in range(0, h):\n            spr(x+start + (y * 16), logo.x + (x * 8), logo.y + (y*8))\n\ndef do_shakescreen():\n    global shakescreen\n\n    shakescreen -= 1\n    if shakescreen \u003c= 0:\n        camera(0,0)\n    else:\n        camera(random.randint(0, 4)-4, random.randint(0, 4)-4)\n\ndef _draw():\n    global snow_particles, state, players, background, trees, shakescreen, config\n\n    if state == 'splash':\n        rectfill(0, 0, 127, 127, 15)\n        rectfill(0, 43, 128, 44, 14)\n        rectfill(0, 38, 128, 40, 14)\n        rectfill(0, 0, 128, 35, 14)\n        rectfill(0, 0, 128, 8, 8)\n        rectfill(0, 10, 128, 11, 8)\n\n        spr_map(16,0, 0,0, 128,128)\n        spr_map(0,0, 0,0, 128,128)\n\n        logo_draw()\n\n        for snow_particle in snow_particles:\n            snow_particle.draw()\n    else:\n        pal()\n\n        if shakescreen \u003e 0:\n            do_shakescreen()\n        else:\n            camera(0, 0)\n\n        rectfill(0, 0, 127, 127, 7)\n\n        background.draw()\n        peoples.draw()\n        particles.draw()\n        players[0].draw()\n\n        for tree in trees:\n            tree.draw()\n\n        px8_print(str(config.dist), 110, 5, 12)"
}
{
    "repo_name": "bblais/Plasticnet",
    "ref": "refs/heads/master",
    "path": "plasticnet/connections/original/backup__init__.py",
    "copies": "1",
    "content": "from .BCM import BCM\nfrom .BCM import BCM_LawCooper\nfrom .BCM_TwoThreshold import BCM_TwoThreshold\nfrom .BCM_TwoThreshold_Harel import BCM_TwoThreshold_Harel\nfrom .BCM import BCM_Dynamic_Weight_Limits\nfrom .constant_connection import Constant_Connection\nfrom .Hebb import Hebb\nfrom .Miller2014 import Miller2014_Eq3\nfrom .Miller2014 import Miller2014_Eq5\nfrom . import process"
}
{
    "repo_name": "rain87/rutracker-keeper-suite",
    "ref": "refs/heads/master",
    "path": "rutracker_ks/tracker_edit.py",
    "copies": "1",
    "content": "#!/usr/bin/env python\n# coding=utf-8\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nimport config\nfrom utils import *\nimport rutracker_api as api\nfrom bencode import bdecode, bencode\nimport re\n\ncnt = 0\ntorrents = scan_folder('/tmp/BT_backup')\nfor torrent in torrents:\n    cnt += 1\n    if cnt % 100 == 0:\n        print('Processing {}'.format(cnt))\n    tracker = ''\n    with open(torrent) as f:\n        data = bdecode(f.read())\n        tracker = re.sub('http://(bt\\d?).*?/', 'http://\\\\1.rutracker.cx/', data['announce'])\n        data['announce'] = tracker\n        #data['announce-list'] = [ tracker ]\n        if 'announce-list' in data:\n            del data['announce-list']\n    with open(torrent, 'w') as f:\n        f.write(bencode(data))\n\n    fastresume = torrent[:-len('.torrent')] + '.fastresume'\n    with open(fastresume) as f:\n        data = bdecode(f.read())\n        #data['trackers'] = [ tracker ]\n        del data['trackers']\n    with open(fastresume, 'w') as f:\n        f.write(bencode(data))\n"
}
{
    "repo_name": "bellisk/opendata-multisearch",
    "ref": "refs/heads/master",
    "path": "ord_hackday/search/migrations/0001_initial.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Portal',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n                ('type', models.CharField(max_length=8, choices=[(b'CKAN', b'CKAN')])),\n                ('url', models.URLField()),\n            ],\n        ),\n    ]\n"
}
{
    "repo_name": "matheuscas/pycpfcnpj",
    "ref": "refs/heads/master",
    "path": "tests/cnpj_tests.py",
    "copies": "1",
    "content": "import unittest\n\nfrom pycpfcnpj import cnpj\n\n\nclass CNPJTests(unittest.TestCase):\n    \"\"\"docstring for CNPJTests\"\"\"\n\n    def setUp(self):\n        self.valid_cnpj = \"11444777000161\"\n        self.masked_valid_cnpj = \"11.444.777/0001-61\"\n        self.invalid_cnpj = \"11444777000162\"\n        self.masked_invalid_cnpj = \"11.444.777/0001-62\"\n        self.invalid_cnpj_whitespaces = \"11444 777000161\"\n        self.invalid_cnpj_with_alphabetic = \"11444d777000161\"\n        self.invalid_cnpj_with_special_character = \"+5575999769162\"\n\n    def test_validate_cnpj_true(self):\n        self.assertTrue(cnpj.validate(self.valid_cnpj))\n\n    def test_validate_masked_cnpj_true(self):\n        self.assertTrue(cnpj.validate(self.masked_valid_cnpj))\n\n    def test_validate_cnpj_false(self):\n        self.assertFalse(cnpj.validate(self.invalid_cnpj))\n\n    def test_validate_masked_cnpj_false(self):\n        self.assertFalse(cnpj.validate(self.invalid_cnpj))\n\n    def test_validate_cnpj_with_same_numbers(self):\n        for i in range(10):\n            self.assertFalse(cnpj.validate(\"{0}\".format(i) * 14))\n\n    def test_validate_cnpj_with_whitespaces(self):\n        self.assertFalse(cnpj.validate(self.invalid_cnpj_whitespaces))\n\n    def test_validate_cnpj_with_alphabetic_characters(self):\n        self.assertFalse(cnpj.validate(self.invalid_cnpj_with_alphabetic))\n\n    def test_validate_cnpj_with_special_characters(self):\n        self.assertFalse(cnpj.validate(self.invalid_cnpj_with_special_character))\n\n\nif __name__ == \"__main__\":\n    unittest.main(verbosity=2)\n"
}
{
    "repo_name": "flavour/tldrmp",
    "ref": "refs/heads/master",
    "path": "modules/s3/s3filter.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\n\n\"\"\" Framework for filtered REST requests\n\n    @copyright: 2013 (c) Sahana Software Foundation\n    @license: MIT\n\n    @requires: U{B{I{gluon}} \u003chttp://web2py.com\u003e}\n\n    Permission is hereby granted, free of charge, to any person\n    obtaining a copy of this software and associated documentation\n    files (the \"Software\"), to deal in the Software without\n    restriction, including without limitation the rights to use,\n    copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the\n    Software is furnished to do so, subject to the following\n    conditions:\n\n    The above copyright notice and this permission notice shall be\n    included in all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n    EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n    OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n    HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n    WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n    OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\nimport datetime\nimport re\n\ntry:\n    import json # try stdlib (Python 2.6)\nexcept ImportError:\n    try:\n        import simplejson as json # try external module\n    except:\n        import gluon.contrib.simplejson as json # fallback to pure-Python module\n\ntry:\n    # Python 2.7\n    from collections import OrderedDict\nexcept:\n    # Python 2.6\n    from gluon.contrib.simplejson.ordered_dict import OrderedDict\n\nfrom gluon import *\nfrom gluon.sqlhtml import MultipleOptionsWidget\nfrom gluon.storage import Storage\nfrom gluon.tools import callback\n\nfrom s3rest import S3Method\nfrom s3resource import S3FieldSelector, S3ResourceField, S3URLQuery\nfrom s3utils import s3_get_foreign_key, s3_unicode, S3TypeConverter\nfrom s3validators import *\nfrom s3widgets import S3DateWidget, S3DateTimeWidget, S3GroupedOptionsWidget, S3MultiSelectWidget, S3OrganisationHierarchyWidget, S3RadioOptionsWidget, s3_grouped_checkboxes_widget, S3SelectChosenWidget\n\n# =============================================================================\nclass S3FilterWidget(object):\n    \"\"\" Filter widget for interactive search forms (base class) \"\"\"\n\n    #: the HTML class for the widget type\n    _class = \"generic-filter\"\n\n    #: the default query operator(s) for the widget type\n    operator = None\n\n    #: alternatives for client-side changeable operators\n    alternatives = None\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Prototype method to render this widget as an instance of\n            a web2py HTML helper class, to be implemented by subclasses.\n\n            @param resource: the S3Resource to render with widget for\n            @param values: the values for this widget from the URL query\n        \"\"\"\n\n        raise NotImplementedError\n\n    # -------------------------------------------------------------------------\n    def variable(self, resource, get_vars=None):\n        \"\"\"\n            Prototype method to generate the name for the URL query variable\n            for this widget, can be overwritten in subclasses.\n\n            @param resource: the resource\n            @return: the URL query variable name (or list of\n                     variable names if there are multiple operators)\n        \"\"\"\n\n        label, self.selector = self._selector(resource, self.field)\n\n        if not self.selector:\n            return None\n\n        if self.alternatives and get_vars is not None:\n            # Get the actual operator from get_vars\n            operator = self._operator(get_vars, self.selector)\n            if operator:\n                self.operator = operator\n\n        if \"label\" not in self.opts:\n            self.opts[\"label\"] = label\n\n        return self._variable(self.selector, self.operator)\n\n    # -------------------------------------------------------------------------\n    def data_element(self, variable):\n        \"\"\"\n            Prototype method to construct the hidden element that holds the\n            URL query term corresponding to an input element in the widget.\n\n            @param variable: the URL query variable\n        \"\"\"\n\n        if type(variable) is list:\n            variable = \"\u0026\".join(variable)\n        return INPUT(_type=\"hidden\",\n                     _id=\"%s-data\" % self.attr[\"_id\"],\n                     _class=\"filter-widget-data %s-data\" % self._class,\n                     _value=variable)\n\n    # -------------------------------------------------------------------------\n    # Helper methods\n    #\n    def __init__(self, field=None, **attr):\n        \"\"\"\n            Constructor to configure the widget\n\n            @param field: the selector(s) for the field(s) to filter by\n            @param attr: configuration options for this widget\n\n            Configuration options:\n            @keyword label: label for the widget\n            @keyword comment: comment for the widget\n            @keyword hidden: render widget initially hidden (=\"advanced\"\n                             option)\n            @keyword levels: list of location hierarchy levels\n                             (L{S3LocationFilter})\n            @keyword widget: widget to use (L{S3OptionsFilter}),\n                             \"multiselect\", \"multiselect-bootstrap\" or\n                             \"groupedopts\" (default)\n            @keyword cols: number of columns of checkboxes (L{S3OptionsFilter}\n                           and L{S3LocationFilter} with \"groupedopts\" widget)\n            @keyword filter: show filter for options (L{S3OptionsFilter},\n                             L{S3LocationFilter} with \"multiselect\" widget)\n            @keyword header: show header in widget (L{S3OptionsFilter},\n                             L{S3LocationFilter} with \"multiselect\" widget)\n            @keyword selectedList: number of selected items to show before\n                                   collapsing into number of items\n                                   (L{S3OptionsFilter}, L{S3LocationFilter}\n                                   with \"multiselect\" widget)\n            @keyword no_opts: text to show if no options available\n                              (L{S3OptionsFilter}, L{S3LocationFilter})\n            @keyword resource: alternative resource to look up options\n                               (L{S3LocationFilter}, L{S3OptionsFilter})\n            @keyword lookup: field in the alternative resource to look up\n                             options (L{S3LocationFilter})\n            @keyword options: fixed set of options (L{S3OptionsFilter}: dict\n                              of {value: label} or a callable that returns one,\n                              L{S3LocationFilter}: list of gis_location IDs)\n            @keyword size: maximum size of multi-letter options groups\n                           (L{S3OptionsFilter} with \"groupedopts\" widget)\n            @keyword help_field: field in the referenced table to display on\n                                 hovering over a foreign key option\n                                 (L{S3OptionsFilter} with \"groupedopts\" widget)\n            @keyword none: label for explicit None-option in many-to-many\n                           fields (L{S3OptionsFilter})\n            @keyword fieldtype: explicit field type \"date\" or \"datetime\" to\n                                use for context or virtual fields\n                                (L{S3DateFilter})\n            @keyword hide_time: don't show time selector (L{S3DateFilter})\n\n        \"\"\"\n\n        self.field = field\n        self.alias = None\n\n        attributes = Storage()\n        options = Storage()\n        for k, v in attr.iteritems():\n            if k[0] == \"_\":\n                attributes[k] = v\n            else:\n                options[k] = v\n        self.attr = attributes\n        self.opts = options\n\n        self.selector = None\n\n    # -------------------------------------------------------------------------\n    def __call__(self, resource, get_vars=None, alias=None):\n        \"\"\"\n            Entry point for the form builder\n\n            @param resource: the S3Resource to render with widget for\n            @param get_vars: the GET vars (URL query vars) to prepopulate\n                             the widget\n            @param alias: the resource alias to use\n        \"\"\"\n\n        self.alias = alias\n\n        # Initialize the widget attributes\n        self._attr(resource)\n\n        # Extract the URL values to populate the widget\n        variable = self.variable(resource, get_vars)\n        if type(variable) is list:\n            values = Storage()\n            for k in variable:\n                values[k] = self._values(get_vars, k)\n        else:\n            values = self._values(get_vars, variable)\n\n        # Construct and populate the widget\n        widget = self.widget(resource, values)\n\n        # Recompute variable in case operator got changed in widget()\n        if self.alternatives:\n            variable = self._variable(self.selector, self.operator)\n\n        # Construct the hidden data element\n        data = self.data_element(variable)\n\n        if type(data) is list:\n            data.append(widget)\n        else:\n            data = [data, widget]\n        return TAG[\"\"](*data)\n\n    # -------------------------------------------------------------------------\n    def _attr(self, resource):\n        \"\"\" Initialize and return the HTML attributes for this widget \"\"\"\n\n        _class = self._class\n\n        # Construct name and id for the widget\n        attr = self.attr\n        if \"_name\" not in attr:\n            if not resource:\n                raise SyntaxError(\"%s: _name parameter required \" \\\n                                  \"when rendered without resource.\" % \\\n                                  self.__class__.__name__)\n            flist = self.field\n            if type(flist) is not list:\n                flist = [flist]\n            colnames = []\n            for f in flist:\n                rfield = S3ResourceField(resource, f)\n                colname = rfield.colname\n                if colname:\n                    colnames.append(colname)\n                else:\n                    colnames.append(rfield.fname)\n            name = \"%s-%s-%s\" % (resource.alias, \"-\".join(colnames), _class)\n            attr[\"_name\"] = name.replace(\".\", \"_\")\n        if \"_id\" not in attr:\n            attr[\"_id\"] = attr[\"_name\"]\n\n        return attr\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _operator(cls, get_vars, selector):\n        \"\"\"\n            Helper method to get the operators from the URL query\n\n            @param get_vars: the GET vars (a dict)\n            @param selector: field selector\n\n            @return: query operator - None, str or list\n        \"\"\"\n\n        variables = [\"%s__%s\" % (selector, op) for op in cls.alternatives]\n        slen = len(selector) + 2\n\n        operators = [k[slen:] for k, v in get_vars.iteritems()\n                                  if k in variables]\n        if not operators:\n            return None\n        elif len(operators) == 1:\n            return operators[0]\n        else:\n            return operators\n\n    # -------------------------------------------------------------------------\n    def _prefix(self, selector):\n        \"\"\"\n            Helper method to prefix an unprefixed field selector\n\n            @param alias: the resource alias to use as prefix\n            @param selector: the field selector\n\n            @return: the prefixed selector\n        \"\"\"\n\n        alias = self.alias\n        if alias is None:\n            alias = \"~\"\n        if \".\" not in selector.split(\"$\", 1)[0]:\n            return \"%s.%s\" % (alias, selector)\n        else:\n            return selector\n\n    # -------------------------------------------------------------------------\n    def _selector(self, resource, fields):\n        \"\"\"\n            Helper method to generate a filter query selector for the\n            given field(s) in the given resource.\n\n            @param resource: the S3Resource\n            @param fields: the field selectors (as strings)\n\n            @return: the field label and the filter query selector, or None\n                     if none of the field selectors could be resolved\n        \"\"\"\n\n        prefix = self._prefix\n        label = None\n\n        if not fields:\n            return label, None\n        if not isinstance(fields, (list, tuple)):\n            fields = [fields]\n        selectors = []\n        for field in fields:\n            if resource:\n                try:\n                    rfield = S3ResourceField(resource, field)\n                except (AttributeError, TypeError):\n                    continue\n                if not rfield.field and not rfield.virtual:\n                    # Unresolvable selector\n                    continue\n                if not label:\n                    label = rfield.label\n                selectors.append(prefix(rfield.selector))\n            else:\n                selectors.append(field)\n        if selectors:\n            return label, \"|\".join(selectors)\n        else:\n            return label, None\n\n    # -------------------------------------------------------------------------\n    @staticmethod\n    def _values(get_vars, variable):\n        \"\"\"\n            Helper method to get all values of a URL query variable\n\n            @param get_vars: the GET vars (a dict)\n            @param variable: the name of the query variable\n\n            @return: a list of values\n        \"\"\"\n\n        if not variable:\n            return []\n        elif variable in get_vars:\n            values = S3URLQuery.parse_value(get_vars[variable])\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n            return values\n        else:\n            return []\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _variable(cls, selector, operator):\n        \"\"\"\n            Construct URL query variable(s) name from a filter query\n            selector and the given operator(s)\n\n            @param selector: the selector\n            @param operator: the operator (or tuple/list of operators)\n\n            @return: the URL query variable name (or list of variable names)\n        \"\"\"\n\n        if isinstance(operator, (tuple, list)):\n            return [cls._variable(selector, o) for o in operator]\n        elif operator:\n            return \"%s__%s\" % (selector, operator)\n        else:\n            return selector\n\n# =============================================================================\nclass S3TextFilter(S3FilterWidget):\n    \"\"\" Text filter widget \"\"\"\n\n    _class = \"text-filter\"\n\n    operator = \"like\"\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self.attr\n\n        if \"_size\" not in attr:\n            attr.update(_size=\"40\")\n        if \"_class\" in attr and attr[\"_class\"]:\n            _class = \"%s %s\" % (attr[\"_class\"], self._class)\n        else:\n            _class = self._class\n        attr[\"_class\"] = _class\n        attr[\"_type\"] = \"text\"\n\n        values = [v.strip(\"*\") for v in values if v is not None]\n        if values:\n            attr[\"_value\"] = \" \".join(values)\n\n        return INPUT(**attr)\n\n# =============================================================================\nclass S3RangeFilter(S3FilterWidget):\n    \"\"\" Numerical Range Filter Widget \"\"\"\n\n    # Overall class\n    _class = \"range-filter\"\n    # Class for visible input boxes.\n    _input_class = \"%s-%s\" % (_class, \"input\")\n\n    operator = [\"ge\", \"le\"]\n\n    # Untranslated labels for individual input boxes.\n    input_labels = {\"ge\": \"Minimum\", \"le\": \"Maximum\"}\n\n    # -------------------------------------------------------------------------\n    def data_element(self, variables):\n        \"\"\"\n            Overrides S3FilterWidget.data_element(), constructs multiple\n            hidden INPUTs (one per variable) with element IDs of the form\n            \u003cid\u003e-\u003coperator\u003e-data (where no operator is translated as \"eq\").\n\n            @param variables: the variables\n        \"\"\"\n\n        if variables is None:\n            operators = self.operator\n            if type(operators) is not list:\n                operators = [operators]\n            variables = self._variable(self.selector, operators)\n        else:\n\n            # Split the operators off the ends of the variables.\n            if type(variables) is not list:\n                variables = [variables]\n            operators = [v.split(\"__\")[1]\n                         if \"__\" in v else \"eq\"\n                         for v in variables]\n\n        elements = []\n        id = self.attr[\"_id\"]\n\n        for o, v in zip(operators, variables):\n\n             elements.append(\n                 INPUT(_type=\"hidden\",\n                       _id=\"%s-%s-data\" % (id, o),\n                       _class=\"filter-widget-data %s-data\" % self._class,\n                       _value=v))\n\n        return elements\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self.attr\n        _class = self._class\n        if \"_class\" in attr and attr[\"_class\"]:\n            _class = \"%s %s\" % (attr[\"_class\"], _class)\n        else:\n            _class = _class\n        attr[\"_class\"] = _class\n\n        input_class = self._input_class\n        input_labels = self.input_labels\n        input_elements = DIV()\n        ie_append = input_elements.append\n\n        selector = self.selector\n\n        _variable = self._variable\n\n        id = attr[\"_id\"]\n        for operator in self.operator:\n\n            input_id = \"%s-%s\" % (id, operator)\n\n            input_box = INPUT(_name=input_id,\n                              _id=input_id,\n                              _type=\"text\",\n                              _class=input_class)\n\n            variable = _variable(selector, operator)\n\n            # Populate with the value, if given\n            # if user has not set any of the limits, we get [] in values.\n            value = values.get(variable, None)\n            if value not in [None, []]:\n                if type(value) is list:\n                    value = value[0]\n                input_box[\"_value\"] = value\n                input_box[\"value\"] = value\n\n            ie_append(DIV(\n                        DIV(LABEL(current.T(input_labels[operator] + \":\"),\n                                  _for=input_id),\n                            _class=\"range-filter-label\"),\n                        DIV(input_box,\n                            _class=\"range-filter-widget\"),\n                        _class=\"range-filter-field\"))\n\n        return input_elements\n\n# =============================================================================\nclass S3DateFilter(S3RangeFilter):\n    \"\"\"\n        Date Range Filter Widget\n        @see: L{Configuration Options\u003cS3FilterWidget.__init__\u003e}\n    \"\"\"\n\n    _class = \"date-filter\"\n\n    # Class for visible input boxes.\n    _input_class = \"%s-%s\" % (_class, \"input\")\n\n    operator = [\"ge\", \"le\"]\n\n    # Untranslated labels for individual input boxes.\n    input_labels = {\"ge\": \"From\", \"le\": \"To\"}\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self.attr\n\n        # CSS class and element ID\n        _class = self._class\n        if \"_class\" in attr and attr[\"_class\"]:\n            _class = \"%s %s\" % (attr[\"_class\"], _class)\n        else:\n            _class = _class\n        _id = attr[\"_id\"]\n\n        # Determine the field type\n        if resource:\n            rfield = S3ResourceField(resource, self.field)\n            field = rfield.field\n        else:\n            rfield = field = None\n        if not field:\n            if not rfield or rfield.virtual:\n                ftype = self.opts.get(\"fieldtype\", \"datetime\")\n            else:\n                # Unresolvable selector\n                return \"\"\n        else:\n            ftype = rfield.ftype\n        if not field:\n            # S3DateTimeWidget requires a Field\n            if rfield:\n                tname, fname = rfield.tname, rfield.fname\n            else:\n                tname, fname = \"notable\", \"datetime\"\n                if not _id:\n                    raise SyntaxError(\"%s: _id parameter required \" \\\n                                      \"when rendered without resource.\" % \\\n                                      self.__class__.__name__)\n            dtformat = current.deployment_settings.get_L10n_date_format()\n            field = Field(fname, ftype,\n                          requires = IS_DATE_IN_RANGE(format = dtformat))\n            field.tablename = field._tablename = tname\n\n        # Options\n        hide_time = self.opts.get(\"hide_time\", False)\n\n        # Generate the input elements\n        T = current.T\n        selector = self.selector\n        _variable = self._variable\n        input_class = self._input_class\n        input_labels = self.input_labels\n        input_elements = DIV(_id=_id, _class=_class)\n        append = input_elements.append\n        for operator in self.operator:\n\n            input_id = \"%s-%s\" % (_id, operator)\n\n            # Determine the widget class\n            if ftype == \"date\":\n                widget = S3DateWidget()\n            else:\n                opts = {}\n                if operator == \"ge\":\n                    opts[\"set_min\"] = \"%s-%s\" % (_id, \"le\")\n                elif operator == \"le\":\n                    opts[\"set_max\"] = \"%s-%s\" % (_id, \"ge\")\n                widget = S3DateTimeWidget(hide_time=hide_time, **opts)\n\n            # Populate with the value, if given\n            # if user has not set any of the limits, we get [] in values.\n            variable = _variable(selector, operator)\n            value = values.get(variable, None)\n            if value not in [None, []]:\n                if type(value) is list:\n                    value = value[0]\n            else:\n                value = None\n\n            # Render the widget\n            picker = widget(field, value,\n                            _name=input_id,\n                            _id=input_id,\n                            _class=input_class)\n\n            # Append label and widget\n            append(DIV(\n                    DIV(LABEL(\"%s:\" % T(input_labels[operator]),\n                            _for=input_id),\n                        _class=\"range-filter-label\"),\n                    DIV(picker,\n                        _class=\"range-filter-widget\"),\n                    _class=\"range-filter-field\"))\n\n        return input_elements\n\n# =============================================================================\nclass S3LocationFilter(S3FilterWidget):\n    \"\"\"\n        Hierarchical Location Filter Widget\n        @see: L{Configuration Options\u003cS3FilterWidget.__init__\u003e}\n\n        NB This will show records linked to all child locations of the Lx\n    \"\"\"\n\n    _class = \"location-filter\"\n\n    operator = \"belongs\"\n\n    # -------------------------------------------------------------------------\n    def __init__(self, field=None, **attr):\n        \"\"\"\n            Constructor to configure the widget\n\n            @param field: the selector(s) for the field(s) to filter by\n            @param attr: configuration options for this widget\n        \"\"\"\n\n        # Translate options using gis_location_name?\n        settings = current.deployment_settings\n        translate = settings.get_L10n_translate_gis_location()\n        if translate:\n            language = current.session.s3.language\n            if language == settings.get_L10n_default_language():\n                translate = False\n        self.translate = translate\n\n        super(S3LocationFilter, self).__init__(field=field, **attr)\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self._attr(resource)\n        opts = self.opts\n        name = attr[\"_name\"]\n\n        ftype, levels, noopt = self._options(resource)\n        if noopt:\n            return SPAN(noopt, _class=\"no-options-available\")\n\n        # Filter class (default+custom)\n        _class = self._class\n        if \"_class\" in attr and attr[\"_class\"]:\n            _class = \"%s %s\" % (_class, attr[\"_class\"])\n        attr[\"_class\"] = _class\n\n        # Store id and name for the data element\n        base_id = attr[\"_id\"]\n        base_name = attr[\"_name\"]\n\n        widgets = []\n        w_append = widgets.append\n        operator = self.operator\n        field_name = self.field\n\n        fname = self._prefix(field_name) if resource else field_name\n        \n        # @ToDo: Hide dropdowns other than first\n        if opts.widget == \"multiselect\":\n\n            # Multiselect Dropdown with Checkboxes\n            if \"multiselect-filter-widget\" not in _class:\n                attr[\"_class\"] = \"%s multiselect-filter-widget\" % _class\n\n            # Add one widget per level\n            for level in levels:\n                # Dummy field\n                name = \"%s-%s\" % (base_name, level)\n                options = levels[level][\"options\"]\n                dummy_field = Storage(name=name,\n                                      type=ftype,\n                                      requires=IS_IN_SET(options,\n                                                         multiple=True))\n                # Unique ID/name\n                attr[\"_id\"] = \"%s-%s\" % (base_id, level)\n                attr[\"_name\"] = name\n                # Find relevant values to pre-populate the widget\n                _values = values.get(\"%s$%s__%s\" % (fname, level, operator))\n                w = S3MultiSelectWidget(filter = opts.get(\"filter\", False),\n                                        header = opts.get(\"header\", False),\n                                        selectedList = opts.get(\"selectedList\", 3),\n                                        noneSelectedText = \"Select %s\" % levels[level][\"label\"])\n                widget = w(dummy_field, _values, **attr)\n                w_append(widget)\n\n        else:\n            # Grouped Checkboxes\n            if \"s3-checkboxes-widget\" not in _class:\n                attr[\"_class\"] = \"%s s3-checkboxes-widget\" % _class\n            attr[\"cols\"] = opts.get(\"cols\", 3)\n\n            # Add one widget per level\n            for level in levels:\n                # Dummy field\n                name = \"%s-%s\" % (base_name, level)\n                options = levels[level][\"options\"]\n                dummy_field = Storage(name=name,\n                                      type=ftype,\n                                      requires=IS_IN_SET(options,\n                                                         multiple=True))\n                # Unique ID/name\n                attr[\"_id\"] = \"%s-%s\" % (base_id, level)\n                attr[\"_name\"] = name\n                # Find relevant values to pre-populate\n                _values = values.get(\"%s$%s__%s\" % (fname, level, operator))\n                w_append(s3_grouped_checkboxes_widget(dummy_field,\n                                                      _values,\n                                                      **attr))\n\n        # Restore id and name for the data_element\n        attr[\"_id\"] = base_id\n        attr[\"_name\"] = base_name\n\n        # Render the filter widget\n        return TAG[\"\"](*widgets)\n\n    # -------------------------------------------------------------------------\n    def data_element(self, variable):\n        \"\"\"\n            Construct the hidden element that holds the\n            URL query term corresponding to an input element in the widget.\n\n            @param variable: the URL query variable\n        \"\"\"\n\n        output = []\n        oappend = output.append\n        i = 0\n        for level in self.levels:\n            widget = INPUT(_type=\"hidden\",\n                           _id=\"%s-%s-data\" % (self.attr[\"_id\"], level),\n                           _class=\"filter-widget-data %s-data\" % self._class,\n                           _value=variable[i])\n            oappend(widget)\n            i += 1\n\n        return output\n\n    # -------------------------------------------------------------------------\n    def ajax_options(self, resource):\n\n        attr = self._attr(resource)\n        ftype, levels, noopt = self._options(resource, inject_hierarchy=False)\n\n        opts = {}\n        base_id = attr[\"_id\"]\n        for level in levels:\n            if noopt:\n                opts[\"%s-%s\" % (base_id, level)] = str(noopt)\n            else:\n                options = levels[level][\"options\"]\n                opts[\"%s-%s\" % (base_id, level)] = options\n        return opts\n\n    # -------------------------------------------------------------------------\n    def _options(self, resource, inject_hierarchy=True):\n\n        T = current.T\n\n        NOOPT = T(\"No options available\")\n\n        attr = self.attr\n        opts = self.opts\n        translate = self.translate\n\n        # Which levels should we display?\n        # Lookup the appropriate labels from the GIS configuration\n        hierarchy = current.gis.get_location_hierarchy()\n        levels = OrderedDict()\n        if \"levels\" in opts:\n            for level in opts[\"levels\"]:\n                levels[level] = hierarchy.get(level, level)\n        else:\n            # @ToDo: Do this dynamically from the data?\n            for level in hierarchy:\n                levels[level] = hierarchy.get(level, level)\n        # Pass to data_element\n        self.levels = levels\n\n        if \"label\" not in opts:\n            opts[\"label\"] = T(\"Filter by Location\")\n\n        ftype = \"reference gis_location\"\n        default = (ftype, levels.keys(), opts.get(\"no_opts\", NOOPT))\n\n        # Resolve the field selector\n        selector = None\n        if resource is None:\n            rname = opts.get(\"resource\")\n            if rname:\n                resource = current.s3db.resource(rname)\n                selector = opts.get(\"lookup\", \"location_id\")\n        else:\n            selector = self.field\n\n        options = opts.get(\"options\")\n        if options:\n            # Fixed options (=list of location IDs)\n            resource = current.s3db.resource(\"gis_location\", id=options)\n            fields = [\"id\"] + [l for l in levels]\n            if translate:\n                fields.append(\"path\")\n            joined = False\n\n        elif selector:\n            # Lookup options from resource\n            rfield = S3ResourceField(resource, selector)\n            if not rfield.field or rfield.ftype != ftype:\n                # Must be a real reference to gis_location\n                return default\n            fields = [selector] + [\"%s$%s\" % (selector, l) for l in levels]\n            if translate:\n                fields.append(\"%s$path\" % selector)\n            joined = True\n            # Filter out old Locations\n            # @ToDo: Allow override\n            resource.add_filter(current.s3db.gis_location.end_date == None)\n\n        else:\n            # Neither fixed options nor resource to look them up\n            return default\n        \n        # Find the options\n        rows = resource.select(fields=fields,\n                               limit=None,\n                               virtual=False,\n                               as_rows=True)\n        # No options?\n        if not rows:\n            return default\n\n        # Initialise Options Storage \u0026 Hierarchy\n        hierarchy = {}\n        first = True\n        for level in levels:\n            if first:\n                hierarchy[level] = {}\n                _level = level\n                first = False\n            # @ToDo: Translate Labels\n            levels[level] = {\"label\": levels[level],\n                             \"options\": {} if translate else [],\n                             }\n\n        # Generate a name localization lookup dict\n        if translate:\n            # Get IDs via Path to lookup name_l10n\n            ids = set()\n            if joined:\n                if \"$\" in selector:\n                    selector = \"%s.%s\" % (rfield.field.tablename, selector.split(\"$\", 1)[1])\n                else:\n                    selector = \"%s.%s\" % (resource.tablename, selector)\n            for row in rows:\n                _row = getattr(row, \"gis_location\") if joined else row\n                path = _row.path\n                if path:\n                    path = path.split(\"/\")\n                else:\n                    # Build it\n                    if joined:\n                        location_id = row[selector]\n                        if location_id:\n                            _row.id = location_id\n                    if \"id\" in _row:\n                        path = current.gis.update_location_tree(_row)\n                if path:\n                    ids |= set(path)\n            # Build lookup table for name_l10n\n            name_l10n = {}\n            s3db = current.s3db\n            table = s3db.gis_location\n            ntable = s3db.gis_location_name\n            query = (table.id.belongs(ids)) \u0026 \\\n                    (ntable.deleted == False) \u0026 \\\n                    (ntable.location_id == table.id) \u0026 \\\n                    (ntable.language == current.session.s3.language)\n            nrows = current.db(query).select(table.name,\n                                             ntable.name_l10n,\n                                             limitby=(0, len(ids)),\n                                             )\n            for row in nrows:\n                name_l10n[row[\"gis_location.name\"]] = row[\"gis_location_name.name_l10n\"]\n\n        # Populate the Options and the Hierarchy\n        for row in rows:\n            _row = getattr(row, \"gis_location\") if joined else row\n            if inject_hierarchy:\n                parent = None\n                grandparent = None\n                greatgrandparent = None\n                greatgreatgrandparent = None\n                greatgreatgreatgrandparent = None\n                i = 0\n            for level in levels:\n                v = _row[level]\n                if v:\n                    o = levels[level][\"options\"]\n                    if v not in o:\n                        if translate:\n                            o[v] = name_l10n.get(v, v)\n                        else:\n                            o.append(v)\n                if inject_hierarchy:\n                    if i == 0:\n                        h = hierarchy[_level]\n                        if v not in h:\n                            h[v] = {}\n                        parent = v\n                    elif i == 1:\n                        h = hierarchy[_level][parent]\n                        if v not in h:\n                            h[v] = {}\n                        grandparent = parent\n                        parent = v\n                    elif i == 2:\n                        h = hierarchy[_level][grandparent][parent]\n                        if v not in h:\n                            h[v] = {}\n                        greatgrandparent = grandparent\n                        grandparent = parent\n                        parent = v\n                    elif i == 3:\n                        h = hierarchy[_level][greatgrandparent][grandparent][parent]\n                        if v not in h:\n                            h[v] = {}\n                        greatgreatgrandparent = greatgrandparent\n                        greatgrandparent = grandparent\n                        grandparent = parent\n                        parent = v\n                    elif i == 4:\n                        h = hierarchy[_level][greatgreatgrandparent][greatgrandparent][grandparent][parent]\n                        if v not in h:\n                            h[v] = {}\n                        greatgreatgreatgrandparent = greatgreatgrandparent\n                        greatgreatgrandparent = greatgrandparent\n                        greatgrandparent = grandparent\n                        grandparent = parent\n                        parent = v\n                    elif i == 5:\n                        h = hierarchy[_level][greatgreatgreatgrandparent][greatgreatgrandparent][greatgrandparent][grandparent][parent]\n                        if v not in h:\n                            h[v] = {}\n                    i += 1\n\n        if translate:\n            # Sort the options dicts\n            for level in levels:\n                options = levels[level][\"options\"]\n                options = OrderedDict(sorted(options.iteritems()))\n        else:\n            # Sort the options lists\n            for level in levels:\n                levels[level][\"options\"].sort()\n\n        if inject_hierarchy:\n            # Inject the Location Hierarchy\n            hierarchy = \"S3.location_filter_hierarchy=%s\" % json.dumps(hierarchy)\n            js_global = current.response.s3.js_global\n            js_global.append(hierarchy)\n            if translate:\n                # Inject lookup list\n                name_l10n = \"S3.location_name_l10n=%s\" % json.dumps(name_l10n)\n                js_global.append(name_l10n)\n\n        return (ftype, levels, None)\n\n    # -------------------------------------------------------------------------\n    def _selector(self, resource, fields):\n        \"\"\"\n            Helper method to generate a filter query selector for the\n            given field(s) in the given resource.\n\n            @param resource: the S3Resource\n            @param fields: the field selectors (as strings)\n\n            @return: the field label and the filter query selector, or None if none of the\n                     field selectors could be resolved\n        \"\"\"\n\n        prefix = self._prefix\n        label = None\n\n        if \"levels\" in self.opts:\n            levels = self.opts.levels\n        else:\n            levels = current.gis.hierarchy_level_keys\n        fields = [\"%s$%s\" % (fields, level) for level in levels]\n        if resource:\n            selectors = []\n            for field in fields:\n                try:\n                    rfield = S3ResourceField(resource, field)\n                except (AttributeError, TypeError):\n                    continue\n                if not label:\n                    label = rfield.label\n                selectors.append(prefix(rfield.selector))\n        else:\n            selectors = fields\n        if selectors:\n            return label, \"|\".join(selectors)\n        else:\n            return label, None\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _variable(cls, selector, operator):\n        \"\"\"\n            Construct URL query variable(s) name from a filter query\n            selector and the given operator(s)\n\n            @param selector: the selector\n            @param operator: the operator (or tuple/list of operators)\n\n            @return: the URL query variable name (or list of variable names)\n        \"\"\"\n\n        selectors = selector.split(\"|\")\n        return [\"%s__%s\" % (selector, operator) for selector in selectors]\n\n# =============================================================================\nclass S3OptionsFilter(S3FilterWidget):\n    \"\"\"\n        Options filter widget\n        @see: L{Configuration Options\u003cS3FilterWidget.__init__\u003e}\n    \"\"\"\n\n    _class = \"options-filter\"\n\n    operator = \"belongs\"\n\n    alternatives = [\"anyof\", \"contains\"]\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self._attr(resource)\n        opts = self.opts\n        name = attr[\"_name\"]\n\n        # Get the options\n        ftype, options, noopt = self._options(resource)\n        if noopt:\n            return SPAN(noopt, _class=\"no-options-available\")\n        else:\n            options = OrderedDict(options)\n\n        # Any-All-Option : for many-to-many fields the user can\n        # search for records containing all the options or any\n        # of the options:\n        if len(options) \u003e 1 and ftype[:4] == \"list\":\n            if self.operator == \"anyof\":\n                filter_type = \"any\"\n            else:\n                filter_type = \"all\"\n                if self.operator == \"belongs\":\n                    self.operator = \"contains\"\n\n            T = current.T\n            any_all = DIV(T(\"Filter type\"),\n                          INPUT(_name=\"%s_filter\" % name,\n                                _id=\"%s_filter_any\" % name,\n                                _type=\"radio\",\n                                _value=\"any\",\n                                value=filter_type),\n                          LABEL(T(\"Any\"),\n                                _for=\"%s_filter_any\" % name),\n                          INPUT(_name=\"%s_filter\" % name,\n                                _id=\"%s_filter_all\" % name,\n                                _type=\"radio\",\n                                _value=\"all\",\n                                value=filter_type),\n                          LABEL(T(\"All\"),\n                                _for=\"%s_filter_all\" % name),\n                          _class=\"s3-options-filter-anyall\")\n        else:\n            any_all = \"\"\n\n        # Initialize widget\n        widget_type = opts[\"widget\"]\n        if widget_type == \"multiselect-bootstrap\":\n            widget_class = \"multiselect-filter-bootstrap\"\n            script = \"/%s/static/scripts/bootstrap-multiselect.js\" % \\\n                        current.request.application\n            scripts = current.response.s3.scripts\n            if script not in scripts:\n                scripts.append(script)\n            w = MultipleOptionsWidget.widget\n        elif widget_type == \"multiselect\":\n            widget_class = \"multiselect-filter-widget\"\n            w = S3MultiSelectWidget(\n                    filter = opts.get(\"filter\", False),\n                    header = opts.get(\"header\", False),\n                    selectedList = opts.get(\"selectedList\", 3))\n        elif widget_type == \"chosen\":\n            widget_class = \"chosen-filter-widget\"\n            w = S3SelectChosenWidget()\n        # Radio is just GroupedOpts with multiple=False\n        #elif widget_type == \"radio\":\n        #    widget_class = \"radio-filter-widget\"\n        #    w = S3RadioOptionsWidget(options = options,\n        #                             cols = opts[\"cols\"],\n        #                             help_field = opts[\"help_field\"],\n        #                             )\n        else:\n            widget_class = \"groupedopts-filter-widget\"\n            w = S3GroupedOptionsWidget(options = options,\n                                       multiple = opts.get(\"multiple\", True),\n                                       cols = opts[\"cols\"],\n                                       size = opts[\"size\"] or 12,\n                                       help_field = opts[\"help_field\"],\n                                       )\n\n        # Add widget class and default class\n        classes = set(attr.get(\"_class\", \"\").split()) | \\\n                  set((widget_class, self._class))\n        attr[\"_class\"] = \" \".join(classes) if classes else None\n\n        # Render the widget\n        dummy_field = Storage(name=name,\n                              type=ftype,\n                              requires=IS_IN_SET(options, multiple=True))\n        widget = w(dummy_field, values, **attr)\n        \n        return TAG[\"\"](any_all, widget)\n\n    # -------------------------------------------------------------------------\n    def ajax_options(self, resource):\n        \"\"\"\n            Method to Ajax-retrieve the current options of this widget\n\n            @param resource: the S3Resource\n        \"\"\"\n\n        opts = self.opts\n        attr = self._attr(resource)\n        ftype, options, noopt = self._options(resource)\n\n        if noopt:\n            options = {attr[\"_id\"]: str(noopt)}\n        else:\n            widget_type = opts[\"widget\"]\n            if widget_type in (\"multiselect-bootstrap\", \"multiselect\"):\n                # Produce a simple list of tuples\n                options = {attr[\"_id\"]: [(k, s3_unicode(v))\n                                         for k, v in options]}\n            else:\n                # Use the widget method to group and sort the options\n                widget = S3GroupedOptionsWidget(\n                                options = options,\n                                multiple = True,\n                                cols = opts[\"cols\"],\n                                size = opts[\"size\"] or 12,\n                                help_field = opts[\"help_field\"]\n                                )\n                options = {attr[\"_id\"]:\n                           widget._options({\"type\": ftype}, [])}\n        return options\n        \n    # -------------------------------------------------------------------------\n    def _options(self, resource):\n        \"\"\"\n            Helper function to retrieve the current options for this\n            filter widget\n\n            @param resource: the S3Resource\n        \"\"\"\n\n        T = current.T\n        NOOPT = T(\"No options available\")\n        EMPTY = T(\"None\")\n\n        attr = self.attr\n        opts = self.opts\n\n        # Resolve the field selector\n        selector = self.field\n        if isinstance(selector, (tuple, list)):\n            selector = selector[0]\n\n        if resource is None:\n            rname = opts.get(\"resource\")\n            if rname:\n                resource = current.s3db.resource(rname)\n\n        if resource:\n            rfield = S3ResourceField(resource, selector)\n            field = rfield.field\n            colname = rfield.colname\n            ftype = rfield.ftype\n        else:\n            rfield = field = colname = None\n            ftype = \"string\"\n\n        # Find the options\n        opt_keys = []\n\n        multiple = ftype[:5] == \"list:\"\n        if opts.options is not None:\n            # Custom dict of options {value: label} or a callable\n            # returning such a dict:\n            options = opts.options\n            if callable(options):\n                options = options()\n            opt_keys = options.keys()\n\n        elif resource:\n            # Determine the options from the field type\n            options = None\n            if ftype == \"boolean\":\n                opt_keys = (True, False)\n\n            elif field or rfield.virtual:\n                groupby = field if field and not multiple else None\n                virtual = field is None\n                rows = resource.select([selector],\n                                       limit=None,\n                                       orderby=field,\n                                       groupby=groupby,\n                                       virtual=virtual,\n                                       as_rows=True)\n                opt_keys = []\n                if rows:\n                    if multiple:\n                        kextend = opt_keys.extend\n                        for row in rows:\n                            vals = row[colname]\n                            if vals:\n                                kextend([v for v in vals\n                                           if v not in opt_keys])\n                    else:\n                        kappend = opt_keys.append\n                        for row in rows:\n                            v = row[colname]\n                            if v not in opt_keys:\n                                kappend(v)\n\n        # No options?\n        if len(opt_keys) \u003c 1 or len(opt_keys) == 1 and not opt_keys[0]:\n            return (ftype, None, opts.get(\"no_opts\", NOOPT))\n\n        # Represent the options\n        opt_list = [] # list of tuples (key, value)\n\n        # Custom represent? (otherwise fall back to field.represent)\n        represent = opts.represent\n        if not represent: # or ftype[:9] != \"reference\":\n            represent = field.represent if field else None\n\n        if options is not None:\n            # Custom dict of {value:label} =\u003e use this label\n            opt_list = options.items()\n\n        elif callable(represent):\n            # Callable representation function:\n\n            if hasattr(represent, \"bulk\"):\n                # S3Represent =\u003e use bulk option\n                opt_dict = represent.bulk(opt_keys,\n                                          list_type=False,\n                                          show_link=False)\n                if None in opt_keys:\n                    opt_dict[None] = EMPTY\n                elif None in opt_dict:\n                    del opt_dict[None]\n                if \"\" in opt_keys:\n                    opt_dict[\"\"] = EMPTY\n                opt_list = opt_dict.items()\n\n            else:\n                # Simple represent function\n                args = {\"show_link\": False} \\\n                       if \"show_link\" in represent.func_code.co_varnames else {}\n                if multiple:\n                    repr_opt = lambda opt: opt in (None, \"\") and (opt, EMPTY) or \\\n                                           (opt, represent([opt], **args))\n                else:\n                    repr_opt = lambda opt: opt in (None, \"\") and (opt, EMPTY) or \\\n                                           (opt, represent(opt, **args))\n                opt_list = map(repr_opt, opt_keys)\n\n        elif isinstance(represent, str) and ftype[:9] == \"reference\":\n            # Represent is a string template to be fed from the\n            # referenced record\n\n            # Get the referenced table\n            db = current.db\n            ktable = db[ftype[10:]]\n\n            k_id = ktable._id.name\n\n            # Get the fields referenced by the string template\n            fieldnames = [k_id]\n            fieldnames += re.findall(\"%\\(([a-zA-Z0-9_]*)\\)s\", represent)\n            represent_fields = [ktable[fieldname] for fieldname in fieldnames]\n\n            # Get the referenced records\n            query = (ktable.id.belongs([k for k in opt_keys\n                                              if str(k).isdigit()])) \u0026 \\\n                    (ktable.deleted == False)\n            rows = db(query).select(*represent_fields).as_dict(key=k_id)\n\n            # Run all referenced records against the format string\n            opt_list = []\n            ol_append = opt_list.append\n            for opt_value in opt_keys:\n                if opt_value in rows:\n                    opt_represent = represent % rows[opt_value]\n                    if opt_represent:\n                        ol_append((opt_value, opt_represent))\n\n        else:\n            # Straight string representations of the values (fallback)\n            opt_list = [(opt_value, s3_unicode(opt_value))\n                        for opt_value in opt_keys if opt_value]\n\n        none = opts[\"none\"]\n\n        try:\n            opt_list.sort(key=lambda item: item[1])\n        except:\n            opt_list.sort(key=lambda item: s3_unicode(item[1]))\n        options = []\n        empty = None\n        for k, v in opt_list:\n            if k is None:\n                empty = (\"NONE\", v)\n            else:\n                options.append((k, v))\n        if empty and none:\n            options.append(empty)\n\n        # Sort the options\n        return (ftype, options, None)\n\n# =============================================================================\nclass S3HierarchyFilter(S3FilterWidget):\n    \"\"\"\n        Filter widget for hierarchical types\n\n        Specific options:\n\n            lookup              name of the lookup table\n            represent           representation method for the key\n\n        @status: experimental\n    \"\"\"\n\n    _class = \"hierarchy-filter\"\n\n    operator = \"belongs\"\n\n    # -------------------------------------------------------------------------\n    def widget(self, resource, values):\n        \"\"\"\n            Render this widget as HTML helper object(s)\n\n            @param resource: the resource\n            @param values: the search values from the URL query\n        \"\"\"\n\n        attr = self._attr(resource)\n        opts = self.opts\n        name = attr[\"_name\"]\n\n        widget_id = attr[\"_id\"]\n        \n        rfield = None\n        selector = self.field\n        \n        lookup = opts.get(\"lookup\")\n        if not lookup:\n            if resource:\n                rfield = S3ResourceField(resource, selector)\n                if rfield.field:\n                    lookup = s3_get_foreign_key(rfield.field, m2m=False)[0]\n            if not lookup:\n                raise SyntaxError(\"No lookup table known for %s\" % selector)\n\n        represent = opts.get(\"represent\")\n        if not represent:\n            if not rfield:\n                rfield = S3ResourceField(resource, selector)\n                if rfield.field:\n                    represent = rfield.field.represent\n\n        from s3hierarchy import S3Hierarchy\n        h = S3Hierarchy(tablename=lookup, represent=represent)\n\n        if not h.config:\n            raise AttributeError(\"No hierarchy configured for %s\" % lookup)\n\n        widget = DIV(INPUT(_type=\"hidden\",\n                           _class=\"s3-hierarchy-input\"),\n                     DIV(h.html(\"%s-tree\" % widget_id),\n                         _class=\"s3-hierarchy-tree\"),\n                     **attr)\n        widget.add_class(self._class)\n\n        s3 = current.response.s3\n        scripts = s3.scripts\n        script_dir = \"/%s/static/scripts\" % current.request.application\n\n        # Currently selected values\n        selected = []\n        append = selected.append\n        if not isinstance(values, (list, tuple, set)):\n            values = [values]\n        for v in values:\n            if isinstance(v, (int, long)) or str(v).isdigit():\n                append(v)\n\n        if s3.debug:\n            script = \"%s/jquery.jstree.js\" % script_dir\n            if script not in scripts:\n                scripts.append(script)\n            script = \"%s/S3/s3.jquery.ui.hierarchicalopts.js\" % script_dir\n            if script not in scripts:\n                scripts.append(script)\n        else:\n            script = \"%s/S3/s3.jstree.min.js\" % script_dir\n            if script not in scripts:\n                scripts.append(script)\n\n        script = '''\n$('#%(widget_id)s').hierarchicalopts({\n    appname: '%(appname)s',\n    selected: %(selected)s\n});''' % {\n            \"appname\": current.request.application,\n            \"widget_id\": widget_id,\n            \"selected\": json.dumps(selected) if selected else \"null\",\n        }\n        s3.jquery_ready.append(script)\n\n        return widget\n\n# =============================================================================\nclass S3FilterForm(object):\n    \"\"\" Helper class to construct and render a filter form for a resource \"\"\"\n\n    def __init__(self, widgets, **attr):\n        \"\"\"\n            Constructor\n\n            @param widgets: the widgets (as list)\n            @param attr: HTML attributes for this form\n        \"\"\"\n\n        self.widgets = widgets\n\n        attributes = Storage()\n        options = Storage()\n        for k, v in attr.iteritems():\n            if k[0] == \"_\":\n                attributes[k] = v\n            else:\n                options[k] = v\n        self.attr = attributes\n        self.opts = options\n\n    # -------------------------------------------------------------------------\n    def html(self, resource, get_vars=None, target=None, alias=None):\n        \"\"\"\n            Render this filter form as HTML form.\n\n            @param resource: the S3Resource\n            @param get_vars: the request GET vars (URL query dict)\n            @param target: the HTML element ID of the target object for\n                           this filter form (e.g. a datatable)\n            @param alias: the resource alias to use in widgets\n        \"\"\"\n\n        attr = self.attr\n        form_id = attr.get(\"_id\")\n        if not form_id:\n            form_id = \"filter-form\"\n        attr[\"_id\"] = form_id\n\n        # Prevent issues with Webkit-based browsers \u0026 Back buttons\n        attr[\"_autocomplete\"] = \"off\"\n\n        opts = self.opts\n\n        # Form style\n        formstyle = opts.get(\"formstyle\", None)\n        if not formstyle:\n            formstyle = self._formstyle\n\n        # Filter widgets\n        rows = self._render_widgets(resource,\n                                    get_vars=get_vars or {},\n                                    alias=alias,\n                                    formstyle=formstyle)\n\n        # Other filter form controls\n        controls = self._render_controls()\n        if controls:\n            rows.append(formstyle(None, \"\", controls, \"\"))\n\n        # Submit elements\n        ajax = opts.get(\"ajax\", False)\n        submit = opts.get(\"submit\", False)\n        if submit:\n\n            settings = current.deployment_settings\n            \n            # Auto-submit\n            auto_submit = settings.get_ui_filter_auto_submit()\n            if auto_submit and opts.get(\"auto_submit\", True):\n                script = '''S3.search.filterFormAutoSubmit('%s',%s)''' % \\\n                         (form_id, auto_submit)\n                current.response.s3.jquery_ready.append(script)\n\n            # Custom label and class\n            _class = None\n            if submit is True:\n                label = current.T(\"Search\")\n            elif isinstance(submit, (list, tuple)):\n                label, _class = submit\n            else:\n                label = submit\n\n            # Submit button\n            submit_button = INPUT(_type=\"button\",\n                                  _value=label,\n                                  _class=\"filter-submit\")\n            #if auto_submit:\n                #submit_button.add_class(\"hide\")\n            if _class:\n                submit_button.add_class(_class)\n\n            # Where to request filtered data from:\n            submit_url = opts.get(\"url\", URL(vars={}))\n            \n            # Where to request updated options from:\n            ajax_url = opts.get(\"ajaxurl\", URL(args=[\"filter.options\"], vars={}))\n\n            # Submit row elements\n            submit = TAG[\"\"](submit_button,\n                             INPUT(_type=\"hidden\",\n                                   _class=\"filter-ajax-url\",\n                                   _value=ajax_url),\n                             INPUT(_type=\"hidden\",\n                                   _class=\"filter-submit-url\",\n                                   _value=submit_url))\n            if ajax and target:\n                submit.append(INPUT(_type=\"hidden\",\n                                    _class=\"filter-submit-target\",\n                                    _value=target))\n\n            # Append submit row\n            submit_row = formstyle(None, \"\", submit, \"\")\n            if auto_submit and hasattr(submit_row, \"add_class\"):\n                submit_row.add_class(\"hide\")\n            rows.append(submit_row)\n\n        # Filter Manager (load/apply/save filters)\n        fm = settings.get_search_filter_manager()\n        if fm and opts.get(\"filter_manager\", resource is not None):\n            filter_manager = self._render_filters(resource, form_id)\n            if filter_manager:\n                fmrow = formstyle(None, \"\", filter_manager, \"\")\n                if hasattr(fmrow, \"add_class\"):\n                    fmrow.add_class(\"filter-manager-row\")\n                rows.append(fmrow)\n\n        # Adapt to formstyle: render a TABLE only if formstyle returns TRs\n        if rows:\n            elements = rows[0]\n            if not isinstance(elements, (list, tuple)):\n                elements = elements.elements()\n            n = len(elements)\n            if n \u003e 0 and elements[0].tag == \"tr\" or \\\n               n \u003e 1 and elements[0].tag == \"\" and elements[1].tag == \"tr\":\n                form = FORM(TABLE(TBODY(rows)), **attr)\n            else:\n                form = FORM(DIV(rows), **attr)\n            form.add_class(\"filter-form\")\n            if ajax:\n                form.add_class(\"filter-ajax\")\n        else:\n            return \"\"\n\n        # Put a copy of formstyle into the form for access by the view\n        form.formstyle = formstyle\n        return form\n\n    # -------------------------------------------------------------------------\n    def fields(self, resource, get_vars=None, alias=None):\n        \"\"\"\n            Render the filter widgets without FORM wrapper, e.g. to\n            embed them as fieldset in another form.\n\n            @param resource: the S3Resource\n            @param get_vars: the request GET vars (URL query dict)\n            @param alias: the resource alias to use in widgets\n        \"\"\"\n\n        formstyle = self.opts.get(\"formstyle\", None)\n        if not formstyle:\n            formstyle = self._formstyle\n\n        rows = self._render_widgets(resource,\n                                    get_vars=get_vars,\n                                    alias=alias,\n                                    formstyle=formstyle)\n\n        controls = self._render_controls()\n        if controls:\n            rows.append(formstyle(None, \"\", controls, \"\"))\n        \n        # Adapt to formstyle: only render a TABLE if formstyle returns TRs\n        if rows:\n            elements = rows[0]\n            if not isinstance(elements, (list, tuple)):\n                elements = elements.elements()\n            n = len(elements)\n            if n \u003e 0 and elements[0].tag == \"tr\" or \\\n               n \u003e 1 and elements[0].tag == \"\" and elements[1].tag == \"tr\":\n                fields = TABLE(TBODY(rows))\n            else:\n                fields = DIV(rows)\n\n        return fields\n\n    # -------------------------------------------------------------------------\n    def _render_controls(self):\n        \"\"\"\n            Render optional additional filter form controls: advanced\n            options toggle, clear filters.\n        \"\"\"\n\n        controls = []\n    \n        advanced = self.opts.get(\"advanced\", False)\n        if advanced:\n            _class = \"filter-advanced\"\n            T = current.T\n            if advanced is True:\n                label = T(\"More Options\")\n            elif isinstance(advanced, (list, tuple)):\n                label = advanced[0]\n                label = advanced[1]\n                if len(advanced \u003e 2):\n                    _class = \"%s %s\" % (advanced[2], _class)\n            else:\n                label = advanced\n            label_off = T(\"Less Options\")\n            advanced = INPUT(_type=\"button\",\n                             _value=label,\n                             _label_on=label,\n                             _label_off=label_off,\n                             _class=_class)\n            controls.append(advanced)\n\n        clear = self.opts.get(\"clear\", True)\n        if clear:\n            _class = \"filter-clear\"\n            if clear is True:\n                label = current.T(\"Clear filter\")\n            elif isinstance(clear, (list, tuple)):\n                label = clear[0]\n                _class = \"%s %s\" % (clear[1], _class)\n            else:\n                label = clear\n            clear = A(label, _class=_class)\n            clear.add_class(\"action-lnk\")\n            controls.append(clear)\n\n        if controls:\n            return DIV(controls, _class=\"filter-controls\")\n        else:\n            return None\n\n    # -------------------------------------------------------------------------\n    def _render_widgets(self,\n                        resource,\n                        get_vars=None,\n                        alias=None,\n                        formstyle=None):\n        \"\"\"\n            Render the filter widgets\n    \n            @param resource: the S3Resource\n            @param get_vars: the request GET vars (URL query dict)\n            @param alias: the resource alias to use in widgets\n            @param formstyle: the formstyle to use\n\n            @return: a list of form rows\n        \"\"\"\n        \n        rows = []\n        rappend = rows.append\n        advanced = False\n        for f in self.widgets:\n            widget = f(resource, get_vars, alias=alias)\n            label = f.opts[\"label\"]\n            comment = f.opts[\"comment\"]\n            hidden = f.opts[\"hidden\"]\n            if hidden:\n                advanced = True\n            widget_id = f.attr[\"_id\"]\n            if widget_id:\n                row_id = \"%s__row\" % widget_id\n                label_id = \"%s__label\" % widget_id\n            else:\n                row_id = None\n                label_id = None\n            if label:\n                label = LABEL(\"%s:\" % label, _id=label_id, _for=widget_id)\n            else:\n                label = \"\"\n            if not comment:\n                comment = \"\"\n            rappend(formstyle(row_id, label, widget, comment, hidden=hidden))\n        if advanced:\n            if resource:\n                self.opts[\"advanced\"] = resource.get_config(\n                                            \"filter_advanced\", True)\n            else:\n                self.opts[\"advanced\"] = True\n        return rows\n            \n    # -------------------------------------------------------------------------\n    def _render_filters(self, resource, form_id):\n        \"\"\"\n            Render a filter manager widget\n\n            @param resource: the resource\n            @return: the widget\n        \"\"\"\n\n        SELECT_FILTER = current.T(\"Saved Filters...\")\n\n        ajaxurl = self.opts.get(\"saveurl\", URL(args=[\"filter.json\"], vars={}))\n        \n        # Current user\n        auth = current.auth\n        pe_id = auth.user.pe_id if auth.s3_logged_in() else None\n        if not pe_id:\n            return None\n    \n        table = current.s3db.pr_filter\n        query = (table.deleted != True) \u0026 \\\n                (table.pe_id == pe_id)\n\n        if resource:\n            query \u0026= (table.resource == resource.tablename)\n        else:\n            query \u0026= (table.resource == None)\n\n        rows = current.db(query).select(table._id,\n                                        table.title,\n                                        table.query,\n                                        orderby=table.title)\n                                        \n        options = [OPTION(SELECT_FILTER,\n                          _value=\"\",\n                          _class=\"filter-manager-prompt\",\n                          _disabled=\"disabled\")]\n        add_option = options.append\n        filters = {}\n        for row in rows:\n            filter_id = row[table._id]\n            add_option(OPTION(row.title, _value=filter_id))\n            query = row.query\n            if query:\n                query = json.loads(query)\n            filters[filter_id] = query\n        widget_id = \"%s-fm\" % form_id\n        widget = DIV(SELECT(options,\n                            _id=widget_id,\n                            _class=\"filter-manager-widget\"),\n                     _class=\"filter-manager-container\")\n\n        # JSON-serializable translator\n        T = current.T\n        _t = lambda s: str(T(s))\n\n        # Configure the widget\n        settings = current.deployment_settings\n        config = dict(\n\n            # Filters and Ajax URL\n            filters = filters,\n            ajaxURL = ajaxurl,\n\n            # Workflow Options\n            allowDelete = settings.get_search_filter_manager_allow_delete(),\n\n            # Tooltips for action icons/buttons\n            createTooltip = _t(\"Save current options as new filter\"),\n            loadTooltip = _t(\"Load filter\"),\n            saveTooltip = _t(\"Update saved filter\"),\n            deleteTooltip = _t(\"Delete saved filter\"),\n\n            # Hints\n            titleHint = _t(\"Enter a title...\"),\n            selectHint = str(SELECT_FILTER),\n            emptyHint = _t(\"No saved filters\"),\n\n            # Confirm update + confirmation text\n            confirmUpdate = _t(\"Update this filter?\"),\n            confirmDelete = _t(\"Delete this filter?\"),\n        )\n\n        # Render actions as buttons with text if configured, otherwise\n        # they will appear as empty DIVs with classes for CSS icons\n        create_text = settings.get_search_filter_manager_save()\n        if create_text:\n            config[\"createText\"] = _t(create_text)\n        update_text = settings.get_search_filter_manager_update()\n        if update_text:\n            config[\"saveText\"] = _t(update_text)\n        delete_text = settings.get_search_filter_manager_delete()\n        if delete_text:\n            config[\"deleteText\"] = _t(delete_text)\n        load_text = settings.get_search_filter_manager_load()\n        if load_text:\n            config[\"loadText\"] = _t(load_text)\n            \n        script = '''$(\"#%s\").filtermanager(%s)''' % (widget_id,\n                                                     json.dumps(config))\n\n        current.response.s3.jquery_ready.append(script)\n\n        return widget\n\n    # -------------------------------------------------------------------------\n    def json(self, resource, get_vars=None):\n        \"\"\"\n            Render this filter form as JSON (for Ajax requests)\n\n            @param resource: the S3Resource\n            @param get_vars: the request GET vars (URL query dict)\n        \"\"\"\n\n        raise NotImplementedError\n\n    # -------------------------------------------------------------------------\n    @staticmethod\n    def _formstyle(row_id, label, widget, comment, hidden=False):\n        \"\"\"\n            Default formstyle for search forms\n\n            @param row_id: HTML id for the row\n            @param label: the label\n            @param widget: the form widget\n            @param comment: the comment\n            @param hidden: whether the row should initially be hidden or not\n        \"\"\"\n\n        if hidden:\n            _class = \"advanced hide\"\n        else:\n            _class = \"\"\n\n        row = TR(TD(label, _class=\"w2p_fl\"), TD(widget),\n                 _id=row_id, _class=_class)\n\n        if comment:\n            row.append(TD(DIV(_class=\"tooltip\",\n                              _title=\"%s|%s\" % (label, comment)),\n                          _class=\"w2p_fc\"))\n        return row\n\n# =============================================================================\nclass S3Filter(S3Method):\n    \"\"\" Back-end for filter forms \"\"\"\n\n    def apply_method(self, r, **attr):\n        \"\"\"\n            Entry point for REST interface\n\n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n\n        representation = r.representation\n        if representation == \"options\":\n            # Return the filter options as JSON\n            return self._options(r, **attr)\n\n        elif representation == \"json\":\n            if r.http == \"GET\":\n                # Load list of saved filters\n                return self._load(r, **attr)\n            elif r.http == \"POST\":\n                if \"delete\" in r.get_vars:\n                    # Delete a filter\n                    return self._delete(r, **attr)\n                else:\n                    # Save a filter\n                    return self._save(r, **attr)\n            else:\n                r.error(405, r.ERROR.BAD_METHOD)\n                \n        elif representation == \"html\":\n            return self._form(r, **attr)\n\n        else:\n            r.error(501, r.ERROR.BAD_FORMAT)\n\n    # -------------------------------------------------------------------------\n    def _form(self, r, **attr):\n        \"\"\"\n            Get the filter form for the target resource as HTML snippet\n\n            GET filter.html\n\n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n\n        r.error(501, r.ERROR.NOT_IMPLEMENTED)\n\n    # -------------------------------------------------------------------------\n    def _options(self, r, **attr):\n        \"\"\"\n            Get the updated options for the filter form for the target\n            resource as JSON\n\n            GET filter.options\n\n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n\n        resource = self.resource\n        get_config = resource.get_config\n\n        options = {}\n\n        filter_widgets = get_config(\"filter_widgets\", None)\n        if filter_widgets:\n            fresource = current.s3db.resource(resource.tablename)\n\n            for widget in filter_widgets:\n                if hasattr(widget, \"ajax_options\"):\n                    opts = widget.ajax_options(fresource)\n                    if opts and isinstance(opts, dict):\n                        options.update(opts)\n\n        options = json.dumps(options)\n        current.response.headers[\"Content-Type\"] = \"application/json\"\n        return options\n\n    # -------------------------------------------------------------------------\n    def _delete(self, r, **attr):\n        \"\"\"\n            Delete a filter, responds to POST filter.json?delete=\n            \n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n            \n        # Authorization, get pe_id\n        auth = current.auth\n        if auth.s3_logged_in():\n            pe_id = current.auth.user.pe_id\n        else:\n            pe_id = None\n        if not pe_id:\n            r.unauthorised()\n\n        # Read the source\n        source = r.body\n        source.seek(0)\n\n        try:\n            data = json.load(source)\n        except ValueError:\n            # Syntax error: no JSON data\n            r.error(501, r.ERROR.BAD_SOURCE)\n\n        # Try to find the record\n        db = current.db\n        s3db = current.s3db\n\n        table = s3db.pr_filter\n        record = None\n        record_id = data.get(\"id\")\n        if record_id:\n            query = (table.id == record_id) \u0026 (table.pe_id == pe_id)\n            record = db(query).select(table.id, limitby=(0, 1)).first()\n        if not record:\n            r.error(501, r.ERROR.BAD_RECORD)\n            \n        resource = s3db.resource(\"pr_filter\", id=record_id)\n        success = resource.delete(ondelete=resource.get_config(\"ondelete\"),\n                                 format=r.representation)\n\n        if not success:\n            raise(400, current.manager.error)\n        else:\n            current.response.headers[\"Content-Type\"] = \"application/json\"\n            return current.xml.json_message(deleted=record_id)\n\n    # -------------------------------------------------------------------------\n    def _save(self, r, **attr):\n        \"\"\"\n            Save a filter, responds to POST filter.json\n            \n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n\n        # Authorization, get pe_id\n        auth = current.auth\n        if auth.s3_logged_in():\n            pe_id = current.auth.user.pe_id\n        else:\n            pe_id = None\n        if not pe_id:\n            r.unauthorised()\n\n        # Read the source\n        source = r.body\n        source.seek(0)\n\n        try:\n            data = json.load(source)\n        except ValueError:\n            r.error(501, r.ERROR.BAD_SOURCE)\n\n        # Try to find the record\n        db = current.db\n        s3db = current.s3db\n        \n        table = s3db.pr_filter\n        record_id = data.get(\"id\")\n        record = None\n        if record_id:\n            query = (table.id == record_id) \u0026 (table.pe_id == pe_id)\n            record = db(query).select(table.id, limitby=(0, 1)).first()\n            if not record:\n                r.error(404, r.ERROR.BAD_RECORD)\n\n        # Build new record\n        filter_data = {\n            \"pe_id\": pe_id,\n            \"controller\": r.controller,\n            \"function\": r.function,\n            \"resource\": self.resource.tablename,\n            \"deleted\": False,\n        }\n\n        title = data.get(\"title\")\n        if title is not None:\n            filter_data[\"title\"] = title\n\n        description = data.get(\"description\")\n        if description is not None:\n            filter_data[\"description\"] = description\n\n        query = data.get(\"query\")\n        if query is not None:\n            filter_data[\"query\"] = json.dumps(query)\n\n        url = data.get(\"url\")\n        if url is not None:\n            filter_data[\"url\"] = url\n\n        # Store record\n        onaccept = None\n        form = Storage(vars=filter_data)\n        if record:\n            success = db(table.id == record_id).update(**filter_data)\n            if success:\n                current.audit(\"update\", \"pr\", \"filter\", form, record_id, \"json\")\n                info = {\"updated\": record_id}\n                onaccept = s3db.get_config(table, \"update_onaccept\",\n                           s3db.get_config(table, \"onaccept\"))\n        else:\n            success = table.insert(**filter_data)\n            if success:\n                record_id = success\n                current.audit(\"create\", \"pr\", \"filter\", form, record_id, \"json\")\n                info = {\"created\": record_id}\n                onaccept = s3db.get_config(table, \"update_onaccept\",\n                           s3db.get_config(table, \"onaccept\"))\n\n        if onaccept is not None:\n            form.vars[\"id\"] = record_id\n            callback(onaccept, form)\n\n        # Success/Error response\n        xml = current.xml\n        if success:\n            msg = xml.json_message(**info)\n        else:\n            msg = xml.json_message(False, 400)\n        current.response.headers[\"Content-Type\"] = \"application/json\"\n        return msg\n\n    # -------------------------------------------------------------------------\n    def _load(self, r, **attr):\n        \"\"\"\n            Load filters\n\n            GET filter.json or GET filter.json?load=\u003cid\u003e\n            \n            @param r: the S3Request\n            @param attr: additional controller parameters\n        \"\"\"\n\n        db = current.db\n        table = current.s3db.pr_filter\n\n        # Authorization, get pe_id\n        auth = current.auth\n        if auth.s3_logged_in():\n            pe_id = current.auth.user.pe_id\n        else:\n            pe_id = None\n        if not pe_id:\n            r.unauthorized()\n\n        # Build query\n        query = (table.deleted != True) \u0026 \\\n                (table.resource == self.resource.tablename) \u0026 \\\n                (table.pe_id == pe_id)\n\n        # Any particular filters?\n        load = r.get_vars.get(\"load\")\n        if load:\n            record_ids = [i for i in load.split(\",\") if i.isdigit()]\n            if record_ids:\n                if len(record_ids) \u003e 1:\n                    query \u0026= table.id.belongs(record_ids)\n                else:\n                    query \u0026= table.id == record_ids[0]\n        else:\n            record_ids = None\n\n        # Retrieve filters\n        rows = db(query).select(table.id,\n                                table.title,\n                                table.description,\n                                table.query)\n\n        # Pack filters\n        filters = []\n        for row in rows:\n            filters.append({\n                \"id\": row.id,\n                \"title\": row.title,\n                \"description\": row.description,\n                \"query\": json.loads(row.query) if row.query else [],\n            })\n\n        # JSON response\n        current.response.headers[\"Content-Type\"] = \"application/json\"\n        return json.dumps(filters)\n\n# =============================================================================\nclass S3FilterString(object):\n    \"\"\"\n        Helper class to render a human-readable representation of a\n        filter query, as representation method of JSON-serialized\n        queries in saved filters.\n    \"\"\"\n\n    def __init__(self, resource, query):\n        \"\"\"\n            Constructor\n\n            @param query: the URL query (list of key-value pairs or a\n                          string with such a list in JSON)\n        \"\"\"\n\n        if type(query) is not list:\n            try:\n                self.query = json.loads(query)\n            except ValueError:\n                self.query = []\n        else:\n            self.query = query\n\n        get_vars = {}\n        for k, v in self.query:\n            if v is not None:\n                key = resource.prefix_selector(k)\n                if key in get_vars:\n                    value = get_vars[key]\n                    if type(value) is list:\n                        value.append(v)\n                    else:\n                        get_vars[key] = [value, v]\n                else:\n                    get_vars[key] = v\n\n        self.resource = resource\n        self.get_vars = get_vars\n\n    # -------------------------------------------------------------------------\n    def represent(self):\n        \"\"\" Render the query representation for the given resource \"\"\"\n\n        default = \"\"\n\n        get_vars = self.get_vars\n        resource = self.resource\n        if not get_vars:\n            return default\n        else:\n            queries = S3URLQuery.parse(resource, get_vars)\n\n        # Get alternative field labels\n        labels = {}\n        get_config = resource.get_config\n        prefix = resource.prefix_selector\n        for config in (\"list_fields\", \"notify_fields\"):\n            fields = get_config(config, set())\n            for f in fields:\n                if type(f) is tuple:\n                    labels[prefix(f[1])] = f[0]\n\n        # Iterate over the sub-queries\n        render = self._render\n        substrings = []\n        append = substrings.append\n        for alias, subqueries in queries.iteritems():\n\n            for subquery in subqueries:\n                s = render(resource, alias, subquery, labels=labels)\n                if s:\n                    append(s)\n\n        if substrings:\n            result = substrings[0]\n            T = current.T\n            for s in substrings[1:]:\n                result = T(\"%s AND %s\") % (result, s)\n            return result\n        else:\n            return default\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _render(cls, resource, alias, query, invert=False, labels=None):\n        \"\"\"\n            Recursively render a human-readable representation of a\n            S3ResourceQuery.\n\n            @param resource: the S3Resource\n            @param query: the S3ResourceQuery\n            @param invert: invert the query\n        \"\"\"\n\n        T = current.T\n\n        if not query:\n            return None\n\n        op = query.op\n\n        l = query.left\n        r = query.right\n        render = lambda q, r=resource, a=alias, invert=False, labels=labels: \\\n                        cls._render(r, a, q, invert=invert, labels=labels)\n\n        if op == query.AND:\n            # Recurse AND\n            l = render(l)\n            r = render(r)\n            if l is not None and r is not None:\n                if invert:\n                    result = T(\"NOT %s OR NOT %s\") % (l, r)\n                else:\n                    result = T(\"%s AND %s\") % (l, r)\n            else:\n                result = l if l is not None else r\n        elif op == query.OR:\n            # Recurse OR\n            l = render(l)\n            r = render(r)\n            if l is not None and r is not None:\n                if invert:\n                    result = T(\"NOT %s AND NOT %s\") % (l, r)\n                else:\n                    result = T(\"%s OR %s\") % (l, r)\n            else:\n                result = l if l is not None else r\n        elif op == query.NOT:\n            # Recurse NOT\n            result = render(l, invert=not invert)\n        else:\n            # Resolve the field selector against the resource\n            try:\n                rfield = l.resolve(resource)\n            except (AttributeError, SyntaxError):\n                return None\n\n            # Convert the filter values into the field type\n            try:\n                values = cls._convert(rfield, r)\n            except (TypeError, ValueError):\n                values = r\n\n            # Alias\n            selector = l.name\n            if labels and selector in labels:\n                rfield.label = labels[selector]\n            # @todo: for duplicate labels, show the table name\n            #else:\n                #tlabel = \" \".join(s.capitalize() for s in rfield.tname.split(\"_\")[1:])\n                #rfield.label = \"(%s) %s\" % (tlabel, rfield.label)\n\n            # Represent the values\n            if values is None:\n                values = T(\"None\")\n            else:\n                list_type = rfield.ftype[:5] == \"list:\"\n                renderer = rfield.represent\n                if not callable(renderer):\n                    renderer = lambda v: s3_unicode(v)\n                if hasattr(renderer, \"linkto\"):\n                    linkto = renderer.linkto\n                    renderer.linkto = None\n                else:\n                    linkto = None\n\n                is_list = type(values) is list\n\n                try:\n                    if is_list and hasattr(renderer, \"bulk\") and not list_type:\n                        fvalues = renderer.bulk(values, list_type=False)\n                        values = [fvalues[v] for v in values if v in fvalues]\n                    elif list_type:\n                        if is_list:\n                            values = renderer(values)\n                        else:\n                            values = renderer([values])\n                    else:\n                        if is_list:\n                            values = [renderer(v) for v in values]\n                        else:\n                            values = renderer(values)\n                except:\n                    values = s3_unicode(values)\n\n            # Translate the query\n            result = cls._translate_query(query, rfield, values, invert=invert)\n\n        return result\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _convert(cls, rfield, value):\n        \"\"\"\n            Convert a filter value according to the field type\n            before representation\n\n            @param rfield: the S3ResourceField\n            @param value: the value\n        \"\"\"\n\n        if value is None:\n            return value\n\n        ftype = rfield.ftype\n        if ftype[:5] == \"list:\":\n            if ftype[5:8] in (\"int\", \"ref\"):\n                ftype = long\n            else:\n                ftype = unicode\n        elif ftype == \"id\" or ftype [:9] == \"reference\":\n            ftype = long\n        elif ftype == \"integer\":\n            ftype = int\n        elif ftype == \"date\":\n            ftype = datetime.date\n        elif ftype == \"time\":\n            ftype = datetime.time\n        elif ftype == \"datetime\":\n            ftype = datetime.datetime\n        elif ftype == \"double\":\n            ftype = float\n        elif ftype == \"boolean\":\n            ftype = bool\n        else:\n            ftype = unicode\n\n        convert = S3TypeConverter.convert\n        if type(value) is list:\n            output = []\n            append = output.append\n            for v in value:\n                try:\n                    append(convert(ftype, v))\n                except TypeError, ValueError:\n                    continue\n        else:\n            try:\n                output = convert(ftype, value)\n            except TypeError, ValueError:\n                output = None\n        return output\n\n    # -------------------------------------------------------------------------\n    @classmethod\n    def _translate_query(cls, query, rfield, values, invert=False):\n        \"\"\"\n            Translate the filter query into human-readable language\n\n            @param query: the S3ResourceQuery\n            @param rfield: the S3ResourceField the query refers to\n            @param values: the filter values\n            @param invert: invert the operation\n        \"\"\"\n\n        T = current.T\n\n        # Value list templates\n        vor = T(\"%s or %s\")\n        vand = T(\"%s and %s\")\n\n        # Operator templates\n        otemplates = {\n            query.LT: (query.GE, vand, \"%(label)s \u003c %(values)s\"),\n            query.LE: (query.GT, vand, \"%(label)s \u003c= %(values)s\"),\n            query.EQ: (query.NE, vor, T(\"%(label)s is %(values)s\")),\n            query.GE: (query.LT, vand, \"%(label)s \u003e= %(values)s\"),\n            query.GT: (query.LE, vand, \"%(label)s \u003e %(values)s\"),\n            query.NE: (query.EQ, vor, T(\"%(label)s != %(values)s\")),\n            query.LIKE: (\"notlike\", vor, T(\"%(label)s like %(values)s\")),\n            query.BELONGS: (query.NE, vor, T(\"%(label)s = %(values)s\")),\n            query.CONTAINS: (\"notall\", vand, T(\"%(label)s contains %(values)s\")),\n            query.ANYOF: (\"notany\", vor, T(\"%(label)s contains any of %(values)s\")),\n            \"notall\": (query.CONTAINS, vand, T(\"%(label)s does not contain %(values)s\")),\n            \"notany\": (query.ANYOF, vor, T(\"%(label)s does not contain %(values)s\")),\n            \"notlike\": (query.LIKE, vor, T(\"%(label)s not like %(values)s\"))\n        }\n\n        # Quote values as necessary\n        ftype = rfield.ftype\n        if ftype in (\"string\", \"text\") or \\\n           ftype[:9] == \"reference\" or \\\n           ftype[:5] == \"list:\" and ftype[5:8] in (\"str\", \"ref\"):\n            if type(values) is list:\n                values = ['\"%s\"' % v for v in values]\n            elif values is not None:\n                values = '\"%s\"' % values\n            else:\n                values = current.messages[\"NONE\"]\n\n        # Render value list template\n        def render_values(template=None, values=None):\n            if not template or type(values) is not list:\n                return str(values)\n            elif not values:\n                return \"()\"\n            elif len(values) == 1:\n                return values[0]\n            else:\n                return template % (\", \".join(values[:-1]), values[-1])\n\n        # Render the operator template\n        op = query.op\n        if op in otemplates:\n            inversion, vtemplate, otemplate = otemplates[op]\n            if invert:\n                inversion, vtemplate, otemplate = otemplates[inversion]\n            return otemplate % dict(label=rfield.label,\n                                    values=render_values(vtemplate, values))\n        else:\n            # Fallback to simple representation\n            return query.represent(resource)\n\n# END =========================================================================\n"
}
{
    "repo_name": "bazwilliams/openhomedevice",
    "ref": "refs/heads/master",
    "path": "tests/DeviceProduct1Test.py",
    "copies": "1",
    "content": "import unittest\nimport os\nimport asyncio\n\nfrom openhomedevice.device import Device\nfrom aioresponses import aioresponses\n\n\ndef async_test(coro):\n    def wrapper(*args, **kwargs):\n        loop = asyncio.new_event_loop()\n        try:\n            return loop.run_until_complete(coro(*args, **kwargs))\n        finally:\n            loop.close()\n\n    return wrapper\n\n\nclass DeviceWithProduct1ServiceTests(unittest.TestCase):\n    @async_test\n    @aioresponses()\n    async def setUp(self, mocked):\n        LOCATION = \"http://mydevice:12345/desc.xml\"\n        with open(\n            os.path.join(os.path.dirname(__file__), \"data/v1description.xml\")\n        ) as file:\n            mocked.get(LOCATION, body=file.read())\n            mocked.get(\n                \"http://mydevice:12345/4c494e4e-1234-ab12-abcd-01234567819f/Upnp/av.openhome.org-Product-1/service.xml\",\n                body=\"\",\n            )\n            mocked.get(\n                \"http://mydevice:12345/4c494e4e-1234-ab12-abcd-01234567819f/Upnp/av.openhome.org-Volume-1/service.xml\",\n                body=\"\",\n            )\n            mocked.get(\n                \"http://mydevice:12345/4c494e4e-1234-ab12-abcd-01234567819f/Upnp/av.openhome.org-Info-1/service.xml\",\n                body=\"\",\n            )\n            mocked.get(\n                \"http://mydevice:12345/4c494e4e-1234-ab12-abcd-01234567819f/Upnp/av.openhome.org-Playlist-1/service.xml\",\n                body=\"\",\n            )\n        self.sut = Device(LOCATION)\n        await self.sut.init()\n        soap_request_calls = []\n        return super().setUp()\n\n    def test_product_service_exists(self):\n        self.assertIsNotNone(self.sut.product_service)\n"
}
{
    "repo_name": "spectras/django-compound-forms",
    "ref": "refs/heads/master",
    "path": "test_project/settings.py",
    "copies": "1",
    "content": "# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\nBASE_DIR = os.path.dirname(os.path.dirname(__file__))\n\nSECRET_KEY = 'b6s4u*8q7\u0026gh1(d04gasmx!^8y_zw#4z*2t90-tgm29*0u*9tb'\nDEBUG = True\nTEMPLATE_DEBUG = True\n\n# Application definition\nINSTALLED_APPS = (\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'app',\n    'tests',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n)\n\nROOT_URLCONF = 'project.urls'\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\nLANGUAGE_CODE = 'en'\nTIME_ZONE = 'UTC'\nUSE_I18N = True\nUSE_L10N = True\nUSE_TZ = True\n"
}
{
    "repo_name": "simonwanggh/mlrampup",
    "ref": "refs/heads/master",
    "path": "mlrampup/supervisedlearning/classifier/decideTreeID3.py",
    "copies": "1",
    "content": "from mlrampup.supervisedlearning.utils import dataProcessor as dp\n'''\ndataSet - 2D numpy array\nfeatureIndex - a tuple contain indexes of features\nlabelIndex - index of label column\n'''\ndef chooseBestFeatureToSplit(dataSet, featureIndexes , labelIndex = -1):\n    baseEntropy = calcShannonEntropy(dataSet, labelIndex)\n    bestFeteature = -1\n    dataSetSize = len(dataSet)\n    baseInfoGain = 0.0\n    for i in featureIndexes:\n        featValueList = [example[i] for example in dataSet]\n        uniqueValue = set(featValueList)\n        newEntropy = 0.0\n        for value in uniqueValue:\n            splitDataSet = dp.splitDataSet(dataSet,i,value)\n            prop = len(splitDataSet)/float(dataSetSize)\n            newEntropy += prop * calcShannonEntropy(splitDataSet,labelIndex)\n        infoGain = baseEntropy - newEntropy\n        if(infoGain \u003e baseInfoGain):\n            baseInfoGain = infoGain\n            bestFeteature = i\n    return bestFeteature,baseInfoGain\n\n\nfrom math import log\n'''\n2D array\nthe last column should be the label\nsample,\n[[1,1,'yes'],\n [1,1,'yes'],\n [1,0,'no' ],\n [0,1,'no' ]]\n the label index is -1 or 2\n\n the default label index is -1\n'''\ndef calcShannonEntropy(dataSet,labelIndex = -1):\n    numberEntities = len(dataSet)\n    labelCount = {}\n    for vert in dataSet:\n        label = vert[labelIndex]\n        if label not in labelCount.keys() :\n            labelCount[label] = 1\n        else:\n            labelCount[label] += 1\n    shannonEnt = 0.0\n    for key in labelCount:\n        prop = float(labelCount[key])/numberEntities\n        shannonEnt -= prop * log(prop,2)\n    return shannonEnt\n\n\ndef creatTree(dataSet, featIndexLabelDict,  labelIndex = -1):\n    labels = featIndexLabelDict.copy()\n    classList = [example[labelIndex] for example in dataSet]\n    if classList.count(classList[0]) == len(classList) :#all are the same class\n        return classList[0]\n    if len(labels) == 0 :# all features are travsed\n        return dp.majorityCount(classList)\n    bestFeat = chooseBestFeatureToSplit(dataSet,labels.keys(),labelIndex)[0]\n    bestFeatLabel = labels[bestFeat]\n    del labels[bestFeat]\n    myTree = {bestFeatLabel:{}}\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    for value in uniqueVals:\n         myTree[bestFeatLabel][value] = creatTree(dp.splitDataSet(dataSet,bestFeat,value),labels,labelIndex)\n\n    return myTree\n\n\ndef classify(item, labelDict, tree):\n    itemList = list(item)\n    for key in tree.keys():\n        branch = tree[key]\n        index = labelDict[key]\n        if type(branch).__name__ != 'dict':\n            return 'unknown'\n        for field in branch.keys():\n            fieldBranch = branch[field]\n            if field == item[index]:\n                if type(fieldBranch).__name__ == 'dict':\n                    return classify(item, labelDict, fieldBranch)\n                else:\n                    return fieldBranch\n\n    return 'unknown'\n\n"
}
{
    "repo_name": "hzlf/discogs-proxy",
    "ref": "refs/heads/master",
    "path": "website/apps/dgsproxy/migrations/0003_auto__del_cachedfile__add_field_cachedresource_content_type.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\nfrom south.utils import datetime_utils as datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Deleting model 'CachedFile'\n        db.delete_table(u'dgsproxy_cachedfile')\n\n        # Adding field 'CachedResource.content_type'\n        db.add_column(u'dgsproxy_cachedresource', 'content_type',\n                      self.gf('django.db.models.fields.CharField')(max_length=36, null=True, blank=True),\n                      keep_default=False)\n\n\n    def backwards(self, orm):\n        # Adding model 'CachedFile'\n        db.create_table(u'dgsproxy_cachedfile', (\n            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('uri', self.gf('django.db.models.fields.CharField')(max_length=1024, null=True, blank=True)),\n        ))\n        db.send_create_signal(u'dgsproxy', ['CachedFile'])\n\n        # Deleting field 'CachedResource.content_type'\n        db.delete_column(u'dgsproxy_cachedresource', 'content_type')\n\n\n    models = {\n        'dgsproxy.cachedresource': {\n            'Meta': {'object_name': 'CachedResource'},\n            'content_type': ('django.db.models.fields.CharField', [], {'max_length': '36', 'null': 'True', 'blank': 'True'}),\n            'created': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),\n            'file': ('django.db.models.fields.files.FileField', [], {'max_length': '512', 'null': 'True', 'blank': 'True'}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'status': ('django.db.models.fields.PositiveIntegerField', [], {'default': '0'}),\n            'type': ('django.db.models.fields.CharField', [], {'max_length': '56', 'null': 'True', 'db_index': 'True'}),\n            'updated': ('django.db.models.fields.DateTimeField', [], {'auto_now': 'True', 'blank': 'True'}),\n            'uri': ('django.db.models.fields.CharField', [], {'max_length': '512', 'null': 'True', 'db_index': 'True'})\n        }\n    }\n\n    complete_apps = ['dgsproxy']"
}
{
    "repo_name": "LaunchKey/launchkey-python",
    "ref": "refs/heads/master",
    "path": "launchkey/clients/base.py",
    "copies": "1",
    "content": "\"\"\"Base Client containing shared functionality for all clients\"\"\"\n\n# pylint: disable=too-few-public-methods, too-many-arguments\n\nfrom functools import wraps\nfrom uuid import UUID\nimport warnings\n\nfrom formencode import Invalid\n\nfrom launchkey.entities.service import Service, ServiceSecurityPolicy\nfrom launchkey.entities.service.policy import ConditionalGeoFencePolicy, \\\n    GeoCircleFence, TerritoryFence, LegacyPolicy, \\\n    MethodAmountPolicy, FactorsPolicy\nfrom launchkey.entities.shared import PublicKey\nfrom launchkey.entities.validation import ServiceValidator, \\\n    PublicKeyValidator, ServiceSecurityPolicyValidator\nfrom launchkey.exceptions import InvalidEntityID, LaunchKeyAPIException, \\\n    InvalidParameters, EntityNotFound, PolicyFailure, InvalidPolicyInput, \\\n    RequestTimedOut, RateLimited, InvalidDirectoryIdentifier, \\\n    UnexpectedAPIResponse, Forbidden, Unauthorized, InvalidRoute, \\\n    ServiceNameTaken, ServiceNotFound, PublicKeyAlreadyInUse, \\\n    InvalidPublicKey, PublicKeyDoesNotExist, LastRemainingKey, \\\n    LastRemainingSDKKey, InvalidSDKKey, DirectoryNameInUse, \\\n    AuthorizationInProgress, Conflict, AuthorizationResponseExists, \\\n    AuthorizationRequestCanceled, UnknownPolicyException, InvalidFenceType\nfrom launchkey.transports.base import APIResponse\nfrom launchkey.utils.shared import iso_format, deprecated\n\nERROR_CODE_MAP = {\n    \"ARG-001\": InvalidParameters,\n    \"ARG-002\": InvalidRoute,\n    \"SVC-001\": ServiceNameTaken,\n    \"SVC-002\": InvalidPolicyInput,\n    \"SVC-003\": PolicyFailure,\n    \"SVC-004\": ServiceNotFound,\n    \"SVC-005\": AuthorizationInProgress,\n    \"SVC-006\": AuthorizationResponseExists,\n    \"SVC-007\": AuthorizationRequestCanceled,\n    \"DIR-001\": InvalidDirectoryIdentifier,\n    \"KEY-001\": InvalidPublicKey,\n    \"KEY-002\": PublicKeyAlreadyInUse,\n    \"KEY-003\": PublicKeyDoesNotExist,\n    \"KEY-004\": LastRemainingKey,\n    \"ORG-003\": DirectoryNameInUse,\n    \"ORG-005\": LastRemainingSDKKey,\n    \"ORG-006\": InvalidSDKKey\n}\n\nSTATUS_CODE_MAP = {\n    401: Unauthorized,\n    403: Forbidden,\n    404: EntityNotFound,\n    408: RequestTimedOut,\n    409: Conflict,\n    429: RateLimited\n}\n\n\ndef api_call(function_):\n    \"\"\"\n    Decorator for handling LaunchKey API Exceptions\n    :param function_:\n    :return:\n    \"\"\"\n\n    @wraps(function_)\n    def wrapper(*args, **kwargs):\n        \"\"\"Decorator function\"\"\"\n\n        try:\n            return function_(*args, **kwargs)\n        except LaunchKeyAPIException as cause:\n            if not isinstance(cause.message, dict) \\\n                    or 'error_code' not in cause.message \\\n                    or 'error_detail' not in cause.message:\n                error_code = \"HTTP-%s\" % cause.status_code\n                error_detail = \"%s\" % cause.reason\n                error_data = None\n            else:\n                error_code = cause.message.get('error_code')\n                error_detail = cause.message.get('error_detail')\n                error_data = cause.message.get('error_data')\n            status_code = cause.status_code\n            if error_code in ERROR_CODE_MAP:\n                raise ERROR_CODE_MAP[error_code](error_detail, status_code,\n                                                 error_data=error_data)\n            if status_code in STATUS_CODE_MAP:\n                raise STATUS_CODE_MAP[status_code](error_detail, status_code,\n                                                   error_data=error_data)\n            raise\n\n    return wrapper\n\n\nclass BaseClient(object):\n    \"\"\"\n    Base Client for performing API queries against the LaunchKey API. Clients\n    are the interfaces that will be used by implementers to query against\n    specific entity based endpoints (service, directory, organization, etc).\n    \"\"\"\n\n    def __init__(self, subject_type, subject_id, transport):\n        \"\"\"\n        :param subject_type: Entity type that the service will represent\n        :param subject_id: Entity id of which the service will represent\n        :param transport: Transport class that will perform API queries\n        \"\"\"\n        try:\n            UUID(str(subject_id))\n        except ValueError:\n            raise InvalidEntityID(\"The given id was invalid. Please ensure \"\n                                  \"it is a UUID.\") from None\n        self._subject = \"%s:%s\" % (subject_type, subject_id)\n        self._transport = transport\n\n    @staticmethod\n    def _validate_response(api_response, validator):\n        if isinstance(api_response, APIResponse):\n            try:\n                return validator.to_python(api_response.data)\n            except Invalid:\n                raise UnexpectedAPIResponse(\n                    api_response.data,\n                    api_response.status_code) from None\n        else:\n            try:\n                return validator.to_python(api_response)\n            except Invalid:\n                raise UnexpectedAPIResponse(api_response) from None\n\n\nclass ServiceManagingBaseClient(BaseClient):\n    \"\"\"Base client containing shared code for managing Services\"\"\"\n\n    def __init__(self, subject_type, subject_id, transport, service_base_path):\n\n        super().__init__(subject_type, subject_id, transport)\n        self.__service_base_path = service_base_path\n\n    @api_call\n    def create_service(self, name, description=None, icon=None,\n                       callback_url=None, active=True):\n        \"\"\"\n        Creates a Service\n        :param name: Unique name that will be displayed in an Auth Request\n        :param description: Optional description that can be viewed in the\n        Admin Center or when retrieving the Service.\n        :param icon: Optional URL to an icon that will be displayed in an\n        Auth Request\n        :param callback_url: URL that Webhooks will be sent to\n        :param active: Whether the Service should be able to send Auth Requests\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters\n        were not correct\n        :raise: launchkey.exceptions.ServiceNameTaken - Service name\n        already taken\n        :return: String - ID of the Service that is created\n        \"\"\"\n        return self._transport.post(self.__service_base_path,\n                                    self._subject, name=name,\n                                    description=description,\n                                    icon=icon, callback_url=callback_url,\n                                    active=active).data['id']\n\n    @api_call\n    def get_all_services(self):\n        \"\"\"\n        Retrieves all Services belonging to the subject entity\n        :return: List - launchkey.entities.service.Service object containing\n        Service details\n        \"\"\"\n        response = self._transport.get(\n            self.__service_base_path,\n            self._subject)\n\n        services = []\n\n        for service_data in response.data:\n            validated_data = self._validate_response(\n                service_data, ServiceValidator)\n            service = Service(validated_data)\n            services.append(service)\n\n        return services\n\n    @api_call\n    def get_services(self, service_ids):\n        \"\"\"\n        Retrieves Services based on an input list of Service IDs\n        :param service_ids: List of unique Service IDs\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :return: List - launchkey.entities.service.Service object containing\n        Service details\n        \"\"\"\n        string_service_ids = [str(service_id) for service_id in service_ids]\n\n        response = self._transport.post(\n            \"{}/list\".format(self.__service_base_path),\n            self._subject, service_ids=string_service_ids)\n\n        services = []\n\n        for service_data in response.data:\n            validated_data = self._validate_response(\n                service_data, ServiceValidator)\n            service = Service(validated_data)\n            services.append(service)\n\n        return services\n\n    @api_call\n    def get_service(self, service_id):\n        \"\"\"\n        Retrieves a Service based on an input Service ID\n        :param service_id: Unique Service ID\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :return: launchkey.entities.service.Service object containing\n        Service details\n        \"\"\"\n        response = self._transport.post(\n            \"{}/list\".format(self.__service_base_path),\n            self._subject, service_ids=[str(service_id)])\n\n        service_data = self._validate_response(response.data[0],\n                                               ServiceValidator)\n        service = Service(service_data)\n\n        return service\n\n    @api_call\n    def update_service(self, service_id, name=False, description=False,\n                       icon=False, callback_url=False, active=None):\n        \"\"\"\n        Updates a Service's general settings. If an optional parameter is not\n        included it will not be updated.\n        :param service_id: Unique Service ID\n        :param name: Unique name that will be displayed in an Auth Request\n        :param description: Description that can be viewed in the Admin Center\n        or when retrieving the Service.\n        :param icon: URL to an icon that will be displayed in an Auth Request\n        :param callback_url: URL that Webhooks will be sent to\n        :param active: Whether the Service should be able to send Auth Requests\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :raise: launchkey.exceptions.ServiceNameTaken - Service name\n        already taken\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return:\n        \"\"\"\n        kwargs = {\"service_id\": str(service_id)}\n        if name is not False:\n            kwargs['name'] = name\n        if description is not False:\n            kwargs['description'] = description\n        if icon is not False:\n            kwargs['icon'] = icon\n        if callback_url is not False:\n            kwargs['callback_url'] = callback_url\n        if active is not None:\n            kwargs['active'] = active\n        self._transport.patch(self.__service_base_path, self._subject,\n                              **kwargs)\n\n    @api_call\n    def add_service_public_key(self, service_id, public_key, expires=None,\n                               active=None, key_type=None):\n        \"\"\"\n        Adds a public key to a Service\n        :param service_id: Unique Service ID\n        :param public_key: String RSA public key\n        :param expires: Optional datetime.datetime stating a time in which the\n        key will no longer be valid\n        :param active: Optional bool stating whether the key should be\n        considered active and usable.\n        :param key_type: Optional KeyType enum to identify whether the key is\n        an encryption key, signature key, or a dual use key\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :raise: launchkey.exceptions.InvalidPublicKey - The public key you\n        supplied is not valid.\n        :raise: launchkey.exceptions.PublicKeyAlreadyInUse - The public key\n        you supplied already exists for the requested entity. It cannot be\n        added again.\n        :return: MD5 fingerprint (key_id) of the public key,\n        IE: e0:2f:a9:5a:76:92:6b:b5:4d:24:67:19:d1:8a:0a:75\n        \"\"\"\n        kwargs = {\"service_id\": str(service_id), \"public_key\": public_key}\n        if expires is not None:\n            kwargs['date_expires'] = iso_format(expires)\n        if active is not None:\n            kwargs['active'] = active\n        if key_type is not None:\n            kwargs['key_type'] = key_type.value\n\n        key_id = self._transport.post(\n            \"{}/keys\".format(self.__service_base_path[0:-1]),\n            self._subject, **kwargs).data['key_id']\n        return key_id\n\n    @api_call\n    def get_service_public_keys(self, service_id):\n        \"\"\"\n        Retrieves a list of Public Keys belonging to a Service\n        :param service_id: Unique Service ID\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :raise: launchkey.exceptions.Forbidden - The Service you requested\n        either does not exist or you do not have sufficient permissions.\n        :return: List - launchkey.entities.shared.PublicKey\n        \"\"\"\n        response = self._transport.post(\n            \"{}/keys/list\".format(self.__service_base_path[0:-1]),\n            self._subject, service_id=str(service_id))\n\n        public_keys = []\n\n        for key in response.data:\n            key_data = self._validate_response(key, PublicKeyValidator)\n            public_key = PublicKey(key_data)\n            public_keys.append(public_key)\n\n        return public_keys\n\n    @api_call\n    def remove_service_public_key(self, service_id, key_id):\n        \"\"\"\n        Removes a public key from a Service. You may only remove\n        a public key if other public keys exist. If you wish for a last\n        remaining key to no longer be usable, use  update_service_public_key to\n        instead and set it as inactive.\n        :param service_id: Unique Service ID\n        :param key_id: MD5 fingerprint of the public key,\n        IE: e0:2f:a9:5a:76:92:6b:b5:4d:24:67:19:d1:8a:0a:75\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters\n        were not correct\n        :raise: launchkey.exceptions.PublicKeyDoesNotExist - The key_id you\n        ]supplied could not be found\n        :raise: launchkey.exceptions.LastRemainingKey - The last remaining\n        public key cannot be removed\n        :raise: launchkey.exceptions.Forbidden - The Service you requested\n        either does not exist or you do not have sufficient permissions.\n        :return:\n        \"\"\"\n        self._transport.delete(\n            \"{}/keys\".format(self.__service_base_path[0:-1]),\n            self._subject, service_id=str(service_id), key_id=key_id)\n\n    @api_call\n    def update_service_public_key(self, service_id, key_id, expires=False,\n                                  active=None):\n        \"\"\"\n        Updates a public key from a Service\n        :param service_id: Unique Service ID\n        :param key_id: MD5 fingerprint of the public key,\n        IE: e0:2f:a9:5a:76:92:6b:b5:4d:24:67:19:d1:8a:0a:75\n        :param expires: datetime.datetime stating a time in which the key will\n        no longer be valid\n        :param active: Bool stating whether the key should be considered active\n        and usable\n        :raise: launchkey.exceptions.PublicKeyDoesNotExist - The key_id you\n        supplied could not be found\n        :raise: launchkey.exceptions.Forbidden - The Service you requested\n        either does not exist or you do not have sufficient permissions.\n        :return:\n        \"\"\"\n        kwargs = {\"service_id\": str(service_id), \"key_id\": key_id}\n        if active is not None:\n            kwargs['active'] = active\n        if expires is not False:\n            kwargs['date_expires'] = iso_format(expires)\n\n        self._transport.patch(\n            \"{}/keys\".format(self.__service_base_path[0:-1]),\n            self._subject, **kwargs)\n\n    @deprecated\n    def get_service_policy(self, service_id):\n        \"\"\"\n        NOTE: This method is being deprecated. Use\n        `get_advanced_service_policy` instead!\n\n        Retrieves a Service's Security Policy\n        :param service_id: Unique Service ID\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return: launchkey.entities.service.ServiceSecurityPolicy object\n        containing policy details\n        :return: None if policy returned from `get_advanced_service_policy` is\n        not a legacy policy\n        \"\"\"\n        current_policy = self.get_advanced_service_policy(service_id)\n        if not isinstance(current_policy, LegacyPolicy):\n            warnings.warn(\"Policy received was not a legacy policy and cannot \"\n                          \"be converted into a ServiceSecurityPolicy.\",\n                          category=DeprecationWarning)\n\n            return None\n\n        policy = ServiceSecurityPolicy(\n            any=current_policy.amount,\n            knowledge=current_policy.knowledge_required,\n            inherence=current_policy.inherence_required,\n            possession=current_policy.possession_required,\n            jailbreak_protection=current_policy.deny_rooted_jailbroken\n        )\n\n        for fence in current_policy.fences:\n            policy.add_geofence(\n                fence.latitude,\n                fence.longitude,\n                fence.radius,\n                name=fence.name\n            )\n\n        for fence in current_policy.time_restrictions:\n            policy.add_timefence(\n                fence.name,\n                fence.start_time,\n                fence.end_time,\n                monday=fence.monday,\n                tuesday=fence.tuesday,\n                wednesday=fence.wednesday,\n                thursday=fence.thursday,\n                friday=fence.friday,\n                saturday=fence.saturday,\n                sunday=fence.sunday\n            )\n\n        return policy\n\n    @api_call\n    def get_advanced_service_policy(self, service_id):\n        \"\"\"\n        Retrieves a Service's Security Policy\n        :param service_id: Unique Service ID\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return: ConditionalGeoFencePolicy, FactorsPolicy, MethodAmountPolicy\n        or LegacyPolicy\n        :raises UnknownPolicyError: in the event an unrecognized policy type\n        is received\n        :raises InvalidFenceType: in the event an unrecognized fence type is\n        received\n        \"\"\"\n        response = self._transport.post(\n            \"{}/policy/item\".format(self.__service_base_path[0:-1]),\n            self._subject, service_id=str(service_id))\n\n        policy_data = self._validate_response(response.data,\n                                              ServiceSecurityPolicyValidator)\n\n        if policy_data[\"type\"] == \"COND_GEO\":\n            inside = self.__process_nested_service_policy(\n                policy_data[\"inside\"]\n            )\n            outside = self.__process_nested_service_policy(\n                policy_data[\"outside\"]\n            )\n\n            policy = ConditionalGeoFencePolicy(\n                inside,\n                outside,\n                deny_rooted_jailbroken=policy_data[\"deny_rooted_jailbroken\"],\n                deny_emulator_simulator=policy_data[\"deny_emulator_simulator\"],\n                fences=self.__generate_fence_objects_from_policy(policy_data)\n            )\n        elif policy_data[\"type\"] == \"LEGACY\":\n            ss_policy = ServiceSecurityPolicy()\n            ss_policy.set_policy(policy_data)\n            policy = self.__service_security_policy_to_legacy_policy(ss_policy)\n        elif policy_data[\"type\"] == \"METHOD_AMOUNT\":\n            policy = MethodAmountPolicy(\n                deny_rooted_jailbroken=policy_data[\n                    \"deny_rooted_jailbroken\"],\n                deny_emulator_simulator=policy_data[\n                    \"deny_emulator_simulator\"],\n                fences=self.__generate_fence_objects_from_policy(policy_data),\n                amount=policy_data[\"amount\"]\n            )\n        elif policy_data[\"type\"] == \"FACTORS\":\n            policy = FactorsPolicy(\n                deny_rooted_jailbroken=policy_data[\n                    \"deny_rooted_jailbroken\"],\n                deny_emulator_simulator=policy_data[\n                    \"deny_emulator_simulator\"],\n                inherence_required=\"INHERENCE\" in policy_data[\"factors\"],\n                knowledge_required=\"KNOWLEDGE\" in policy_data[\"factors\"],\n                possession_required=\"POSSESSION\" in policy_data[\"factors\"],\n                fences=self.__generate_fence_objects_from_policy(policy_data)\n            )\n        else:\n            raise UnknownPolicyException(\n                \"The Policy {0} was not a known Policy type\".format(\n                    policy_data[\"type\"])\n            )\n\n        return policy\n\n    def __service_security_policy_to_legacy_policy(self, policy):\n        return LegacyPolicy(\n            amount=policy.minimum_amount,\n            inherence_required=\"inherence\" in policy.minimum_requirements,\n            knowledge_required=\"knowledge\" in policy.minimum_requirements,\n            possession_required=\"possession\" in policy.minimum_requirements,\n            deny_rooted_jailbroken=policy.jailbreak_protection,\n            fences=list(map(self.__geofence_to_geo_circle, policy.geofences)),\n            time_restrictions=policy.timefences\n        )\n\n    @staticmethod\n    def __geofence_to_geo_circle(geofence):\n        return GeoCircleFence(\n            geofence.latitude,\n            geofence.longitude,\n            geofence.radius,\n            name=geofence.name\n        )\n\n    @staticmethod\n    def __generate_fence_objects_from_policy(policy):\n        fences = list()\n        for fence in policy[\"fences\"]:\n            if fence[\"type\"] == \"GEO_CIRCLE\":\n                fences.append(\n                    GeoCircleFence(\n                        latitude=fence[\"latitude\"],\n                        longitude=fence[\"longitude\"],\n                        radius=fence[\"radius\"],\n                        name=fence[\"name\"]\n                    )\n                )\n            elif fence[\"type\"] == \"TERRITORY\":\n                fences.append(\n                    TerritoryFence(\n                        country=fence[\"country\"],\n                        administrative_area=fence[\"administrative_area\"],\n                        postal_code=fence[\"postal_code\"],\n                        name=fence[\"name\"]\n                    )\n                )\n            else:\n                raise InvalidFenceType(\n                    \"Fence type \\\"{0}\\\" was not a valid Fence type\".format(\n                        fence[\"type\"]\n                    )\n                )\n\n        return fences\n\n    @staticmethod\n    def __process_nested_service_policy(policy):\n        if policy[\"type\"] == \"METHOD_AMOUNT\":\n            new_policy = MethodAmountPolicy(\n                amount=policy[\"amount\"],\n                deny_rooted_jailbroken=None,\n                deny_emulator_simulator=None,\n                fences=policy[\"fences\"]\n            )\n        elif policy[\"type\"] == \"FACTORS\":\n            new_policy = FactorsPolicy(\n                deny_rooted_jailbroken=None,\n                deny_emulator_simulator=None,\n                inherence_required=\"INHERENCE\" in policy[\"factors\"],\n                knowledge_required=\"KNOWLEDGE\" in policy[\"factors\"],\n                possession_required=\"POSSESSION\" in policy[\"factors\"],\n                fences=policy[\"fences\"]\n            )\n        else:\n            raise UnknownPolicyException(\n                \"Valid nested Policy types for ConditionalGeofence Policies \"\n                \"are: [\\\"METHOD_AMOUNT\\\", \\\"FACTORS\\\"]\"\n            )\n        return new_policy\n\n    @deprecated\n    def set_service_policy(self, service_id, policy):\n        \"\"\"\n        NOTE: This method is being deprecated. Use\n        `set_advanced_service_policy` instead!\n\n        Sets a Service's Security Policy\n        :param service_id: Unique Service ID\n        :param policy: launchkey.clients.shared.ServiceSecurityPolicy\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return:\n        \"\"\"\n        self.set_advanced_service_policy(service_id, policy)\n\n    @api_call\n    def set_advanced_service_policy(self, service_id, policy):\n        \"\"\"\n        Sets a Service's Security Policy\n        :param service_id: Unique Service ID\n        :param policy: LegacyPolicy, ConditionalGeoFencePolicy,\n        MethodAmountPolicy, FactorsPolicy, or ServiceSecurityPolicy\n        :raise: launchkey.exceptions.InvalidParameters - Input parameters were\n        not correct\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return:\n        \"\"\"\n        self._transport.put(\n            \"{}/policy\".format(self.__service_base_path[0:-1]),\n            self._subject, service_id=str(service_id),\n            policy=policy.to_dict())\n\n    @api_call\n    def remove_service_policy(self, service_id):\n        \"\"\"\n        Resets a Service's Security Policy back to default\n        :param service_id: Unique Service ID\n        :raise: launchkey.exceptions.ServiceNotFound - No Service could be\n        found matching the input ID\n        :return:\n        \"\"\"\n        self._transport.delete(\n            \"{}/policy\".format(self.__service_base_path[0:-1]),\n            self._subject, service_id=str(service_id))\n"
}
{
    "repo_name": "mike840609/django_web",
    "ref": "refs/heads/master",
    "path": "blog/urls.py",
    "copies": "1",
    "content": "from django.conf.urls import url\nfrom . import views\n\n# As you can see, we're now assigning a view called post_list\nurlpatterns = [\n    #  call post_list function\n    url(r'^$', views.post_list, name='post_list'),\n\n    # \\d  also tells us that it can only be a digit, not a letter (so everything between 0 and 9)\n    #  +  means that there needs to be one or more digits there\n    url(r'^post/(?P\u003cpk\u003e\\d+)/$', views.post_detail, name='post_detail'),\n]"
}
{
    "repo_name": "diego-d5000/MisValesMd",
    "ref": "refs/heads/master",
    "path": "env/lib/python2.7/site-packages/django/views/static.py",
    "copies": "1",
    "content": "\"\"\"\r\nViews and functions for serving static files. These are only to be used\r\nduring development, and SHOULD NOT be used in a production setting.\r\n\"\"\"\r\nfrom __future__ import unicode_literals\r\n\r\nimport mimetypes\r\nimport os\r\nimport posixpath\r\nimport re\r\nimport stat\r\n\r\nfrom django.http import (\r\n    FileResponse, Http404, HttpResponse, HttpResponseNotModified,\r\n    HttpResponseRedirect,\r\n)\r\nfrom django.template import Context, Engine, TemplateDoesNotExist, loader\r\nfrom django.utils.http import http_date, parse_http_date\r\nfrom django.utils.six.moves.urllib.parse import unquote\r\nfrom django.utils.translation import ugettext as _, ugettext_lazy\r\n\r\n\r\ndef serve(request, path, document_root=None, show_indexes=False):\r\n    \"\"\"\r\n    Serve static files below a given point in the directory structure.\r\n\r\n    To use, put a URL pattern such as::\r\n\r\n        from django.views.static import serve\r\n\r\n        url(r'^(?P\u003cpath\u003e.*)$', serve, {'document_root': '/path/to/my/files/'})\r\n\r\n    in your URLconf. You must provide the ``document_root`` param. You may\r\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\r\n    of the directory.  This index view will use the template hardcoded below,\r\n    but if you'd like to override it, you can create a template called\r\n    ``static/directory_index.html``.\r\n    \"\"\"\r\n    path = posixpath.normpath(unquote(path))\r\n    path = path.lstrip('/')\r\n    newpath = ''\r\n    for part in path.split('/'):\r\n        if not part:\r\n            # Strip empty path components.\r\n            continue\r\n        drive, part = os.path.splitdrive(part)\r\n        head, part = os.path.split(part)\r\n        if part in (os.curdir, os.pardir):\r\n            # Strip '.' and '..' in path.\r\n            continue\r\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\r\n    if newpath and path != newpath:\r\n        return HttpResponseRedirect(newpath)\r\n    fullpath = os.path.join(document_root, newpath)\r\n    if os.path.isdir(fullpath):\r\n        if show_indexes:\r\n            return directory_index(newpath, fullpath)\r\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\r\n    if not os.path.exists(fullpath):\r\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\r\n    # Respect the If-Modified-Since header.\r\n    statobj = os.stat(fullpath)\r\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\r\n                              statobj.st_mtime, statobj.st_size):\r\n        return HttpResponseNotModified()\r\n    content_type, encoding = mimetypes.guess_type(fullpath)\r\n    content_type = content_type or 'application/octet-stream'\r\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\r\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\r\n    if stat.S_ISREG(statobj.st_mode):\r\n        response[\"Content-Length\"] = statobj.st_size\r\n    if encoding:\r\n        response[\"Content-Encoding\"] = encoding\r\n    return response\r\n\r\n\r\nDEFAULT_DIRECTORY_INDEX_TEMPLATE = \"\"\"\r\n{% load i18n %}\r\n\u003c!DOCTYPE html\u003e\r\n\u003chtml lang=\"en\"\u003e\r\n  \u003chead\u003e\r\n    \u003cmeta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" /\u003e\r\n    \u003cmeta http-equiv=\"Content-Language\" content=\"en-us\" /\u003e\r\n    \u003cmeta name=\"robots\" content=\"NONE,NOARCHIVE\" /\u003e\r\n    \u003ctitle\u003e{% blocktrans %}Index of {{ directory }}{% endblocktrans %}\u003c/title\u003e\r\n  \u003c/head\u003e\r\n  \u003cbody\u003e\r\n    \u003ch1\u003e{% blocktrans %}Index of {{ directory }}{% endblocktrans %}\u003c/h1\u003e\r\n    \u003cul\u003e\r\n      {% ifnotequal directory \"/\" %}\r\n      \u003cli\u003e\u003ca href=\"../\"\u003e../\u003c/a\u003e\u003c/li\u003e\r\n      {% endifnotequal %}\r\n      {% for f in file_list %}\r\n      \u003cli\u003e\u003ca href=\"{{ f|urlencode }}\"\u003e{{ f }}\u003c/a\u003e\u003c/li\u003e\r\n      {% endfor %}\r\n    \u003c/ul\u003e\r\n  \u003c/body\u003e\r\n\u003c/html\u003e\r\n\"\"\"\r\ntemplate_translatable = ugettext_lazy(\"Index of %(directory)s\")\r\n\r\n\r\ndef directory_index(path, fullpath):\r\n    try:\r\n        t = loader.select_template([\r\n            'static/directory_index.html',\r\n            'static/directory_index',\r\n        ])\r\n    except TemplateDoesNotExist:\r\n        t = Engine().from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\r\n    files = []\r\n    for f in os.listdir(fullpath):\r\n        if not f.startswith('.'):\r\n            if os.path.isdir(os.path.join(fullpath, f)):\r\n                f += '/'\r\n            files.append(f)\r\n    c = Context({\r\n        'directory': path + '/',\r\n        'file_list': files,\r\n    })\r\n    return HttpResponse(t.render(c))\r\n\r\n\r\ndef was_modified_since(header=None, mtime=0, size=0):\r\n    \"\"\"\r\n    Was something modified since the user last downloaded it?\r\n\r\n    header\r\n      This is the value of the If-Modified-Since header.  If this is None,\r\n      I'll just return True.\r\n\r\n    mtime\r\n      This is the modification time of the item we're talking about.\r\n\r\n    size\r\n      This is the size of the item we're talking about.\r\n    \"\"\"\r\n    try:\r\n        if header is None:\r\n            raise ValueError\r\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\r\n                           re.IGNORECASE)\r\n        header_mtime = parse_http_date(matches.group(1))\r\n        header_len = matches.group(3)\r\n        if header_len and int(header_len) != size:\r\n            raise ValueError\r\n        if int(mtime) \u003e header_mtime:\r\n            raise ValueError\r\n    except (AttributeError, ValueError, OverflowError):\r\n        return True\r\n    return False\r\n"
}
{
    "repo_name": "moranzcw/Computer-Networking-A-Top-Down-Approach-NOTES",
    "ref": "refs/heads/master",
    "path": "Resource/7th-Python-Solution/Solutions/ProxyServer/ProxyServer.py",
    "copies": "1",
    "content": "from socket import *\nimport sys\n\nif len(sys.argv) \u003c= 1:\n\tprint('Usage : \"python ProxyServer.py server_ip\"\\n[server_ip : It is the IP Address Of Proxy Server')\n\tsys.exit(2)\n\t\n# Create a server socket, bind it to a port and start listening\ntcpSerSock = socket(AF_INET, SOCK_STREAM)\ntcpSerSock.bind((sys.argv[1], 8888))\ntcpSerSock.listen(100)\n\nwhile 1:\n\t# Strat receiving data from the client\n\tprint('Ready to serve...')\n\ttcpCliSock, addr = tcpSerSock.accept()\n\tprint('Received a connection from:', addr)\n\tmessage = tcpCliSock.recv(1024)\n\tprint(message)\n\t# Extract the filename from the given message\n\tprint(message.split()[1])\n\tfilename = message.split()[1].partition(\"/\")[2]\n\tprint(filename)\n\tfileExist = \"false\"\n\tfiletouse = \"/\" + filename\n\tprint(filetouse)\n\ttry:\n\t\t# Check wether the file exist in the cache\n\t\tf = open(filetouse[1:], \"r\")                      \n\t\toutputdata = f.readlines()                        \n\t\tfileExist = \"true\"\n\t\t# ProxyServer finds a cache hit and generates a response message\n\t\ttcpCliSock.send(\"HTTP/1.0 200 OK\\r\\n\")            \n\t\ttcpCliSock.send(\"Content-Type:text/html\\r\\n\")\n\t\tfor i in range(0, len(outputdata)):               \n\t\t\ttcpCliSock.send(outputdata[i])\n\t\t\tprint('Read from cache')   \n\t# Error handling for file not found in cache\n\texcept IOError:\n\t\tif fileExist == \"false\": \n\t\t\t# Create a socket on the proxyserver\n\t\t\tc = socket(AF_INET, SOCK_STREAM)            \n\t\t\thostn = filename.replace(\"www.\",\"\",1)         \n\t\t\tprint(hostn)                                   \n\t\t\ttry:\n\t\t\t\t# Connect to the socket to port 80\n\t\t\t\tc.connect((hostn, 80))\n\t\t\t\t# Create a temporary file on this socket and ask port 80 for the file requested by the client\n\t\t\t\tfileobj = c.makefile('r', 0)               \n\t\t\t\tfileobj.write(\"GET \"+\"http://\" + filename + \" HTTP/1.0\\n\\n\")  \n\t\t\t\t# Read the response into buffer\n\t\t\t\tbuff = fileobj.readlines()\n\t\t\t\t# Create a new file in the cache for the requested file. Also send the response in the buffer to client socket and the corresponding file in the cache\n\t\t\t\ttmpFile = open(\"./\" + filename,\"wb\")  \n\t\t\t\tfor line in buff:                                                     \n\t\t\t\t\ttmpFile.write(line);                                               \n\t\t\t\t\ttcpCliSock.send(line);\n\t\t\texcept:\n\t\t\t\tprint(\"Illegal request\")                                               \n\t\telse:\n\t\t\t# HTTP response message for file not found\n\t\t\ttcpCliSock.send(\"HTTP/1.0 404 sendErrorErrorError\\r\\n\")                             \n\t\t\ttcpCliSock.send(\"Content-Type:text/html\\r\\n\")\n\t\t\ttcpCliSock.send(\"\\r\\n\")\n\t# Close the client and the server sockets    \n\ttcpCliSock.close() \ntcpSerSock.close()\n"
}
{
    "repo_name": "samuelhwilliams/cryptracker-web",
    "ref": "refs/heads/master",
    "path": "app/tests/test_user.py",
    "copies": "1",
    "content": "# app/server/tests/test_user.py\n\n\nimport datetime\nimport unittest\n\nfrom flask_login import current_user\n\nfrom base import BaseTestCase\nfrom app.server import bcrypt\nfrom app.server.models import User\nfrom app.server.user.forms import LoginForm\n\n\nclass TestUserBlueprint(BaseTestCase):\n\n    def test_correct_login(self):\n        # Ensure login behaves correctly with correct credentials.\n        with self.client:\n            response = self.client.post(\n                '/login',\n                data=dict(email=\"ad@min.com\", password=\"admin_user\"),\n                follow_redirects=True\n            )\n            self.assertIn(b'Welcome', response.data)\n            self.assertIn(b'Logout', response.data)\n            self.assertIn(b'Members', response.data)\n            self.assertTrue(current_user.email == \"ad@min.com\")\n            self.assertTrue(current_user.is_active())\n            self.assertEqual(response.status_code, 200)\n\n    def test_logout_behaves_correctly(self):\n        # Ensure logout behaves correctly - regarding the session.\n        with self.client:\n            self.client.post(\n                '/login',\n                data=dict(email=\"ad@min.com\", password=\"admin_user\"),\n                follow_redirects=True\n            )\n            response = self.client.get('/logout', follow_redirects=True)\n            self.assertIn(b'You were logged out. Bye!', response.data)\n            self.assertFalse(current_user.is_active)\n\n    def test_logout_route_requires_login(self):\n        # Ensure logout route requres logged in user.\n        response = self.client.get('/logout', follow_redirects=True)\n        self.assertIn(b'Please log in to access this page', response.data)\n\n    def test_member_route_requires_login(self):\n        # Ensure member route requres logged in user.\n        response = self.client.get('/members', follow_redirects=True)\n        self.assertIn(b'Please log in to access this page', response.data)\n\n    def test_validate_success_login_form(self):\n        # Ensure correct data validates.\n        form = LoginForm(email='ad@min.com', password='admin_user')\n        self.assertTrue(form.validate())\n\n    def test_validate_invalid_email_format(self):\n        # Ensure invalid email format throws error.\n        form = LoginForm(email='unknown', password='example')\n        self.assertFalse(form.validate())\n\n    def test_get_by_id(self):\n        # Ensure id is correct for the current/logged in user.\n        with self.client:\n            self.client.post('/login', data=dict(\n                email='ad@min.com', password='admin_user'\n            ), follow_redirects=True)\n            self.assertTrue(current_user.id == 1)\n\n    def test_registered_on_defaults_to_datetime(self):\n        # Ensure that registered_on is a datetime.\n        with self.client:\n            self.client.post('/login', data=dict(\n                email='ad@min.com', password='admin_user'\n            ), follow_redirects=True)\n            user = User.query.filter_by(email='ad@min.com').first()\n            self.assertIsInstance(user.registered_on, datetime.datetime)\n\n    def test_check_password(self):\n        # Ensure given password is correct after unhashing.\n        user = User.query.filter_by(email='ad@min.com').first()\n        self.assertTrue(bcrypt.check_password_hash(user.password, 'admin_user'))\n        self.assertFalse(bcrypt.check_password_hash(user.password, 'foobar'))\n\n    def test_validate_invalid_password(self):\n        # Ensure user can't login when the pasword is incorrect.\n        with self.client:\n            response = self.client.post('/login', data=dict(\n                email='ad@min.com', password='foo_bar'\n            ), follow_redirects=True)\n        self.assertIn(b'Invalid email and/or password.', response.data)\n\n    def test_register_route(self):\n        # Ensure about route behaves correctly.\n        response = self.client.get('/register', follow_redirects=True)\n        self.assertIn(b'\u003ch1\u003ePlease Register\u003c/h1\u003e\\n', response.data)\n\n    def test_user_registration(self):\n        # Ensure registration behaves correctlys.\n        with self.client:\n            response = self.client.post(\n                '/register',\n                data=dict(email=\"test@tester.com\", password=\"testing\",\n                          confirm=\"testing\"),\n                follow_redirects=True\n            )\n            self.assertIn(b'Welcome', response.data)\n            self.assertTrue(current_user.email == \"test@tester.com\")\n            self.assertTrue(current_user.is_active())\n            self.assertEqual(response.status_code, 200)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"
}
{
    "repo_name": "Widdershin/PyGM",
    "ref": "refs/heads/master",
    "path": "core/input/keys.py",
    "copies": "1",
    "content": "import pygame\n\n\ndef createKeys():\n\t\"\"\"\n\tRemaps the Pygame key constants to a PyGM dict yo\n\t\"\"\"\n\tkeys = {}\n\tkeys['backspace'] = pygame.K_BACKSPACE\n\tkeys['tab'] = pygame.K_TAB\n\tkeys['clear'] = pygame.K_CLEAR\n\tkeys['return'] = pygame.K_RETURN\n\tkeys['pause'] = pygame.K_PAUSE\n\tkeys['escape'] = pygame.K_ESCAPE\n\tkeys['space'] = pygame.K_SPACE\n\tkeys['exclaim'] = pygame.K_EXCLAIM\n\tkeys['quotedbl'] = pygame.K_QUOTEDBL\n\tkeys['hash'] = pygame.K_HASH\n\tkeys['dollar'] = pygame.K_DOLLAR\n\tkeys['ampersand'] = pygame.K_AMPERSAND\n\tkeys['quote'] = pygame.K_QUOTE\n\tkeys['leftparen'] = pygame.K_LEFTPAREN\n\tkeys['rightparen'] = pygame.K_RIGHTPAREN\n\tkeys['asterisk'] = pygame.K_ASTERISK\n\tkeys['plus'] = pygame.K_PLUS\n\tkeys['comma'] = pygame.K_COMMA\n\tkeys['minus'] = pygame.K_MINUS\n\tkeys['period'] = pygame.K_PERIOD\n\tkeys['slash'] = pygame.K_SLASH\n\tkeys['0'] = pygame.K_0\n\tkeys['1'] = pygame.K_1\n\tkeys['2'] = pygame.K_2\n\tkeys['3'] = pygame.K_3\n\tkeys['4'] = pygame.K_4\n\tkeys['5'] = pygame.K_5\n\tkeys['6'] = pygame.K_6\n\tkeys['7'] = pygame.K_7\n\tkeys['8'] = pygame.K_8\n\tkeys['9'] = pygame.K_9\n\tkeys['colon'] = pygame.K_COLON\n\tkeys['semicolon'] = pygame.K_SEMICOLON\n\tkeys['less'] = pygame.K_LESS\n\tkeys['equals'] = pygame.K_EQUALS\n\tkeys['greater'] = pygame.K_GREATER\n\tkeys['question'] = pygame.K_QUESTION\n\tkeys['at'] = pygame.K_AT\n\tkeys['leftbracket'] = pygame.K_LEFTBRACKET\n\tkeys['backslash'] = pygame.K_BACKSLASH\n\tkeys['rightbracket'] = pygame.K_RIGHTBRACKET\n\tkeys['caret'] = pygame.K_CARET\n\tkeys['underscore'] = pygame.K_UNDERSCORE\n\tkeys['backquote'] = pygame.K_BACKQUOTE\n\tkeys['a'] = pygame.K_a\n\tkeys['b'] = pygame.K_b\n\tkeys['c'] = pygame.K_c\n\tkeys['d'] = pygame.K_d\n\tkeys['e'] = pygame.K_e\n\tkeys['f'] = pygame.K_f\n\tkeys['g'] = pygame.K_g\n\tkeys['h'] = pygame.K_h\n\tkeys['i'] = pygame.K_i\n\tkeys['j'] = pygame.K_j\n\tkeys['k'] = pygame.K_k\n\tkeys['l'] = pygame.K_l\n\tkeys['m'] = pygame.K_m\n\tkeys['n'] = pygame.K_n\n\tkeys['o'] = pygame.K_o\n\tkeys['p'] = pygame.K_p\n\tkeys['q'] = pygame.K_q\n\tkeys['r'] = pygame.K_r\n\tkeys['s'] = pygame.K_s\n\tkeys['t'] = pygame.K_t\n\tkeys['u'] = pygame.K_u\n\tkeys['v'] = pygame.K_v\n\tkeys['w'] = pygame.K_w\n\tkeys['x'] = pygame.K_x\n\tkeys['y'] = pygame.K_y\n\tkeys['z'] = pygame.K_z\n\tkeys['delete'] = pygame.K_DELETE\n\tkeys['kp0'] = pygame.K_KP0\n\tkeys['kp1'] = pygame.K_KP1\n\tkeys['kp2'] = pygame.K_KP2\n\tkeys['kp3'] = pygame.K_KP3\n\tkeys['kp4'] = pygame.K_KP4\n\tkeys['kp5'] = pygame.K_KP5\n\tkeys['kp6'] = pygame.K_KP6\n\tkeys['kp7'] = pygame.K_KP7\n\tkeys['kp8'] = pygame.K_KP8\n\tkeys['kp9'] = pygame.K_KP9\n\tkeys['kp_period'] = pygame.K_KP_PERIOD\n\tkeys['kp_divide'] = pygame.K_KP_DIVIDE\n\tkeys['kp_multiply'] = pygame.K_KP_MULTIPLY\n\tkeys['kp_minus'] = pygame.K_KP_MINUS\n\tkeys['kp_plus'] = pygame.K_KP_PLUS\n\tkeys['kp_enter'] = pygame.K_KP_ENTER\n\tkeys['kp_equals'] = pygame.K_KP_EQUALS\n\tkeys['up'] = pygame.K_UP\n\tkeys['down'] = pygame.K_DOWN\n\tkeys['right'] = pygame.K_RIGHT\n\tkeys['left'] = pygame.K_LEFT\n\tkeys['insert'] = pygame.K_INSERT\n\tkeys['home'] = pygame.K_HOME\n\tkeys['end'] = pygame.K_END\n\tkeys['pageup'] = pygame.K_PAGEUP\n\tkeys['pagedown'] = pygame.K_PAGEDOWN\n\tkeys['f1'] = pygame.K_F1\n\tkeys['f2'] = pygame.K_F2\n\tkeys['f3'] = pygame.K_F3\n\tkeys['f4'] = pygame.K_F4\n\tkeys['f5'] = pygame.K_F5\n\tkeys['f6'] = pygame.K_F6\n\tkeys['f7'] = pygame.K_F7\n\tkeys['f8'] = pygame.K_F8\n\tkeys['f9'] = pygame.K_F9\n\tkeys['f10'] = pygame.K_F10\n\tkeys['f11'] = pygame.K_F11\n\tkeys['f12'] = pygame.K_F12\n\tkeys['f13'] = pygame.K_F13\n\tkeys['f14'] = pygame.K_F14\n\tkeys['f15'] = pygame.K_F15\n\tkeys['numlock'] = pygame.K_NUMLOCK\n\tkeys['capslock'] = pygame.K_CAPSLOCK\n\tkeys['scrollock'] = pygame.K_SCROLLOCK\n\tkeys['rshift'] = pygame.K_RSHIFT\n\tkeys['lshift'] = pygame.K_LSHIFT\n\tkeys['rctrl'] = pygame.K_RCTRL\n\tkeys['lctrl'] = pygame.K_LCTRL\n\tkeys['ralt'] = pygame.K_RALT\n\tkeys['lalt'] = pygame.K_LALT\n\tkeys['rmeta'] = pygame.K_RMETA\n\tkeys['lmeta'] = pygame.K_LMETA\n\tkeys['lsuper'] = pygame.K_LSUPER\n\tkeys['rsuper'] = pygame.K_RSUPER\n\tkeys['mode'] = pygame.K_MODE\n\tkeys['help'] = pygame.K_HELP\n\tkeys['print'] = pygame.K_PRINT\n\tkeys['sysreq'] = pygame.K_SYSREQ\n\tkeys['break'] = pygame.K_BREAK\n\tkeys['menu'] = pygame.K_MENU\n\tkeys['power'] = pygame.K_POWER\n\tkeys['euro'] = pygame.K_EURO\n\n\treturn keys\n\n\ndef createMB():\n\t\"\"\"\n\tReturns a dict of the mouse buttons\n\t\"\"\"\n\n\tbuttons = {}\n\n\tbuttons['left'] = 0\n\tbuttons['right'] = 1\n\tbuttons['middle'] = 2\n\n\treturn buttons\n"
}
{
    "repo_name": "jmorse/cyanide",
    "ref": "refs/heads/master",
    "path": "lint-imports/importvisitor.py",
    "copies": "1",
    "content": "\nimport ast\n\nclass ImportVisitor(ast.NodeVisitor):\n\n\tdef __init__(self, names):\n\t\tself._names = names\n\t\tself._info = {}\n\n\tdef visit_Import(self, node):\n\t\tself._visitImport(node, [a.name for a in node.names])\n\n\tdef visit_ImportFrom(self, node):\n\t\tparts = [node.module+'.'+a.name for a in node.names]\n\t\tparts.append(node.module)\n\t\tself._visitImport(node, parts)\n\n\tdef _visitImport(self, node, imports):\n\t\tfor name in imports:\n\t\t\tif name in self._names:\n\t\t\t\tself._info[name] = node.lineno\n\n\t\treturn\n\t\t# allow parser to continue to parse the statement's children\n\t\tsuper(ImportVisitor, self).generic_visit(node)\n\n\tdef getImportsInfo(self):\n\t\treturn self._info\n\ndef _test():\n\tcode = \"\"\"\nimport stuff\nfrom bananas import taste\nfrom pounds import pence\ndef foo():\n\timport bacon as face, nose\na = b + 5\n\"\"\"\n\tprint code\n\tprint '----'\n\n\ttree = ast.parse(code)\n#\tprint dir(tree)\n#\tprint tree.__class__\n\tiv = ImportVisitor(['bacon', 'bananas', 'pounds.pence'])\n\tiv.visit(tree)\n\n\tprint iv.getImportsInfo()\n\nif __name__ == '__main__':\n\t_test()\n"
}
{
    "repo_name": "VU-Cog-Sci/PRF_experiment",
    "ref": "refs/heads/master",
    "path": "MapperSession.py",
    "copies": "1",
    "content": "from __future__ import division\nfrom psychopy import visual, core, misc, event\nimport numpy as np\nfrom IPython import embed as shell\nfrom math import *\n\nimport os, sys, time, pickle\nimport pygame\nfrom pygame.locals import *\n# from pygame import mixer, time\n\nimport Quest\n\nsys.path.append( 'exp_tools' )\n# sys.path.append( os.environ['EXPERIMENT_HOME'] )\n\nfrom Session import *\nfrom MapperTrial import *\nfrom standard_parameters import *\nfrom Staircase import YesNoStaircase\n\nimport appnope\nappnope.nope()\n\nclass MapperSession(EyelinkSession):\n\tdef __init__(self, subject_initials, index_number, scanner, tracker_on):\n\t\tsuper(MapperSession, self).__init__( subject_initials, index_number)\n\t\t\n\t\tself.create_screen( size = screen_res, full_screen = 0, physical_screen_distance = 159.0, background_color = background_color, physical_screen_size = (70, 40) )\n\n\t\tself.standard_parameters = standard_parameters\n\t\tself.response_button_signs = response_button_signs\n\n\t\ttext_file_name = \"data/%s_color_ratios.txt\"%self.subject_initials\n\t\tassert os.path.isfile(text_file_name), 'NO COLOR RATIO TEXT FILE PRESENT!!!!!!!!'\n\t\ttext_file = open(text_file_name, \"r\")\n\t\tRG_BY_ratio = float(text_file.readline().split('ratio: ')[-1][:-1])\n\t\ttext_file.close()\n\t\tif RG_BY_ratio \u003e 1:\n\t\t\tself.standard_parameters['RG_color'] = 1\n\t\t\tself.standard_parameters['BY_color'] = 1/RG_BY_ratio\n\t\telse:\n\t\t\tself.standard_parameters['BY_color'] = 1\n\t\t\tself.standard_parameters['RG_color'] = 1/RG_BY_ratio\n\n\n\t\t# text_file_name = \"data/%s_speed_ratios.txt\"%self.subject_initials\n\t\t# assert os.path.isfile(text_file_name), 'NO SPEED RATIO TEXT FILE PRESENT!!!!!!!!'\n\t\t# text_file = open(text_file_name, \"r\")\n\t\t# self.fast_ratio = float(text_file.readline().split('ratio: ')[-1][:-1])\n\t\t# self.slow_ratio = 1-self.fast_ratio\n\t\tself.fast_ratio = self.slow_ratio = 0.5\n\n\t\tself.create_output_file_name()\n\t\tif tracker_on:\n\t\t\tself.create_tracker(auto_trigger_calibration = 1, calibration_type = 'HV9')\n\t\t\tif self.tracker_on:\n\t\t\t\tself.tracker_setup()\n\t\telse:\n\t\t\tself.create_tracker(tracker_on = False)\n\t\t\n\t\tself.scanner = scanner\n\t\t# trials can be set up independently of the staircases that support their parameters\n\t\tself.prepare_trials()\n\t\tself.prepare_staircases()\n\n\t\tself.ready_for_next_pulse = True\n\t\tself.exp_start_time = 0.0\n\n\t\t# setup fix transient and redraws in session to let it continuously run. This happens in multitudes of 'time_steps', which is equal to the redraw steps in the PRF experiment.\n\t\tself.time_steps = self.standard_parameters['TR']/self.standard_parameters['redraws_per_TR']\n\t\tself.transient_occurrences = np.round(np.cumsum(np.random.exponential(self.standard_parameters['task_rate'], size = 20000) + self.standard_parameters['minimum_pulse_gap']) * (1/self.time_steps)) / (1/self.time_steps)\n\n\n\n\tdef prepare_staircases(self):\n\t\t# staircases\n\t\tself.initial_value = 2 # for self.unique_tasks, \n\t\tself.staircase_file_name = os.path.join(os.path.split(self.output_file)[0], self.subject_initials + '_mapper_quest.pickle')\n\t\tif os.path.exists( self.staircase_file_name ):\n\t\t\twith open(self.staircase_file_name) as f:\n\t\t\t\tself.staircases = pickle.load(f)\n\t\telse:\n\t\t\t# create staircases\n\t\t\tself.staircases={}\n\t\t\tself.staircases.update({'fix':\n\t\t\t\t\t\tQuest.QuestObject(\n\t\t\t\t\t\t\t\ttGuess = self.initial_value, \n\t\t\t\t\t\t\t\ttGuessSd = self.initial_value * 0.35, \n\t\t\t\t\t\t\t\tpThreshold = 0.83, \n\t\t\t\t\t\t\t\tbeta = 3.5, \n\t\t\t\t\t\t\t\tdelta = 0.01, \n\t\t\t\t\t\t\t\tgamma = 0.0, \n\t\t\t\t\t\t\t\tgrain = 0.01, \n\t\t\t\t\t\t\t\trange = None \n\t\t\t\t\t\t\t\t) \n\t\t\t\t\t\t\t})\n\n\t\n\tdef prepare_trials(self):\n\t\t\"\"\"docstring for prepare_trials(self):\"\"\"\n\t\t\n\t\t# create random m-sequence for the 5 trial types of length (5^3)-1 = 124. I then add the first trial type to the end of the array, so that all trial types have even occurences\n\t\tfrom psychopy.contrib import mseq\n\t\tself.tasks = np.array(['fix_no_stim','no_color_no_speed','yes_color_no_speed','no_color_yes_speed','yes_color_yes_speed'])\n\t\tself.trial_array = np.hstack([[0],mseq.mseq(5,3,1,np.random.randint(200))]) # base (number of trial types), power (sequence length is base^power-1), shift (to shift last values of sequence to first), random sequence out of the 200 possibilities\n\t\t\t\n\t\tself.phase_durations = np.array([\n\t\t\t-0.001, # instruct time\n\t\t\t-0.001, # wait for t at beginnning of every trial\n\t\t\tself.standard_parameters['TR'] * self.standard_parameters['mapper_stim_in_TR'],   #stimulation time\n\t\t\tself.standard_parameters['TR'] * self.standard_parameters['mapper_ITI_in_TR'] ]) # ITI time\n\n\t\t# stimuli\n\t\tself.fixation_rim = visual.PatchStim(self.screen, mask='raisedCos',tex=None, size=12.5, pos = np.array((0.0,0.0)), color = (0,0,0), maskParams = {'fringeWidth':0.4})\n\t\tself.fixation_outer_rim = visual.PatchStim(self.screen, mask='raisedCos',tex=None, size=17.5, pos = np.array((0.0,0.0)), color = (-1.0,-1.0,-1.0), maskParams = {'fringeWidth':0.4})\n\t\tself.fixation = visual.PatchStim(self.screen, mask='raisedCos',tex=None, size=9.0, pos = np.array((0.0,0.0)), color = (0,0,0), opacity = 1.0, maskParams = {'fringeWidth':0.4})\n\n\t\tscreen_width, screen_height = self.screen_pix_size\n\t\t\n\t\tecc_mask = filters.makeMask(matrixSize = 2048, shape='raisedCosine', radius=self.standard_parameters['stim_size'] * self.screen_pix_size[1] / self.screen_pix_size[0], center=(0.0, 0.0), range=[1, -1], fringeWidth=0.1 )\n\t\tself.mask_stim = visual.PatchStim(self.screen, mask=ecc_mask,tex=None, size=(self.screen_pix_size[0], self.screen_pix_size[0]), pos = np.array((0.0,0.0)), color = self.screen.background_color) # \n\t\n\t\t# this will be roughly 4 * 124 = 496, which is 8:15 minutes\n\t\tself.exp_duration = np.sum(self.phase_durations) * len(self.trial_array)\n\n\tdef close(self):\n\t\tsuper(MapperSession, self).close()\n\t\twith open(self.staircase_file_name, 'w') as f:\n\t\t\tpickle.dump(self.staircases, f)\n\t\tprint 'Fix staircase mean {}, standard deviation {}'.format(self.staircases['fix'].mean(), self.staircases['fix'].sd())\n\t\t\n\t\n\tdef run(self):\n\t\t\"\"\"docstring for fname\"\"\"\n\t\t# cycle through trials\n\t\tfor i in range(len(self.trial_array)):\n\t\t\t# prepare the parameters of the following trial based on the shuffled trial array\n\t\t\tthis_trial_parameters = self.standard_parameters.copy()\n\t\t\tthis_trial_parameters['task'] = self.trial_array[i]\n\n\t\t\tthese_phase_durations = self.phase_durations.copy()\n\t\t\tthis_trial = MapperTrial(this_trial_parameters, phase_durations = these_phase_durations, session = self, screen = self.screen, tracker = self.tracker)\n\t\t\t\n\t\t\t# run the prepared trial\n\t\t\tthis_trial.run(ID = i)\n\t\t\tif self.stopped == True:\n\t\t\t\tbreak\n\t\tself.close()\n\t\n\n"
}
{
    "repo_name": "jmoore987/sharp_aquos_rc",
    "ref": "refs/heads/master",
    "path": "sharp_aquos_rc/__init__.py",
    "copies": "1",
    "content": "\"\"\"Module to control a Sharp Aquos Remote Control enabled TV via TCP/IP\"\"\"\n\nfrom .tv import TV\n"
}
{
    "repo_name": "kevinr/750book-web",
    "ref": "refs/heads/master",
    "path": "750book-web-env/lib/python2.7/site-packages/djcelery/admin.py",
    "copies": "1",
    "content": "from __future__ import absolute_import\nfrom __future__ import with_statement\n\nfrom django import forms\nfrom django.conf import settings\nfrom django.contrib import admin\nfrom django.contrib.admin import helpers\nfrom django.contrib.admin.views import main as main_views\nfrom django.shortcuts import render_to_response\nfrom django.template import RequestContext\nfrom django.utils.encoding import force_unicode\nfrom django.utils.html import escape\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom celery import current_app\nfrom celery import states\nfrom celery import registry\nfrom celery.task.control import broadcast, revoke, rate_limit\nfrom celery.utils import abbrtask\n\nfrom . import loaders\nfrom .admin_utils import action, display_field, fixedwidth\nfrom .models import (TaskState, WorkerState,\n                     PeriodicTask, IntervalSchedule, CrontabSchedule)\nfrom .humanize import naturaldate\n\n\nTASK_STATE_COLORS = {states.SUCCESS: \"green\",\n                     states.FAILURE: \"red\",\n                     states.REVOKED: \"magenta\",\n                     states.STARTED: \"yellow\",\n                     states.RETRY: \"orange\",\n                     \"RECEIVED\": \"blue\"}\nNODE_STATE_COLORS = {\"ONLINE\": \"green\",\n                     \"OFFLINE\": \"gray\"}\n\n\nclass MonitorList(main_views.ChangeList):\n\n    def __init__(self, *args, **kwargs):\n        super(MonitorList, self).__init__(*args, **kwargs)\n        self.title = self.model_admin.list_page_title\n\n\n@display_field(_(\"state\"), \"state\")\ndef colored_state(task):\n    state = escape(task.state)\n    color = TASK_STATE_COLORS.get(task.state, \"black\")\n    return \"\"\"\u003cb\u003e\u003cspan style=\"color: %s;\"\u003e%s\u003c/span\u003e\u003c/b\u003e\"\"\" % (color, state)\n\n\n@display_field(_(\"state\"), \"last_heartbeat\")\ndef node_state(node):\n    state = node.is_alive() and \"ONLINE\" or \"OFFLINE\"\n    color = NODE_STATE_COLORS[state]\n    return \"\"\"\u003cb\u003e\u003cspan style=\"color: %s;\"\u003e%s\u003c/span\u003e\u003c/b\u003e\"\"\" % (color, state)\n\n\n@display_field(_(\"ETA\"), \"eta\")\ndef eta(task):\n    if not task.eta:\n        return \"\"\"\u003cspan style=\"color: gray;\"\u003enone\u003c/span\u003e\"\"\"\n    return escape(task.eta)\n\n\n@display_field(_(\"when\"), \"tstamp\")\ndef tstamp(task):\n    return \"\"\"\u003cdiv title=\"%s\"\u003e%s\u003c/div\u003e\"\"\" % (escape(str(task.tstamp)),\n                                             escape(naturaldate(task.tstamp)))\n\n\n@display_field(_(\"name\"), \"name\")\ndef name(task):\n    short_name = abbrtask(task.name, 16)\n    return \"\"\"\u003cdiv title=\"%s\"\u003e\u003cb\u003e%s\u003c/b\u003e\u003c/div\u003e\"\"\" % (escape(task.name),\n                                                    escape(short_name))\n\n\nclass ModelMonitor(admin.ModelAdmin):\n    can_add = False\n    can_delete = False\n\n    def get_changelist(self, request, **kwargs):\n        return MonitorList\n\n    def change_view(self, request, object_id, extra_context=None):\n        extra_context = extra_context or {}\n        extra_context.setdefault(\"title\", self.detail_title)\n        return super(ModelMonitor, self).change_view(request, object_id,\n                                                     extra_context)\n\n    def has_delete_permission(self, request, obj=None):\n        if not self.can_delete:\n            return False\n        return super(ModelMonitor, self).has_delete_permission(request, obj)\n\n    def has_add_permission(self, request):\n        if not self.can_add:\n            return False\n        return super(ModelMonitor, self).has_add_permission(request)\n\n\nclass TaskMonitor(ModelMonitor):\n    detail_title = _(\"Task detail\")\n    list_page_title = _(\"Tasks\")\n    rate_limit_confirmation_template = \"djcelery/confirm_rate_limit.html\"\n    date_hierarchy = \"tstamp\"\n    fieldsets = (\n            (None, {\n                \"fields\": (\"state\", \"task_id\", \"name\", \"args\", \"kwargs\",\n                           \"eta\", \"runtime\", \"worker\", \"tstamp\"),\n                \"classes\": (\"extrapretty\", ),\n            }),\n            (\"Details\", {\n                \"classes\": (\"collapse\", \"extrapretty\"),\n                \"fields\": (\"result\", \"traceback\", \"expires\"),\n            }),\n    )\n    list_display = (fixedwidth(\"task_id\", name=_(\"UUID\"), pt=8),\n                    colored_state,\n                    name,\n                    fixedwidth(\"args\", pretty=True),\n                    fixedwidth(\"kwargs\", pretty=True),\n                    eta,\n                    tstamp,\n                    \"worker\")\n    readonly_fields = (\"state\", \"task_id\", \"name\", \"args\", \"kwargs\",\n                       \"eta\", \"runtime\", \"worker\", \"result\", \"traceback\",\n                       \"expires\", \"tstamp\")\n    list_filter = (\"state\", \"name\", \"tstamp\", \"eta\", \"worker\")\n    search_fields = (\"name\", \"task_id\", \"args\", \"kwargs\", \"worker__hostname\")\n    actions = [\"revoke_tasks\",\n               \"terminate_tasks\",\n               \"kill_tasks\",\n               \"rate_limit_tasks\"]\n\n    @action(_(\"Revoke selected tasks\"))\n    def revoke_tasks(self, request, queryset):\n        with current_app.default_connection() as connection:\n            for state in queryset:\n                revoke(state.task_id, connection=connection)\n\n    @action(_(\"Terminate selected tasks\"))\n    def terminate_tasks(self, request, queryset):\n        with current_app.default_connection() as connection:\n            for state in queryset:\n                revoke(state.task_id, connection=connection, terminate=True)\n\n    @action(_(\"Kill selected tasks\"))\n    def kill_tasks(self, request, queryset):\n        with current_app.default_connection() as connection:\n            for state in queryset:\n                revoke(state.task_id, connection=connection,\n                       terminate=True, signal=\"KILL\")\n\n    @action(_(\"Rate limit selected tasks\"))\n    def rate_limit_tasks(self, request, queryset):\n        tasks = set([task.name for task in queryset])\n        opts = self.model._meta\n        app_label = opts.app_label\n        if request.POST.get(\"post\"):\n            rate = request.POST[\"rate_limit\"]\n            with current_app.default_connection() as connection:\n                for task_name in tasks:\n                    rate_limit(task_name, rate, connection=connection)\n            return None\n\n        context = {\n            \"title\": _(\"Rate limit selection\"),\n            \"queryset\": queryset,\n            \"object_name\": force_unicode(opts.verbose_name),\n            \"action_checkbox_name\": helpers.ACTION_CHECKBOX_NAME,\n            \"opts\": opts,\n            \"app_label\": app_label,\n        }\n\n        return render_to_response(self.rate_limit_confirmation_template,\n                context, context_instance=RequestContext(request))\n\n    def get_actions(self, request):\n        actions = super(TaskMonitor, self).get_actions(request)\n        actions.pop(\"delete_selected\", None)\n        return actions\n\n    def queryset(self, request):\n        qs = super(TaskMonitor, self).queryset(request)\n        return qs.select_related('worker')\n\n\nclass WorkerMonitor(ModelMonitor):\n    can_add = True\n    detail_title = _(\"Node detail\")\n    list_page_title = _(\"Worker Nodes\")\n    list_display = (\"hostname\", node_state)\n    readonly_fields = (\"last_heartbeat\", )\n    actions = [\"shutdown_nodes\",\n               \"enable_events\",\n               \"disable_events\"]\n\n    @action(_(\"Shutdown selected worker nodes\"))\n    def shutdown_nodes(self, request, queryset):\n        broadcast(\"shutdown\", destination=[n.hostname for n in queryset])\n\n    @action(_(\"Enable event mode for selected nodes.\"))\n    def enable_events(self, request, queryset):\n        broadcast(\"enable_events\",\n                  destination=[n.hostname for n in queryset])\n\n    @action(_(\"Disable event mode for selected nodes.\"))\n    def disable_events(self, request, queryset):\n        broadcast(\"disable_events\",\n                  destination=[n.hostname for n in queryset])\n\n    def get_actions(self, request):\n        actions = super(WorkerMonitor, self).get_actions(request)\n        actions.pop(\"delete_selected\", None)\n        return actions\n\nadmin.site.register(TaskState, TaskMonitor)\nadmin.site.register(WorkerState, WorkerMonitor)\n\n\n# ### Periodic Tasks\n\n\nclass LaxChoiceField(forms.ChoiceField):\n\n    def valid_value(self, value):\n        return True\n\n\ndef periodic_task_form():\n    loaders.autodiscover()\n    tasks = list(sorted(registry.tasks.regular().keys()))\n    choices = ((\"\", \"\"), ) + tuple(zip(tasks, tasks))\n\n    class PeriodicTaskForm(forms.ModelForm):\n        regtask = LaxChoiceField(label=_(u\"Task (registered)\"),\n                                 choices=choices, required=False)\n        task = forms.CharField(label=_(\"Task (custom)\"), required=False,\n                               max_length=200)\n\n        class Meta:\n            model = PeriodicTask\n\n        def clean(self):\n            data = super(PeriodicTaskForm, self).clean()\n            regtask = data.get(\"regtask\")\n            if regtask:\n                data[\"task\"] = regtask\n            if not data[\"task\"]:\n                exc = forms.ValidationError(_(u\"Need name of task\"))\n                self._errors[\"task\"] = self.error_class(exc.messages)\n                raise exc\n            return data\n\n    return PeriodicTaskForm\n\n\nclass PeriodicTaskAdmin(admin.ModelAdmin):\n    model = PeriodicTask\n    form = periodic_task_form()\n    fieldsets = (\n            (None, {\n                \"fields\": (\"name\", \"regtask\", \"task\", \"enabled\"),\n                \"classes\": (\"extrapretty\", \"wide\"),\n            }),\n            (\"Schedule\", {\n                \"fields\": (\"interval\", \"crontab\"),\n                \"classes\": (\"extrapretty\", \"wide\", ),\n            }),\n            (\"Arguments\", {\n                \"fields\": (\"args\", \"kwargs\"),\n                \"classes\": (\"extrapretty\", \"wide\", \"collapse\"),\n            }),\n            (\"Execution Options\", {\n                \"fields\": (\"expires\", \"queue\", \"exchange\", \"routing_key\"),\n                \"classes\": (\"extrapretty\", \"wide\", \"collapse\"),\n            }),\n    )\n\n    def __init__(self, *args, **kwargs):\n        super(PeriodicTaskAdmin, self).__init__(*args, **kwargs)\n        self.form = periodic_task_form()\n\n    def changelist_view(self, request, extra_context=None):\n        extra_context = extra_context or {}\n        scheduler = getattr(settings, \"CELERYBEAT_SCHEDULER\", None)\n        if scheduler != 'djcelery.schedulers.DatabaseScheduler':\n            extra_context['wrong_scheduler'] = True\n        return super(PeriodicTaskAdmin, self).changelist_view(request,\n                                                              extra_context)\n\n    def queryset(self, request):\n        qs = super(PeriodicTaskAdmin, self).queryset(request)\n        return qs.select_related('interval', 'crontab')\n\n\nadmin.site.register(IntervalSchedule)\nadmin.site.register(CrontabSchedule)\nadmin.site.register(PeriodicTask, PeriodicTaskAdmin)\n"
}
{
    "repo_name": "jhsu802701/packer-debian-jessie",
    "ref": "refs/heads/master",
    "path": "json.py",
    "copies": "1",
    "content": "#! /usr/bin/python\n\nimport datetime, os, sys, shutil\nimport urllib2, re\n\nabbrev = sys.argv[1]\ndeb_branch = sys.argv[2]\n\n# From \n# http://stackoverflow.com/questions/17140886/how-to-search-and-replace-text-in-a-file-using-python\ndef replace_string_in_file(file_local, string1, string2):\n  fname1 = file_local\n  fname2 = \"%s.temp\" %file_local\n  f1 = open(fname1, 'r')\n  f2 = open(fname2, 'w')\n  for line in f1:\n    f2.write(line.replace(string1, string2))\n  f1.close()\n  f2.close()\n  os.remove(fname1)\n  os.rename(fname2, fname1)\n\ndef string_from_file(file_local):\n  f=open(file_local, 'r')\n  output_str = f.read()\n  return output_str\n\n# Copy original template\nshutil.copy('templates_json/debian.txt', 'debian.json')\n\n# Update name of virtual machine\nt1 = datetime.datetime.now()\nt2 = t1.strftime(\"%Y-%m%d-%H%M-%S\")\nreplace_string_in_file('debian.json', 'packer-debian', \"packer-debian-%s-%s\" % (deb_branch, t2))\n\n# Update scripts in debian.json for rbenv and rbenv-rubymn\nif abbrev \u003c\u003e 'min':\n  # Update provisioning scripts executed as root\n  str_root_txt = string_from_file('templates_json/root.txt')\n  replace_string_in_file('debian.json', '\"scripts/root/vagrant.sh\"', str_root_txt)\n\n  # Update provisioning scripts executed as user\n  file_template_scripts = \"templates_json/user-%s.txt\" %(abbrev)\n  str_scripts = string_from_file(file_template_scripts)\n  replace_string_in_file('debian.json', '\"scripts/user/nothing.sh\"', str_scripts)\n  \n  # Update file name of Vagrant box\n  replace_string_in_file('debian.json', 'vagrant-debian-jessie.box', \"vagrant-debian-%s-%s.box\" % (deb_branch, abbrev))\n"
}
{
    "repo_name": "hagarwa3/stackoverflow-search",
    "ref": "refs/heads/master",
    "path": "parsing/xml-to-json.py",
    "copies": "1",
    "content": "import json, xmljson\nfrom lxml.etree import fromstring\nf = open(\"C:\\Users\\Harshit Agarwal\\Desktop\\stackoverflow.com-Posts\\Postsnewjson4.txt\", \"w\")\nwith open(\"C:\\Users\\Harshit Agarwal\\Desktop\\stackoverflow.com-Posts\\Postsnew.xml\") as fileobject:\n    i = 0\n    for i in xrange(5):\n        fileobject.next()\n    for line in fileobject:\n        if len(line)\u003e40:\n            xml = fromstring(line)\n            a = json.dumps(xmljson.badgerfish.data(xml))\n            a= a[8:-1] + \"\\n\"\n            f.write(a)\n\nf.close()"
}
{
    "repo_name": "christianmemije/kolibri",
    "ref": "refs/heads/master",
    "path": "kolibri/utils/version.py",
    "copies": "1",
    "content": "\"\"\"\nWe follow semantic versioning 2.0.0 according to\n`semver.org \u003chttp://semver.org/\u003e`__ but for Python distributions and in the\ninternal string representation in Python, you will find a\n`PEP-440 \u003chttps://www.python.org/dev/peps/pep-0440/\u003e`__ flavor.\n\n * ``1.1.0`` (Semver)  = ``1.1.0`` (PEP-440).\n * ``1.0.0-alpha1`` (Semver)  = ``1.0.0a1`` (PEP-440).\n\nHere's how version numbers are generated:\n\n * ``kolibri.__version__`` is automatically set, runtime environments use it\n   to decide the version of Kolibri as a string. This is especially something\n   that PyPi and setuptools use.\n\n * ``kolibri.VERSION`` is a tuple containing version information, it's set in\n   ``kolibri/__init__.py`` is automatically suffixed in pre-releases by a\n   number of rules defined below. For a final release (not a pre-release),\n   it will be used exactly as it appears.\n\n * ``kolibri/VERSION`` is a file containing the exact version of Kolibri for a\n   distributed environment (pre-releases only!)\n\n * ``git describe --tags`` is a command run to fetch tag information from a git\n   checkout with the Kolibri code. The information is used to validate the\n   major components of ``kolibri.VERSION`` and to suffix the final version of\n   prereleases. This information is stored permanently in ``kolibri/VERSION``\n   before shipping a pre-release by calling ``make writeversion`` during\n   ``make dist`` etc.\n\n\nConfused? Here's a table:\n\n\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| Release type | ``kolibri.VERSION`` | ``kolibri/VERSION`` | Git data                  | Examples                            |\n+==============+=====================+=====================+===========================+=====================================+\n| Final        | Canonical, only     | N/A                 | N/A                       | 0.1.0, 0.2.2,                       |\n|              | information used    |                     |                           | 0.2.post1                           |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| dev release  | (1, 2, 3, 'alpha',  | Fallback            | timestamp of latest       | 0.4.0.dev020170605181124-f1234567   |\n| (alpha0)     | 0), 0th alpha = a   |                     | commit + hash             |                                     |\n|              | dev release! Never  |                     |                           |                                     |\n|              | used as a canonical |                     |                           |                                     |\n|              |                     |                     |                           |                                     |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| alpha1+      | (1, 2, 3, 'alpha',  | Fallback            | ``git describe --tags``   | Clean head:                         |\n|              | 1)                  |                     |                           | 1.2.3a1,                            |\n|              |                     |                     |                           | Changes                             |\n|              |                     |                     |                           | since tag:                          |\n|              |                     |                     |                           | 1.2.3a1.dev123-f1234567             |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| beta1+       | (1, 2, 3, 'alpha',  | Fallback            | ``git describe --tags``   | Clean head:                         |\n|              | 1)                  |                     |                           | 1.2.3b1,                            |\n|              |                     |                     |                           | Changes                             |\n|              |                     |                     |                           | since tag:                          |\n|              |                     |                     |                           | 1.2.3b1.dev123-f1234567             |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| rc1+         | (1, 2, 3, 'alpha',  | Fallback            | ``git describe --tags``   | Clean head:                         |\n| (release     | 1)                  |                     |                           | 1.2.3rc1,                           |\n| candidate)   |                     |                     |                           | Changes                             |\n|              |                     |                     |                           | since tag:                          |\n|              |                     |                     |                           | 1.2.3rc1.dev123-f1234567            |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n| beta0, rc0,  | Not recommended,    | Fallback            | timestamp of latest       | 0.4.0b0.dev020170605181124-f1234567 |\n| post0, x.y.0 | but if you use it,  |                     | commit + hash             |                                     |\n|              | your release        |                     |                           |                                     |\n|              | transforms into a   |                     |                           |                                     |\n|              | X.Y.0b0.dev{suffix} |                     |                           |                                     |\n|              | release, which in   |                     |                           |                                     |\n|              | most cases should   |                     |                           |                                     |\n|              | be assigned to the  |                     |                           |                                     |\n|              | preceding release   |                     |                           |                                     |\n|              | type.               |                     |                           |                                     |\n|              |                     |                     |                           |                                     |\n+--------------+---------------------+---------------------+---------------------------+-------------------------------------+\n\n\n**Fallback**: ``kolibri/VERSION`` is auto-generated with ``make writeversion``\nduring the build process. The file is read as a fallback when there's no git\ndata available in a pre-release (which is the case in an installed\nenvironment).\n\n\nRelease order example 1.2.3 release:\n\n * ``VERSION = (1, 2, 3, 'alpha', 0)`` throughout the development phase, this\n   results in a lot of ``1.2.3.dev0YYYYMMDDHHMMSS-1234abcd`` with no need for\n   git tags.\n * ``VERSION = (1, 2, 3, 'alpha', 1)`` for the first alpha release. When it's\n   tagged and released,\n\n.. warning::\n    Do not import anything from the rest of Kolibri in this module, it's\n    crucial that it can be loaded without the settings/configuration/django\n    stack.\n\nDo not import this file in other package's __init__, because installation with\nsetup.py should not depend on other packages. In case you were to have a\npackage foo that depended on kolibri, and kolibri is installed as a dependency\nwhile foo is installing, then foo won't be able to access kolibri before after\nsetuptools has completed installation of everything.\n\"\"\"\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport datetime\nimport logging\nimport os\nimport pkgutil\nimport re\nimport subprocess\n\nfrom .compat import parse_version\nfrom .lru_cache import lru_cache\n\nlogger = logging.getLogger(__name__)\n\nORDERED_VERSIONS = ('alpha', 'beta', 'rc', 'final')\n\n\ndef get_major_version(version=None):\n    \"\"\"\n    :returns: String w/ first digit part of version tuple x.y.z\n    \"\"\"\n    version = get_complete_version(version)\n    major = '.'.join(str(x) for x in version[:3])\n    return major\n\n\ndef get_complete_version(version=None):\n    \"\"\"\n    :returns: A tuple of the version. If version argument is non-empty, then\n              checks for correctness of the tuple provided.\n    \"\"\"\n    if version is None:\n        from kolibri import VERSION as version\n    else:\n        assert len(version) == 5\n        assert version[3] in ORDERED_VERSIONS\n\n    return version\n\n\ndef get_docs_version(version=None):\n    \"\"\"\n    :returns: Version string for use in Sphinx docs\n    \"\"\"\n    version = get_complete_version(version)\n    if version[3] != 'final':\n        return 'dev'\n    else:\n        return '%d.%d' % version[:2]\n\n\ndef get_git_changeset():\n    \"\"\"\n    Returns a numeric identifier of the latest git changeset.\n\n    The result is the UTC timestamp of the changeset in YYYYMMDDHHMMSS format.\n    This value isn't guaranteed to be unique, but collisions are very unlikely,\n    so it's sufficient for generating the development version numbers.\n\n    If there is no git data or git installed, it will return None\n    \"\"\"\n    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    try:\n        git_log = subprocess.Popen(\n            'git log --pretty=format:%ct --quiet -1 HEAD',\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n            cwd=repo_dir,\n            universal_newlines=True\n        )\n        # This does not fail if git is not available or current dir isn't a git\n        # repo - it's safe.\n        timestamp = git_log.communicate()[0]\n        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))\n        return \"+git-{}\".format(timestamp.strftime('%Y%m%d%H%M%S'))\n    except (EnvironmentError, ValueError):\n        return None\n\n\ndef get_git_describe():\n    \"\"\"\n    Detects a valid tag, 1.2.3-\u003calpha|beta|rc\u003e(-123-sha123)\n    :returns: None if no git tag available (no git, no tags, or not in a repo)\n    \"\"\"\n    valid_pattern = re.compile(r\"^v[0-9\\\\-\\\\.]+(-(alpha|beta|rc)[0-9]+)?(-\\d+-\\w+)?$\")\n    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    try:\n        p = subprocess.Popen(\n            \"git describe --tags --match 'v[0-9]*'\",\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n            cwd=repo_dir,\n            universal_newlines=True\n        )\n        # This does not fail if git is not available or current dir isn't a git\n        # repo - it's safe.\n        version_string = p.communicate()[0].rstrip()\n        return version_string if valid_pattern.match(version_string) else None\n    except EnvironmentError:\n        return None\n\ndef get_version_from_git(get_git_describe_string):\n    \"\"\"\n    Fetches the latest git tag (NB! broken behavior!)\n\n    :returns: A validated tuple, same format as kolibri.VERSION, but with extra\n        data suffixed. Example: (1, 2, 3, 'alpha', '1-123-f12345')\n\n    \"\"\"\n    git_tag_validity_check = re.compile(\n        r'v(?P\u003cversion\u003e\\d+\\.\\d+(\\.\\d+)?)'\n        r'(-(?P\u003crelease\u003ealpha|beta|rc|post)(?P\u003crelease_number\u003e\\d+))?'\n        r'(?P\u003csuffix\u003e'\n        r'(-(?P\u003cbuild\u003e\\d+))?'\n        r'(-(?P\u003chash\u003e.+))?'\n        r')'\n    )\n    m = git_tag_validity_check.match(get_git_describe_string)\n    if not m:\n        raise AssertionError(\"Unparsable git describe info: {}\".format(get_git_describe_string))\n\n    version = m.group('version')\n    version_split = version.split(\".\")\n    if len(version_split) == 2:\n        major, minor = version_split\n        patch = 0\n    else:\n        major, minor, patch = version_split\n\n    suffix = m.group('suffix')\n    suffix = \".dev+git\" + suffix if suffix else \"\"\n\n    return get_complete_version((\n        int(major),\n        int(minor),\n        int(patch),\n        m.group('release') or \"final\",\n        int(m.group('release_number') or 0)\n    )), suffix\n\n\ndef get_version_file():\n    \"\"\"\n    Looks for a file VERSION in the package data and returns the contents in\n    this. Does not check consistency.\n    \"\"\"\n    return pkgutil.get_data('kolibri', 'VERSION').decode('utf-8')\n\n\ndef get_prerelease_version(version):\n    \"\"\"\n    Called when kolibri.VERSION is set to a non-final version:\n\n    if version ==\n    \\*, \\*, \\*, \"alpha\", 0: Maps to latest commit timestamp\n    \\*, \\*, \\*, \"alpha\", \u003e0: Uses latest git tag, asserting that there is such.\n    \"\"\"\n\n    mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}\n    if version[4] == 0 and version[3] == 'alpha':\n        mapping['alpha'] = '.dev'\n\n    major = get_major_version(version)\n    major_and_release = major + mapping[version[3]] + str(version[4])\n\n    # Calculate suffix...\n    tag_describe = get_git_describe()\n\n    # If the detected git describe data is not valid, then either respect\n    # that we are in alpha-0 mode or raise an error\n    if tag_describe:\n\n        git_version, suffix = get_version_from_git(tag_describe)\n\n        if not git_version[:3] == version[:3]:\n            # If it's the 0th alpha, load suffix info from git changeset\n            if version[4] == 0 and version[3] == 'alpha':\n                # Throw away the description from git\n                suffix = get_git_changeset()\n                # Replace 'alpha' with .dev\n                return major + \".dev\" + suffix\n\n            # If the tag was not of a final version, we will fail.\n            elif not git_version[4] == 'final' and git_version[:3] \u003e version[:3]:\n                raise AssertionError(\n                    (\n                        \"Version detected from git describe --tags, but it's \"\n                        \"inconsistent with kolibri.__version__.\"\n                        \"__version__ is: {}, tag says: {}.\"\n                    ).format(\n                        str(version),\n                        git_version,\n                    )\n                )\n\n        if git_version[3] == 'final' and version[3] != 'final':\n            raise AssertionError(\n                \"You have added a final tag without bumping kolibri.VERSION, \" +\n                \"OR you need to make a new alpha0 tag. Current tag: {}\".format(git_version)\n            )\n\n        return (\n            get_major_version(git_version) +\n            mapping[git_version[3]] +\n            str(git_version[4]) +\n            suffix\n        )\n\n    # No git data, will look for a VERSION file\n    version_file = get_version_file()\n\n    # Check that the version file is consistent\n    if version_file:\n\n        # Because \\n may have been appended\n        version_file = version_file.strip()\n\n        # If there is a '.dev', we can remove it, otherwise we check it\n        # for consistency and fail if inconsistent\n        version_file_base = parse_version(version_file).base_version\n\n        # If a final release is specified in the VERSION file, then it\n        # has to be a final release in the VERSION tuple as well.\n        # A final release specified in a VERSION file (pep 440) is\n        # something that doesn't end like a1, b1, post1, and rc1\n        pep440_is_final = re.compile(r\"^\\d+(\\.\\d)+(\\.post\\d+)?$\")\n        version_file_is_final = pep440_is_final.match(version_file)\n\n        if version_file_is_final and version_file != major_and_release:\n            raise AssertionError(\n                (\n                    \"kolibri/VERSION file specified as final release but \"\n                    \"kolibri.__version__. is not a final release.\"\n                    \"__version__ is: {}, file says: {}.\"\n                ).format(\n                    str(version),\n                    version_file,\n                )\n            )\n\n        if not major_and_release.startswith(version_file_base):\n            raise AssertionError(\n                (\n                    \"kolibri/VERSION file inconsistent with \"\n                    \"kolibri.__version__.\\n\"\n                    \"__version__ is: {}, file says: {}\\n\\n{} should start \"\n                    \"with {}\"\n                ).format(\n                    str(version),\n                    version_file,\n                    major_and_release,\n                    version_file_base\n                )\n            )\n        return version_file\n\n    # In all circumstances, return the initial findings\n    return major_and_release\n\n\n@lru_cache()\ndef get_version(version=None):\n    \"\"\"\n    Returns a PEP 440-compliant version number from VERSION.\n    Derives additional information from git repository if code is contained\n    in such.\n\n    This is important to read from PEP-404 (which this function is compliant\n    with):\n\n    Within a numeric release ( 1.0.0 , 2.7.3 ), the following suffixes are\n    permitted and MUST be ordered as shown:\n\n    .devN, aN, bN, rcN, \u003cno suffix\u003e, .postN\n    \"\"\"\n    version = get_complete_version(version)\n\n    # Prerelease versions are special, we parse git data and look for special\n    # VERSION files in package data to fetch auto-generated data.\n    if version[3] != 'final':\n        return get_prerelease_version(version)\n\n    major = get_major_version(version)\n\n    sub = ''\n    if version[4] \u003e 0:\n        sub = \".post{}\".format(version[4])\n\n    return str(major + sub)\n"
}
{
    "repo_name": "omaraboumrad/pythonbot",
    "ref": "refs/heads/master",
    "path": "plugins/counter.py",
    "copies": "1",
    "content": "import re\n\nfrom helpers import msg\n\ncounter = {}\n\n\n@msg('(?P\u003ctext\u003e.*)')\ndef handle(bot, grouper):\n    words = ['C++', 'Java', 'Python', 'Haskell']\n    target = grouper('target')\n    text = grouper('text')\n    caught = []\n    for word in words:\n        total = len(re.findall(re.escape(word), text))\n        if total:\n            counter[word] = counter.get(word, 0) + total\n            caught.append((word, counter[word]))\n\n    if caught:\n        message = '{} has been mentioned {} time(s)'\n        friendly = ', '.join(message.format(a, b) for a, b in caught)\n        bot.privmsg(target, friendly)\n"
}
{
    "repo_name": "chipx86/the-cure",
    "ref": "refs/heads/master",
    "path": "thecure/engine.py",
    "copies": "1",
    "content": "import sys\n\nimport pygame\nfrom pygame.locals import *\n\nfrom thecure import set_engine\nfrom thecure.levels import get_levels\nfrom thecure.resources import get_font_filename\nfrom thecure.signals import Signal\nfrom thecure.sprites import Player\nfrom thecure.timer import Timer\nfrom thecure.ui import GameUI\n\n\nclass Camera(object):\n    SCREEN_PAD = 64\n\n    def __init__(self, engine):\n        self.engine = engine\n        self.rect = self.engine.screen.get_rect()\n        self.old_player_rect = None\n\n    def update(self):\n        if self.engine.paused:\n            return\n\n        player_rect = self.engine.player.rect\n\n        if player_rect == self.old_player_rect:\n            return\n\n        if player_rect.centerx \u003e self.rect.centerx + self.SCREEN_PAD:\n            self.rect.centerx = player_rect.centerx - self.SCREEN_PAD\n        elif player_rect.centerx \u003c self.rect.centerx - self.SCREEN_PAD:\n            self.rect.centerx = player_rect.centerx + self.SCREEN_PAD\n\n        if player_rect.centery \u003e self.rect.centery + self.SCREEN_PAD:\n            self.rect.centery = player_rect.centery - self.SCREEN_PAD\n        elif player_rect.centery \u003c self.rect.centery - self.SCREEN_PAD:\n            self.rect.centery = player_rect.centery + self.SCREEN_PAD\n\n        self.rect.clamp_ip(\n            pygame.Rect(0, 0, *self.engine.active_level.size))\n\n        self.old_player_rect = player_rect.copy()\n\n\nclass TheCureEngine(object):\n    FPS = 30\n    DEBUG_COLOR = (255, 0, 0)\n    DEBUG_POS = (30, 50)\n\n    def __init__(self, screen):\n        set_engine(self)\n\n        # Signals\n        self.tick = Signal()\n\n        # State and objects\n        self.active_level = None\n        self.paused = False\n        self.screen = screen\n        self.clock = pygame.time.Clock()\n        self.player = Player()\n        self.levels = []\n        self.level_draw_pos = (0, 0)\n        self.level_draw_area = None\n        self.camera = None\n\n        self.ui = GameUI(self)\n\n        # Debug flags\n        self.debug_rects = False\n        self.show_debug_info = False\n\n    def run(self):\n        self.ui.show_opening_scene(self._setup_game)\n\n        self._mainloop()\n\n    def quit(self):\n        pygame.quit()\n        sys.exit(0)\n\n    def set_level_draw_area(self, x, y, w, h):\n        self.level_draw_pos = (x, y)\n        self.level_draw_area = pygame.Rect(0, 0, w, h)\n\n    def _setup_game(self):\n        self.camera = Camera(self)\n        self.tick.clear()\n\n        self.player.reset()\n        self.player.layer = None\n\n        self.active_level = None\n        self.levels = [level(self) for level in get_levels()]\n        self.switch_level(0)\n\n        self.paused = True\n\n        self.show_tutorial()\n\n    def show_tutorial(self):\n        def on_done():\n            self.paused = False\n\n        textbox = self.ui.show_textbox([\n            'Explore and find the five items you need:',\n            'Crate of equipment, mushroom, salt crystal, giant web, '\n            'and a puffy red flower.',\n            'Use arrow keys to move and C to shoot.',\n            'Then find Laura!'\n        ])\n        textbox.closed.connect(on_done)\n\n    def switch_level(self, num):\n        assert num \u003c len(self.levels)\n\n        pygame.mixer.music.fadeout(2000)\n\n        if self.active_level:\n            self.active_level.stop()\n            self.active_level.main_layer.remove(self.player)\n\n        self.player.reset()\n        self.active_level = self.levels[num]\n        self.active_level.reset()\n        self.active_level.main_layer.add(self.player)\n\n        self.surface = pygame.Surface(self.screen.get_size())\n\n        self.player.move_to(*self.active_level.start_pos)\n        self.camera.update()\n\n        self.active_level.start()\n\n    def dead(self):\n        if self.player.lives == 1:\n            s = 'You have 1 more chance to get this right.'\n        else:\n            s = 'You have %d more chances to get this right.' % \\\n                self.player.lives\n\n        widget = self.ui.show_textbox(s)\n        self.paused = True\n        self.player.stop()\n        widget.closed.connect(self.restart_level)\n\n    def restart_level(self):\n        self.paused = False\n        self.player.move_to(*self.active_level.start_pos)\n        self.player.start()\n\n    def game_over(self):\n        widget = self.ui.show_textbox(\n            ['You died. Maybe it was for the best.',\n             'Game over.'])\n        self.paused = True\n        widget.closed.connect(self._setup_game)\n\n    def _mainloop(self):\n        while 1:\n            for event in pygame.event.get():\n                if not self._handle_event(event):\n                    return\n\n            if not self.paused:\n                self.tick.emit()\n\n            self._draw()\n            self.clock.tick(self.FPS)\n\n    def _handle_event(self, event):\n        if event.type == QUIT:\n            self.quit()\n            return False\n\n        if self.ui.handle_event(event):\n            return True\n        elif event.type == KEYDOWN and event.key == K_F2:\n            self.show_debug_info = not self.show_debug_info\n        elif event.type == KEYDOWN and event.key == K_F3:\n            self.debug_rects = not self.debug_rects\n        elif event.type == KEYDOWN and event.key == K_ESCAPE:\n            self.ui.confirm_quit()\n        elif self.active_level:\n            if event.type == KEYDOWN and event.key == K_RETURN:\n                if self.paused:\n                    self._unpause()\n                else:\n                    self._pause()\n            elif not self.paused and not self.player.handle_event(event):\n                player_rects = self.player.get_absolute_collision_rects()\n\n                for eventbox in self.active_level.event_handlers:\n                    for player_rect in player_rects:\n                        if (player_rect.collidelist(eventbox.rects) != -1 and\n                            eventbox.handle_event(event)):\n                            break\n\n        return True\n\n    def _pause(self):\n        self.paused = True\n        self.ui.pause()\n\n    def _unpause(self):\n        self.paused = False\n        self.ui.unpause()\n\n    def _draw(self):\n        if self.camera:\n            self.camera.update()\n\n        if self.active_level:\n            self.active_level.draw(self.surface, self.camera.rect)\n            self.screen.blit(self.surface,\n                             self.level_draw_pos,\n                             self.level_draw_area)\n\n        self.ui.draw(self.screen)\n\n        if self.show_debug_info:\n            debug_str = '%0.f FPS    X: %s    Y: %s' % (\n                self.clock.get_fps(),\n                self.player.rect.left, self.player.rect.top)\n\n            self.screen.blit(\n                self.ui.font.render(debug_str, True, self.DEBUG_COLOR),\n                self.DEBUG_POS)\n\n        pygame.display.flip()\n"
}
{
    "repo_name": "carloderamo/mushroom",
    "ref": "refs/heads/master",
    "path": "tests/approximators/test_torch_approximator.py",
    "copies": "1",
    "content": "import numpy as np\n\nfrom mushroom_rl.core import Logger\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.approximators.parametric import *\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\nclass ExampleNet(nn.Module):\n    def __init__(self, input_shape, output_shape,\n                 **kwargs):\n        super(ExampleNet, self).__init__()\n\n        self._q = nn.Linear(input_shape[0], output_shape[0])\n\n        nn.init.xavier_uniform_(self._q.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, x, a=None):\n        x = x.float()\n        q = self._q(x)\n\n        if a is None:\n            return q\n        else:\n            action = a.long()\n            q_acted = torch.squeeze(q.gather(1, action))\n\n            return q_acted\n\n\ndef test_pytorch_approximator():\n    np.random.seed(1)\n    torch.manual_seed(1)\n\n    n_actions = 2\n    s = np.random.rand(1000, 4)\n    a = np.random.randint(n_actions, size=(1000, 1))\n    q = np.random.rand(1000)\n\n    approximator = Regressor(TorchApproximator, input_shape=(4,),\n                             output_shape=(2,), n_actions=n_actions,\n                             network=ExampleNet,\n                             optimizer={'class': optim.Adam,\n                                        'params': {}}, loss=F.mse_loss,\n                             batch_size=100, quiet=True)\n\n    approximator.fit(s, a, q, n_epochs=20)\n\n    x_s = np.random.rand(2, 4)\n    x_a = np.random.randint(n_actions, size=(2, 1))\n    y = approximator.predict(x_s, x_a)\n    y_test = np.array([0.37191153, 0.5920861])\n\n    assert np.allclose(y, y_test)\n\n    y = approximator.predict(x_s)\n    y_test = np.array([[0.47908658, 0.37191153],\n                       [0.5920861, 0.27575058]])\n\n    assert np.allclose(y, y_test)\n\n    gradient = approximator.diff(x_s[0], x_a[0])\n    gradient_test = np.array([0., 0., 0., 0., 0.02627479, 0.76513696,\n                              0.6672573, 0.35979462, 0., 1.])\n    assert np.allclose(gradient, gradient_test)\n\n    gradient = approximator.diff(x_s[0])\n    gradient_test = np.array([[0.02627479, 0.], [0.76513696, 0.],\n                              [0.6672573, 0.], [0.35979462, 0.],\n                              [0., 0.02627479], [0., 0.76513696],\n                              [0., 0.6672573], [0., 0.35979462], [1, 0.],\n                              [0., 1.]])\n    assert np.allclose(gradient, gradient_test)\n\n    old_weights = approximator.get_weights()\n    approximator.set_weights(old_weights)\n    new_weights = approximator.get_weights()\n\n    assert np.array_equal(new_weights, old_weights)\n\n    random_weights = np.random.randn(*old_weights.shape).astype(np.float32)\n    approximator.set_weights(random_weights)\n    random_weight_new = approximator.get_weights()\n\n    assert np.array_equal(random_weights, random_weight_new)\n    assert not np.any(np.equal(random_weights, old_weights))\n\n\ndef test_torch_ensemble_logger(tmpdir):\n    np.random.seed(1)\n    torch.manual_seed(1)\n\n    logger = Logger('ensemble_logger', results_dir=tmpdir, use_timestamp=True)\n\n    approximator = Regressor(TorchApproximator, input_shape=(4,),\n                             output_shape=(2,), n_models=3,\n                             network=ExampleNet,\n                             optimizer={'class': optim.Adam,\n                                        'params': {}}, loss=F.mse_loss,\n                             batch_size=100, quiet=True)\n\n    approximator.set_logger(logger)\n\n    x = np.random.rand(1000, 4)\n    y = np.random.rand(1000, 2)\n\n    for i in range(50):\n        approximator.fit(x, y)\n\n    loss_file = np.load(logger.path / 'loss.npy')\n\n    assert loss_file.shape == (50, 3)\n    assert np.allclose(loss_file[0], np.array([0.29083753, 0.86829887, 1.0505845])) and \\\n           np.allclose(loss_file[-1], np.array([0.09410495, 0.18786799, 0.15016919]))\n"
}
{
    "repo_name": "ultracoldYEG/cycle-control",
    "ref": "refs/heads/master",
    "path": "CycleControl/objects/instruction.py",
    "copies": "1",
    "content": "from CycleControl.objects.hardware import *\nfrom CycleControl.helpers import *\n\n\nclass TimingType(object):\n    key = '-'\n    label = '-'\n\n\nclass ContinueTimingType(TimingType):\n    key = 'continue'\n    label = 'Continue'\n\n\nclass WaitTimingType(TimingType):\n    key = 'wait'\n    label = 'Wait'\n\n\ntiming_type_enum = [\n    ContinueTimingType,\n    WaitTimingType\n]\n\n\nclass Instruction(object):\n    TIMING_TYPES = [ContinueTimingType, WaitTimingType]\n\n    def __init__(self, hardware = HardwareSetup(), **kwargs):\n        self._name = kwargs.get('name', '')\n        self._duration = kwargs.get('duration', '0.0')\n        self._stepsize = kwargs.get('stepsize', '0.0')\n        self._timing_type = ContinueTimingType\n\n        self.digital_pins = {board.id: '0' * 24 for board in hardware.pulseblasters}\n        self.analog_functions = {board.id: ['0.0'] * 8 for board in hardware.ni_boards}\n        self.novatech_functions = {board.id: ['0'] * 12 for board in hardware.novatechs}\n\n    def __eq__(self, other):\n        attrs = ['name', 'duration', 'stepsize', 'digital_pins', 'analog_functions', 'novatech_functions']\n        return all(getattr(self, attr, None) == getattr(other, attr, None) for attr in attrs)\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def duration(self):\n        return self._duration\n\n    @property\n    def stepsize(self):\n        return self._stepsize\n\n    @property\n    def timing_type(self):\n        return self._timing_type\n\n    @property\n    def timing_type_key(self):\n        return self._timing_type.key\n\n    @name.setter\n    def name(self, name):\n        try:\n            self._name = sterilize_string(name)\n        except ValueError:\n            pass\n\n    @duration.setter\n    def duration(self, duration):\n        self._duration = duration\n\n\n    @stepsize.setter\n    def stepsize(self, stepsize):\n        self._stepsize = stepsize\n\n    @timing_type.setter\n    def timing_type(self, timing_type_key):\n        for type in self.TIMING_TYPES:\n            if type.key.lower() == timing_type_key.lower():\n                self._timing_type = type\n\n\nclass DefaultSetup(Instruction):\n    def __init__(self, hardware = HardwareSetup()):\n        Instruction.__init__(self, hardware)\n        self._name = 'Default'\n        self._duration = '-'\n        self._stepsize = '-'\n        self._timing_type = TimingType\n\n    def save_to_file(self, fp):\n        with open(fp, 'w+') as f:\n            json.dump(OrderedDict([(attr, getattr(self, attr)) for attr in [\n                'digital_pins',\n                'analog_functions',\n                'novatech_functions'\n            ]]), f, indent=2)\n\n    def load_from_file(self, fp):\n        with open(fp, 'rb') as f:\n            context = json.load(f)\n\n        self.digital_pins.update(context.get('digital_pins'))\n        self.analog_functions.update(context.get('analog_functions'))\n        self.novatech_functions.update(context.get('novatech_functions'))\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def duration(self):\n        return self._duration\n\n    @property\n    def stepsize(self):\n        return self._stepsize\n\n    @property\n    def timing_type(self):\n        return self._timing_type\n\n    @name.setter\n    def name(self, name):\n        pass\n\n    @duration.setter\n    def duration(self, duration):\n        pass\n\n    @stepsize.setter\n    def stepsize(self, stepsize):\n        pass\n\n    @timing_type.setter\n    def timing_type(self, timing_type_key):\n        pass\n\n\nclass StaticProcessVariable(object):\n    PROPERTIES = ['name', 'default']\n    def __init__(self, **kwargs):\n        self._name = kwargs.get('name', '')\n        self._default = kwargs.get('default', '0.0')\n\n    def __eq__(self, other):\n        return all(getattr(self, attr, None) == getattr(other, attr, None) for attr in self.PROPERTIES)\n\n    def __getitem__(self, idx):\n        return getattr(self, self.PROPERTIES[idx])\n\n    def __setitem__(self, idx, val):\n        setattr(self, self.PROPERTIES[idx], val)\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @name.setter\n    def name(self, val):\n        try:\n            self._name = sterilize_string(val)\n        except ValueError:\n            pass\n\n    @default.setter\n    def default(self, default):\n        try:\n            float(default)\n            self._default = default\n        except ValueError:\n            pass\n\n\nclass DynamicProcessVariable(StaticProcessVariable):\n    PROPERTIES = ['name', 'default', 'start', 'end', 'logarithmic', 'send', ]\n\n    def __init__(self, **kwargs):\n        StaticProcessVariable.__init__(self, **kwargs)\n        self._start = kwargs.get('start', '0.0')\n        self._end = kwargs.get('end', '0.0')\n        self._logarithmic = kwargs.get('logarithmic', False)\n        self._send = kwargs.get('send', False)\n\n    @property\n    def start(self):\n        return self._start\n\n    @property\n    def end(self):\n        return self._end\n\n    @property\n    def logarithmic(self):\n        return self._logarithmic\n\n    @property\n    def send(self):\n        return self._send\n\n    @start.setter\n    def start(self, start):\n        try:\n            float(start)\n            self._start = start\n        except ValueError:\n            pass\n\n    @end.setter\n    def end(self, end):\n        try:\n            float(end)\n            self._end = end\n        except ValueError:\n            pass\n\n    @logarithmic.setter\n    def logarithmic(self, state):\n        try:\n            self._logarithmic = bool(state)\n        except ValueError:\n            pass\n\n    @send.setter\n    def send(self, state):\n        try:\n            self._send = bool(state)\n        except ValueError:\n            pass\n\n    def get_stepsize(self, steps):\n        start = float(self.start)\n        end = float(self.end)\n        if steps \u003e 1:\n            steps -= 1\n        try:\n            if self.logarithmic:\n                return pow(end / start, 1.0 / float(steps))\n            return (end - start) / float(steps)\n        except Exception:\n            return 'NaN'\n"
}
{
    "repo_name": "alanchanzm/PropertyManager",
    "ref": "refs/heads/master",
    "path": "manage.py",
    "copies": "1",
    "content": "#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == \"__main__\":\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"PropertyManager.settings\")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError:\n        # The above import may fail for some other reason. Ensure that the\n        # issue is really that Django is missing to avoid masking other\n        # exceptions on Python 2.\n        try:\n            import django\n        except ImportError:\n            raise ImportError(\n                \"Couldn't import Django. Are you sure it's installed and \"\n                \"available on your PYTHONPATH environment variable? Did you \"\n                \"forget to activate a virtual environment?\"\n            )\n        raise\n    execute_from_command_line(sys.argv)\n"
}
{
    "repo_name": "josefschneider/switchboard",
    "ref": "refs/heads/master",
    "path": "examples/simple_counters/client2.py",
    "copies": "1",
    "content": "#!/usr/bin/env python\n\nfrom switchboard.client import SwitchboardOutputDevice\nfrom switchboard.app import ClientApp\n\ndef set_value(value):\n    print('Received value {}'.format(value))\n\napp = ClientApp()\napp.add_device(SwitchboardOutputDevice('output.o', set_value))\napp.run()\n"
}
{
    "repo_name": "djangophx/beer-tracker",
    "ref": "refs/heads/master",
    "path": "tracker/urls.py",
    "copies": "1",
    "content": "from django.conf.urls import url\n\nfrom . import views\n\n\nurlpatterns = [\n    url(r'^beers/$', views.BeerList.as_view(), name='beer-list'),\n    url(r'^beers/(?P\u003cpk\u003e[0-9]+)/$', views.BeerDetail.as_view(), name='beer-detail'),\n    url(r'^breweries/$', views.BreweryList.as_view(), name='brewery-list'),\n    url(r'^breweries/(?P\u003cpk\u003e[0-9]+)/$', views.BreweryDetail.as_view(), name='brewery-detail'),\n    url(r'^styles/$', views.StyleList.as_view(), name='style-list'),\n    url(r'^styles/(?P\u003cpk\u003e[0-9]+)/$', views.StyleDetail.as_view(), name='style-detail'),\n    url(r'^venues/$', views.VenueList.as_view(), name='venue-list'),\n    url(r'^venues/(?P\u003cpk\u003e[0-9]+)/$', views.VenueDetail.as_view(), name='venue-detail'),\n]\n"
}
{
    "repo_name": "balrok/Flashget",
    "ref": "refs/heads/master",
    "path": "flashget/captchas/CaptchaTrader.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\n\"\"\"\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 3 of the License,\n    or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n    See the GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program; if not, see \u003chttp://www.gnu.org/licenses/\u003e.\n\n    @author: mkaay, RaNaN\n\"\"\"\n\ntry:\n    from json import loads\nexcept ImportError:\n    from simplejson import loads\n\nfrom thread import start_new_thread\nfrom pycurl import FORM_FILE, LOW_SPEED_TIME\n\nfrom module.network.RequestFactory import getURL, getRequest\nfrom module.network.HTTPRequest import BadHeader\n\nfrom module.plugins.Hook import Hook\n\nPYLOAD_KEY = \"9f65e7f381c3af2b076ea680ae96b0b7\"\n\nclass CaptchaTraderException(Exception):\n    def __init__(self, err):\n        self.err = err\n\n    def getCode(self):\n        return self.err\n\n    def __str__(self):\n        return \"\u003cCaptchaTraderException %s\u003e\" % self.err\n\n    def __repr__(self):\n        return \"\u003cCaptchaTraderException %s\u003e\" % self.err\n\nclass CaptchaTrader(Hook):\n    __name__ = \"CaptchaTrader\"\n    __version__ = \"0.14\"\n    __description__ = \"\"\"send captchas to captchatrader.com\"\"\"\n    __config__ = [(\"activated\", \"bool\", \"Activated\", True),\n                  (\"username\", \"str\", \"Username\", \"\"),\n                  (\"force\", \"bool\", \"Force CT even if client is connected\", False),\n                  (\"passkey\", \"password\", \"Password\", \"\"),]\n    __author_name__ = (\"RaNaN\")\n    __author_mail__ = (\"RaNaN@pyload.org\")\n\n    SUBMIT_URL = \"http://api.captchatrader.com/submit\"\n    RESPOND_URL = \"http://api.captchatrader.com/respond\"\n    GETCREDITS_URL = \"http://api.captchatrader.com/get_credits/username:%(user)s/password:%(password)s/\"\n\n    def setup(self):\n        self.info = {}\n\n    def getCredits(self):\n        json = getURL(CaptchaTrader.GETCREDITS_URL % {\"user\": self.getConfig(\"username\"),\n                                                           \"password\": self.getConfig(\"passkey\")})\n        response = loads(json)\n        if response[0] \u003c 0:\n            raise CaptchaTraderException(response[1])\n        else:\n            self.logInfo(_(\"%s credits left\") % response[1])\n            self.info[\"credits\"] = response[1]\n            return response[1]\n\n    def submit(self, captcha, captchaType=\"file\", match=None):\n        if not PYLOAD_KEY:\n            raise CaptchaTraderException(\"No API Key Specified!\")\n\n        #if type(captcha) == str and captchaType == \"file\":\n        #    raise CaptchaTraderException(\"Invalid Type\")\n        assert captchaType in (\"file\", \"url-jpg\", \"url-jpeg\", \"url-png\", \"url-bmp\")\n\n        req = getRequest()\n\n        #raise timeout threshold\n        req.c.setopt(LOW_SPEED_TIME, 80)\n\n        try:\n            json = req.load(CaptchaTrader.SUBMIT_URL, post={\"api_key\": PYLOAD_KEY,\n                                                           \"username\": self.getConfig(\"username\"),\n                                                           \"password\": self.getConfig(\"passkey\"),\n                                                           \"value\": (FORM_FILE, captcha),\n                                                           \"type\": captchaType}, multipart=True)\n        finally:\n            req.close()\n\n        response = loads(json)\n        if response[0] \u003c 0:\n            raise CaptchaTraderException(response[1])\n\n        ticket = response[0]\n        result = response[1]\n        self.logDebug(\"result %s : %s\" % (ticket, result))\n\n        return ticket, result\n\n    def respond(self, ticket, success):\n        try:\n            json = getURL(CaptchaTrader.RESPOND_URL, post={\"is_correct\": 1 if success else 0,\n                                                            \"username\": self.getConfig(\"username\"),\n                                                            \"password\": self.getConfig(\"passkey\"),\n                                                            \"ticket\": ticket})\n\n            response = loads(json)\n            if response[0] \u003c 0:\n                raise CaptchaTraderException(response[1])\n\n        except BadHeader, e:\n            self.logError(_(\"Could not send response.\"), str(e))\n\n    def newCaptchaTask(self, task):\n        if not task.isTextual():\n            return False\n\n        if not self.getConfig(\"username\") or not self.getConfig(\"passkey\"):\n            return False\n\n        if self.core.isClientConnected() and not self.getConfig(\"force\"):\n            return False\n\n        if self.getCredits() \u003e 10:\n            task.handler.append(self)\n            task.setWaiting(100)\n            start_new_thread(self.processCaptcha, (task,))\n\n        else:\n            self.logInfo(_(\"Your CaptchaTrader Account has not enough credits\"))\n\n    def captchaCorrect(self, task):\n        if \"ticket\" in task.data:\n            ticket = task.data[\"ticket\"]\n            self.respond(ticket, True)\n\n    def captchaInvalid(self, task):\n        if \"ticket\" in task.data:\n            ticket = task.data[\"ticket\"]\n            self.respond(ticket, False)\n\n    def processCaptcha(self, task):\n        c = task.captchaFile\n        try:\n            ticket, result = self.submit(c)\n        except CaptchaTraderException, e:\n            task.error = e.getCode()\n            return\n\n        task.data[\"ticket\"] = ticket\n        task.setResult(result)\n"
}
{
    "repo_name": "pgbovine/python-parse-to-json",
    "ref": "refs/heads/master",
    "path": "pythonparser/lexer.py",
    "copies": "1",
    "content": "\"\"\"\nThe :mod:`lexer` module concerns itself with tokenizing Python source.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nfrom . import source, diagnostic\n#import regex as re # \u003c-- original code; requires an external regex module\nimport re # \u003c-- modified by pgbovine to use built-in standard library re\nimport unicodedata\nimport sys\n\nif sys.version_info[0] == 3:\n    unichr = chr\n\nclass Token:\n    \"\"\"\n    The :class:`Token` encapsulates a single lexer token and its location\n    in the source code.\n\n    :ivar loc: (:class:`pythonparser.source.Range`) token location\n    :ivar kind: (string) token kind\n    :ivar value: token value; None or a kind-specific class\n    \"\"\"\n    def __init__(self, loc, kind, value=None):\n        self.loc, self.kind, self.value = loc, kind, value\n\n    def __repr__(self):\n        return \"Token(%s, \\\"%s\\\", %s)\" % (repr(self.loc), self.kind, repr(self.value))\n\nclass Lexer:\n    \"\"\"\n    The :class:`Lexer` class extracts tokens and comments from\n    a :class:`pythonparser.source.Buffer`.\n\n    :class:`Lexer` is an iterable.\n\n    :ivar version: (tuple of (*major*, *minor*))\n        the version of Python, determining the grammar used\n    :ivar source_buffer: (:class:`pythonparser.source.Buffer`)\n        the source buffer\n    :ivar diagnostic_engine: (:class:`pythonparser.diagnostic.Engine`)\n        the diagnostic engine\n    :ivar offset: (integer) character offset into ``source_buffer``\n        indicating where the next token will be recognized\n    :ivar interactive: (boolean) whether a completely empty line\n        should generate a NEWLINE token, for use in REPLs\n    \"\"\"\n\n    _reserved_2_6 = frozenset([\n        \"!=\", \"%\", \"%=\", \"\u0026\", \"\u0026=\", \"(\", \")\", \"*\", \"**\", \"**=\", \"*=\", \"+\", \"+=\",\n        \",\", \"-\", \"-=\", \".\", \"/\", \"//\", \"//=\", \"/=\", \":\", \";\", \"\u003c\", \"\u003c\u003c\", \"\u003c\u003c=\",\n        \"\u003c=\", \"\u003c\u003e\", \"=\", \"==\", \"\u003e\", \"\u003e=\", \"\u003e\u003e\", \"\u003e\u003e=\", \"@\", \"[\", \"]\", \"^\", \"^=\", \"`\",\n        \"and\", \"as\", \"assert\", \"break\", \"class\", \"continue\", \"def\", \"del\", \"elif\",\n        \"else\", \"except\", \"exec\", \"finally\", \"for\", \"from\", \"global\", \"if\", \"import\",\n        \"in\", \"is\", \"lambda\", \"not\", \"or\", \"pass\", \"print\", \"raise\", \"return\", \"try\",\n        \"while\", \"with\", \"yield\", \"{\", \"|\", \"|=\", \"}\", \"~\"\n    ])\n\n    _reserved_3_0 = _reserved_2_6 \\\n        - set([\"\u003c\u003e\", \"`\", \"exec\", \"print\"]) \\\n        | set([\"-\u003e\", \"...\", \"False\", \"None\", \"nonlocal\", \"True\"])\n\n    _reserved_3_1 = _reserved_3_0 \\\n        | set([\"\u003c\u003e\"])\n\n    _reserved_3_5 = _reserved_3_1 \\\n        | set([\"@\", \"@=\"])\n\n    _reserved = {\n        (2, 6): _reserved_2_6,\n        (2, 7): _reserved_2_6,\n        (3, 0): _reserved_3_0,\n        (3, 1): _reserved_3_1,\n        (3, 2): _reserved_3_1,\n        (3, 3): _reserved_3_1,\n        (3, 4): _reserved_3_1,\n        (3, 5): _reserved_3_5,\n    }\n    \"\"\"\n    A map from a tuple (*major*, *minor*) corresponding to Python version to\n    :class:`frozenset`\\s of keywords.\n    \"\"\"\n\n    _string_prefixes_3_1 = frozenset([\"\", \"r\", \"b\", \"br\"])\n    _string_prefixes_3_3 = frozenset([\"\", \"r\", \"u\", \"b\", \"br\", \"rb\"])\n\n    # holy mother of god why\n    _string_prefixes = {\n        (2, 6): frozenset([\"\", \"r\", \"u\", \"ur\"]),\n        (2, 7): frozenset([\"\", \"r\", \"u\", \"ur\", \"b\", \"br\"]),\n        (3, 0): frozenset([\"\", \"r\", \"b\"]),\n        (3, 1): _string_prefixes_3_1,\n        (3, 2): _string_prefixes_3_1,\n        (3, 3): _string_prefixes_3_3,\n        (3, 4): _string_prefixes_3_3,\n        (3, 5): _string_prefixes_3_3,\n    }\n    \"\"\"\n    A map from a tuple (*major*, *minor*) corresponding to Python version to\n    :class:`frozenset`\\s of string prefixes.\n    \"\"\"\n\n    def __init__(self, source_buffer, version, diagnostic_engine, interactive=False):\n        self.source_buffer = source_buffer\n        self.version = version\n        self.diagnostic_engine = diagnostic_engine\n        self.interactive = interactive\n        self.print_function = False\n\n        self.offset = 0\n        self.new_line = True\n        self.indent = [(0, source.Range(source_buffer, 0, 0), \"\")]\n        self.comments = []\n        self.queue = []\n        self.parentheses = []\n        self.curly_braces = []\n        self.square_braces = []\n\n        try:\n            reserved = self._reserved[version]\n        except KeyError:\n            raise NotImplementedError(\"pythonparser.lexer.Lexer cannot lex Python %s\" % str(version))\n\n        # Sort for the regexp to obey longest-match rule.\n        re_reserved  = sorted(reserved, reverse=True, key=len)\n        re_keywords  = \"|\".join([kw for kw in re_reserved if kw.isalnum()])\n        re_operators = \"|\".join([re.escape(op) for op in re_reserved if not op.isalnum()])\n\n        # Python 3.0 uses ID_Start, \u003e3.0 uses XID_Start\n        if self.version == (3, 0):\n            id_xid = \"\"\n        else:\n            id_xid = \"X\"\n\n        # To speed things up on CPython, we use the re module to generate a DFA\n        # from our token set and execute it in C. Every result yielded by\n        # iterating this regular expression has exactly one non-empty group\n        # that would correspond to a e.g. lex scanner branch.\n        # The only thing left to Python code is then to select one from this\n        # small set of groups, which is much faster than dissecting the strings.\n        #\n        # A lexer has to obey longest-match rule, but a regular expression does not.\n        # Therefore, the cases in it are carefully sorted so that the longest\n        # ones come up first. The exception is the identifier case, which would\n        # otherwise grab all keywords; it is made to work by making it impossible\n        # for the keyword case to match a word prefix, and ordering it before\n        # the identifier case.\n        self._lex_token_re = re.compile(r\"\"\"\n        [ \\t\\f]* # initial whitespace\n        ( # 1\n            (\\\\)? # ?2 line continuation\n            ([\\n]|[\\r][\\n]|[\\r]) # 3 newline\n        |   (\\#.*) # 4 comment\n        |   ( # 5 floating point or complex literal\n                (?: [0-9]* \\.  [0-9]+\n                |   [0-9]+ \\.?\n                ) [eE] [+-]? [0-9]+\n            |   [0-9]* \\. [0-9]+\n            |   [0-9]+ \\.\n            ) ([jJ])? # ?6 complex suffix\n        |   ([0-9]+) [jJ] # 7 complex literal\n        |   (?: # integer literal\n                ( [1-9]   [0-9]* )       # 8 dec\n            |     0[oO] ( [0-7]+ )       # 9 oct\n            |     0[xX] ( [0-9A-Fa-f]+ ) # 10 hex\n            |     0[bB] ( [01]+ )        # 11 bin\n            |   ( [0-9]   [0-9]* )       # 12 bare oct\n            )\n            [Ll]?\n        |   ([BbUu]?[Rr]?) # ?13 string literal options\n            (?: # string literal start\n                # 14, 15, 16 long string\n                (\"\"\\\"|''') ((?: \\\\?[\\n] | \\\\. | . )*?) (\\14)\n                # 17, 18, 19 short string\n            |   (\"   |'  ) ((?: \\\\ [\\n] | \\\\. | . )*?) (\\17)\n                # 20 unterminated\n            |   (\"\"\\\"|'''|\"|')\n            )\n        |   ((?:{keywords})\\b|{operators}) # 21 keywords and operators\n        |   ([A-Za-z_][A-Za-z0-9_]*\\b) # 22 identifier\n        |   (\\p{{{id_xid}ID_Start}}\\p{{{id_xid}ID_Continue}}*) # 23 Unicode identifier\n        |   ($) # 24 end-of-file\n        )\n        \"\"\".format(keywords=re_keywords, operators=re_operators,\n                   id_xid=id_xid), re.VERBOSE|re.UNICODE)\n\n    # These are identical for all lexer instances.\n    _lex_escape_re = re.compile(r\"\"\"\n    \\\\(?:\n        ([\\n\\\\'\"abfnrtv]) # 1 single-char\n    |   ([0-7]{3})        # 2 oct\n    |   x([0-9A-Fa-f]{2}) # 3 hex\n    )\n    \"\"\", re.VERBOSE)\n\n    _lex_escape_unicode_re = re.compile(_lex_escape_re.pattern + r\"\"\"\n    | \\\\(?:\n        u([0-9A-Fa-f]{4}) # 4 unicode-16\n    |   U([0-9A-Fa-f]{8}) # 5 unicode-32\n    |   N\\{(.+?)\\}        # 6 unicode-name\n    )\n    \"\"\", re.VERBOSE)\n\n    _lex_check_byte_re = re.compile(\"[^\\x00-\\x7f]\")\n\n    def next(self, eof_token=False):\n        \"\"\"\n        Returns token at ``offset`` as a :class:`Token` and advances ``offset``\n        to point past the end of the token, where the token has:\n\n        - *range* which is a :class:`pythonparser.source.Range` that includes\n          the token but not surrounding whitespace,\n        - *kind* which is a string containing one of Python keywords or operators,\n          ``newline``, ``float``, ``int``, ``complex``, ``strbegin``,\n          ``strdata``, ``strend``, ``ident``, ``indent``, ``dedent`` or ``eof``\n          (if ``eof_token`` is True).\n        - *value* which is the flags as lowercase string if *kind* is ``strbegin``,\n          the string contents if *kind* is ``strdata``,\n          the numeric value if *kind* is ``float``, ``int`` or ``complex``,\n          the identifier if *kind* is ``ident`` and ``None`` in any other case.\n\n        :param eof_token: if true, will return a token with kind ``eof``\n            when the input is exhausted; if false, will raise ``StopIteration``.\n        \"\"\"\n        if len(self.queue) == 0:\n            self._refill(eof_token)\n\n        return self.queue.pop(0)\n\n    def peek(self, eof_token=False):\n        \"\"\"Same as :meth:`next`, except the token is not dequeued.\"\"\"\n        if len(self.queue) == 0:\n            self._refill(eof_token)\n\n        return self.queue[-1]\n\n    # We need separate next and _refill because lexing can sometimes\n    # generate several tokens, e.g. INDENT\n    def _refill(self, eof_token):\n        if self.offset == len(self.source_buffer.source):\n            range = source.Range(self.source_buffer, self.offset, self.offset)\n\n            for i in self.indent[1:]:\n                self.indent.pop(-1)\n                self.queue.append(Token(range, \"dedent\"))\n\n            if eof_token:\n                self.queue.append(Token(range, \"eof\"))\n            elif len(self.queue) == 0:\n                raise StopIteration\n\n            return\n\n        match = self._lex_token_re.match(self.source_buffer.source, self.offset)\n        if match is None:\n            diag = diagnostic.Diagnostic(\n                \"fatal\", \"unexpected {character}\",\n                {\"character\": repr(self.source_buffer.source[self.offset]).lstrip(\"u\")},\n                source.Range(self.source_buffer, self.offset, self.offset + 1))\n            self.diagnostic_engine.process(diag)\n\n        # Should we emit indent/dedent?\n        if self.new_line and \\\n                match.group(3) is None and \\\n                match.group(4) is None: # not a blank line\n            whitespace = match.string[match.start(0):match.start(1)]\n            level = len(whitespace.expandtabs())\n            range = source.Range(self.source_buffer, match.start(1), match.start(1))\n            if level \u003e self.indent[-1][0]:\n                self.indent.append((level, range, whitespace))\n                self.queue.append(Token(range, \"indent\"))\n            elif level \u003c self.indent[-1][0]:\n                exact = False\n                while level \u003c= self.indent[-1][0]:\n                    if level == self.indent[-1][0] or self.indent[-1][0] == 0:\n                        exact = True\n                        break\n                    self.indent.pop(-1)\n                    self.queue.append(Token(range, \"dedent\"))\n                if not exact:\n                    note = diagnostic.Diagnostic(\n                        \"note\", \"expected to match level here\", {},\n                        self.indent[-1][1])\n                    error = diagnostic.Diagnostic(\n                        \"fatal\", \"inconsistent indentation\", {},\n                        range, notes=[note])\n                    self.diagnostic_engine.process(error)\n            elif whitespace != self.indent[-1][2] and self.version \u003e= (3, 0):\n                error = diagnostic.Diagnostic(\n                    \"error\", \"inconsistent use of tabs and spaces in indentation\", {},\n                    range)\n                self.diagnostic_engine.process(error)\n\n        # Prepare for next token.\n        self.offset = match.end(0)\n\n        tok_range = source.Range(self.source_buffer, *match.span(1))\n        if match.group(3) is not None: # newline\n            if len(self.parentheses) + len(self.square_braces) + len(self.curly_braces) \u003e 0:\n                # 2.1.6 Implicit line joining\n                return self._refill(eof_token)\n            if match.group(2) is not None:\n                # 2.1.5. Explicit line joining\n                return self._refill(eof_token)\n            if self.new_line and not \\\n                    (self.interactive and match.group(0) == match.group(3)): # REPL terminator\n                # 2.1.7. Blank lines\n                return self._refill(eof_token)\n\n            self.new_line = True\n            self.queue.append(Token(tok_range, \"newline\"))\n            return\n\n        if match.group(4) is not None: # comment\n            self.comments.append(source.Comment(tok_range, match.group(4)))\n            return self._refill(eof_token)\n\n        # Lexing non-whitespace now.\n        self.new_line = False\n\n        if match.group(5) is not None: # floating point or complex literal\n            if match.group(6) is None:\n                self.queue.append(Token(tok_range, \"float\", float(match.group(5))))\n            else:\n                self.queue.append(Token(tok_range, \"complex\", float(match.group(5)) * 1j))\n\n        elif match.group(7) is not None: # complex literal\n            self.queue.append(Token(tok_range, \"complex\", int(match.group(7)) * 1j))\n\n        elif match.group(8) is not None: # integer literal, dec\n            literal = match.group(8)\n            self._check_long_literal(tok_range, match.group(1))\n            self.queue.append(Token(tok_range, \"int\", int(literal)))\n\n        elif match.group(9) is not None: # integer literal, oct\n            literal = match.group(9)\n            self._check_long_literal(tok_range, match.group(1))\n            self.queue.append(Token(tok_range, \"int\", int(literal, 8)))\n\n        elif match.group(10) is not None: # integer literal, hex\n            literal = match.group(10)\n            self._check_long_literal(tok_range, match.group(1))\n            self.queue.append(Token(tok_range, \"int\", int(literal, 16)))\n\n        elif match.group(11) is not None: # integer literal, bin\n            literal = match.group(11)\n            self._check_long_literal(tok_range, match.group(1))\n            self.queue.append(Token(tok_range, \"int\", int(literal, 2)))\n\n        elif match.group(12) is not None: # integer literal, bare oct\n            literal = match.group(12)\n            if len(literal) \u003e 1 and self.version \u003e= (3, 0):\n                error = diagnostic.Diagnostic(\n                    \"error\", \"in Python 3, decimal literals must not start with a zero\", {},\n                    source.Range(self.source_buffer, tok_range.begin_pos, tok_range.begin_pos + 1))\n                self.diagnostic_engine.process(error)\n            self.queue.append(Token(tok_range, \"int\", int(literal, 8)))\n\n        elif match.group(14) is not None: # long string literal\n            self._string_literal(\n                options=match.group(13), begin_span=(match.start(13), match.end(14)),\n                data=match.group(15), data_span=match.span(15),\n                end_span=match.span(16))\n\n        elif match.group(17) is not None: # short string literal\n            self._string_literal(\n                options=match.group(13), begin_span=(match.start(13), match.end(17)),\n                data=match.group(18), data_span=match.span(18),\n                end_span=match.span(19))\n\n        elif match.group(20) is not None: # unterminated string\n            error = diagnostic.Diagnostic(\n                \"fatal\", \"unterminated string\", {},\n                tok_range)\n            self.diagnostic_engine.process(error)\n\n        elif match.group(21) is not None: # keywords and operators\n            kwop = match.group(21)\n            self._match_pair_delim(tok_range, kwop)\n            if kwop == \"print\" and self.print_function:\n                self.queue.append(Token(tok_range, \"ident\", \"print\"))\n            else:\n                self.queue.append(Token(tok_range, kwop))\n\n        elif match.group(22) is not None: # identifier\n            self.queue.append(Token(tok_range, \"ident\", match.group(22)))\n\n        elif match.group(23) is not None: # Unicode identifier\n            if self.version \u003c (3, 0):\n                error = diagnostic.Diagnostic(\n                    \"error\", \"in Python 2, Unicode identifiers are not allowed\", {},\n                    tok_range)\n                self.diagnostic_engine.process(error)\n            self.queue.append(Token(tok_range, \"ident\", match.group(23)))\n\n        elif match.group(24) is not None: # end-of-file\n            # Reuse the EOF logic\n            return self._refill(eof_token)\n\n        else:\n            assert False\n\n    def _string_literal(self, options, begin_span, data, data_span, end_span):\n        options = options.lower()\n        begin_range = source.Range(self.source_buffer, *begin_span)\n        data_range = source.Range(self.source_buffer, *data_span)\n\n        if options not in self._string_prefixes[self.version]:\n            error = diagnostic.Diagnostic(\n                \"error\", \"string prefix '{prefix}' is not available in Python {major}.{minor}\",\n                {\"prefix\": options, \"major\": self.version[0], \"minor\": self.version[1]},\n                begin_range)\n            self.diagnostic_engine.process(error)\n\n        self.queue.append(Token(begin_range, \"strbegin\", options))\n        self.queue.append(Token(data_range,\n                          \"strdata\", self._replace_escape(data_range, options, data)))\n        self.queue.append(Token(source.Range(self.source_buffer, *end_span),\n                          \"strend\"))\n\n    def _replace_escape(self, range, mode, value):\n        is_raw     = (\"r\" in mode)\n        is_byte    = (\"b\" in mode)\n        is_unicode = (\"u\" in mode)\n\n        if is_raw:\n            return value\n\n        if is_byte and self._lex_check_byte_re.match(value):\n            error = diagnostic.Diagnostic(\n                \"error\", \"non-7-bit character in a byte literal\", {},\n                tok_range)\n            self.diagnostic_engine.process(error)\n\n        if is_unicode or self.version \u003e= (3, 0):\n            re = self._lex_escape_unicode_re\n        else:\n            re = self._lex_escape_re\n\n        chunks = []\n        offset = 0\n        while offset \u003c len(value):\n            match = re.search(value, offset)\n            if match is None:\n                # Append the remaining of the string\n                chunks.append(value[offset:])\n                break\n\n            # Append the part of string before match\n            chunks.append(value[offset:match.start()])\n            offset = match.end()\n\n            # Process the escape\n            if match.group(1) is not None: # single-char\n                chr = match.group(1)\n                if chr == \"\\n\":\n                    pass\n                elif chr == \"\\\\\" or chr == \"'\" or chr == \"\\\"\":\n                    chunks.append(chr)\n                elif chr == \"a\":\n                    chunks.append(\"\\a\")\n                elif chr == \"b\":\n                    chunks.append(\"\\b\")\n                elif chr == \"f\":\n                    chunks.append(\"\\f\")\n                elif chr == \"n\":\n                    chunks.append(\"\\n\")\n                elif chr == \"r\":\n                    chunks.append(\"\\r\")\n                elif chr == \"t\":\n                    chunks.append(\"\\t\")\n                elif chr == \"v\":\n                    chunks.append(\"\\v\")\n            elif match.group(2) is not None: # oct\n                chunks.append(unichr(int(match.group(2), 8)))\n            elif match.group(3) is not None: # hex\n                chunks.append(unichr(int(match.group(3), 16)))\n            elif match.group(4) is not None: # unicode-16\n                chunks.append(unichr(int(match.group(4), 16)))\n            elif match.group(5) is not None: # unicode-32\n                try:\n                    chunks.append(unichr(int(match.group(5), 16)))\n                except ValueError:\n                    error = diagnostic.Diagnostic(\n                        \"error\", \"unicode character out of range\", {},\n                        source.Range(self.source_buffer,\n                                     range.begin_pos + match.start(0),\n                                     range.begin_pos + match.end(0)))\n                    self.diagnostic_engine.process(error)\n            elif match.group(6) is not None: # unicode-name\n                try:\n                    chunks.append(unicodedata.lookup(match.group(6)))\n                except KeyError:\n                    error = diagnostic.Diagnostic(\n                        \"error\", \"unknown unicode character name\", {},\n                        source.Range(self.source_buffer,\n                                     range.begin_pos + match.start(0),\n                                     range.begin_pos + match.end(0)))\n                    self.diagnostic_engine.process(error)\n\n        return \"\".join(chunks)\n\n    def _check_long_literal(self, range, literal):\n        if literal[-1] in \"lL\" and self.version \u003e= (3, 0):\n            error = diagnostic.Diagnostic(\n                \"error\", \"in Python 3, long integer literals were removed\", {},\n                source.Range(self.source_buffer, range.end_pos - 1, range.end_pos))\n            self.diagnostic_engine.process(error)\n\n    def _match_pair_delim(self, range, kwop):\n        if kwop == \"(\":\n            self.parentheses.append(range)\n        elif kwop == \"[\":\n            self.square_braces.append(range)\n        elif kwop == \"{\":\n            self.curly_braces.append(range)\n        elif kwop == \")\":\n            self._check_innermost_pair_delim(range, \"(\")\n            self.parentheses.pop()\n        elif kwop == \"]\":\n            self._check_innermost_pair_delim(range, \"[\")\n            self.square_braces.pop()\n        elif kwop == \"}\":\n            self._check_innermost_pair_delim(range, \"{\")\n            self.curly_braces.pop()\n\n    def _check_innermost_pair_delim(self, range, expected):\n        ranges = []\n        if len(self.parentheses) \u003e 0:\n            ranges.append((\"(\", self.parentheses[-1]))\n        if len(self.square_braces) \u003e 0:\n            ranges.append((\"[\", self.square_braces[-1]))\n        if len(self.curly_braces) \u003e 0:\n            ranges.append((\"{\", self.curly_braces[-1]))\n\n        ranges.sort(key=lambda k: k[1].begin_pos)\n        if any(ranges):\n            compl_kind, compl_range = ranges[-1]\n            if compl_kind != expected:\n                note = diagnostic.Diagnostic(\n                    \"note\", \"'{delimiter}' opened here\",\n                    {\"delimiter\": compl_kind},\n                    compl_range)\n                error = diagnostic.Diagnostic(\n                    \"fatal\", \"mismatched '{delimiter}'\",\n                    {\"delimiter\": range.source()},\n                    range, notes=[note])\n                self.diagnostic_engine.process(error)\n        else:\n            error = diagnostic.Diagnostic(\n                \"fatal\", \"mismatched '{delimiter}'\",\n                {\"delimiter\": range.source()},\n                range)\n            self.diagnostic_engine.process(error)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n"
}
{
    "repo_name": "Qumeric/instabattle",
    "ref": "refs/heads/master",
    "path": "tests/test_client.py",
    "copies": "1",
    "content": "import re\nimport unittest\nfrom datetime import datetime\nfrom flask import url_for\nfrom app import create_app, db\nfrom app.models import User, Role\n\n\nclass FlaskClientTestCase(unittest.TestCase):\n    def setUp(self):\n        self.app = create_app('testing')\n        self.app_context = self.app.app_context()\n        self.app_context.push()\n        db.create_all()\n        Role.insert_roles()\n        self.client = self.app.test_client(use_cookies=True)\n\n    def tearDown(self):\n        db.session.remove()\n        db.drop_all()\n        self.app_context.pop()\n\n    def test_register_and_login(self):\n        # register a new account\n        response = self.client.post(\n            url_for('auth.register'),\n            data={\n                'email': 'john@example.com',\n                'username': 'john',\n                'password': '123',\n            })\n        self.assertTrue(response.status_code == 302)\n\n        # login with the new account\n        response = self.client.post(\n            url_for('auth.login'),\n            data={\n                'email': 'john@example.com',\n                'password': '123'\n            },\n            follow_redirects=True)\n\n        # send a confirmation token\n        user = User.query.filter_by(email='john@example.com').first()\n        token = user.generate_confirmation_token()\n        response = self.client.get(\n            url_for('auth.confirm', token=token),\n            follow_redirects=True)\n        self.assertTrue(b'confirmed' in response.data)\n\n        # log out\n        response = self.client.get(\n            url_for('auth.logout'),\n            follow_redirects=True)\n        self.assertTrue(b'You have been logged out' in response.data)\n"
}
{
    "repo_name": "schreiberx/sweet",
    "ref": "refs/heads/master",
    "path": "mule/python/mule/postprocessing/JobData.py",
    "copies": "1",
    "content": "#! /usr/bin/env python3\n\nfrom mule_local.JobGeneration import *\nfrom mule.InfoError import *\n\nimport glob\nimport os\nimport re\nimport sys\n\n\n\nclass JobData(InfoError):\n    \"\"\"\n    Load all available data within job directory.\n\n    This data in includes\n        a) All .pickle files within the directory\n        b) All [MULE] prefixed output in the output.out file\n    \"\"\"\n\n\n    def __init__(\n        self,\n        jobdir = None,\n        verbosity = 10\n    ):\n        InfoError.__init__(self, 'JobData')\n\n        self.verbosity = verbosity\n\n        # Load raw job data\n        self.__load_job_raw_data(jobdir)\n\n        # Create flattened data\n        self.__create_flattened_data()\n\n\n\n    def __str__(self):\n        retstr = \"\"\n        for key, value in self.__flattened_job_data.items():\n            retstr += str(key)+\" =\u003e \"+str(value)+\"\\n\"\n        return retstr\n\n\n\n    def __parse_job_output(\n        self,\n        output_lines\n    ):\n        retdict = {}\n        for l in output_lines:\n            m=re.match(\"^\\[MULE\\] ([^ :]*): (.*)$\", l)\n            if m != None:\n                tag = m.group(1)\n                data = m.group(2)\n\n                if tag in retdict:\n                    print(f\"WARNING: Duplicated tag '{tag}' found with value '{data}'\")\n\n                retdict[tag] = data\n\n        return retdict\n\n\n\n    def __load_job_raw_data(\n            self,\n            jobdir = None\n    ):\n        \"\"\"\n        Parse all output.out files and extract all kind of job output information\n\n        Return a dictionary with content from the job directories\n        {\n            #\n            # Dictionary with data from job generation\n            # (read from [jobdir]/jobgeneration.pickle)\n            #\n            'jobgeneration':    # From jobgeneration.pickle\n            {\n                'compile': [...],\n                'runtime': [...],\n                'parallelization': [...],\n                'platforms_platform': [...],\n                'platform_resources': [...],\n            },\n            'output':        # From output.out with prefix [MULE]\n            {\n                'simulation_benchmark_timings.main': [value],\n                'simulation_benchmark_timings.main_simulationLoop': [value],\n                [...]\n            },\n            '[filename(.pickle)]':\n            {\n                [...]\n            }\n            },\n            '[filename(.pickle)]':\n            {\n                [...]\n            }\n        \"\"\"\n\n        # \n        self.__job_raw_data = {}\n\n        if self.verbosity \u003e 5:\n            self.info(\"\")\n            self.info(\"Processing '\"+jobdir+\"'\")\n\n        \"\"\"\n        * Process 'output.out'\n        \"\"\"\n        if self.verbosity \u003e 5:\n            self.info(\"Loading job output file 'output.out'\")\n        outfile = jobdir+'/output.out'\n        try:\n        #if True:\n            with open(outfile, 'r') as f:\n                content = f.readlines()\n                self.__job_raw_data['output'] = self.__parse_job_output(content)\n\n        except Exception as err:\n            print(\"*\"*80)\n            print(\"* WARNINNG: opening '\"+outfile+\"' (ignoring)\")\n            print(\"* \"+str(err))\n            print(\"*\"*80)\n\n\n        \"\"\"\n        Process 'jobgeneration.pickle'\n        \"\"\"\n        # Ensure that 'jobgeneration.pickle' exists\n        jobgenerationfile = jobdir+'/jobgeneration.pickle'\n        if self.verbosity \u003e 5:\n            self.info(\"Loading 'jobgeneration.pickle'\")\n        j = JobGeneration(dummy_init=True)\n        self.__job_raw_data['jobgeneration'] = j.load_attributes_dict(jobgenerationfile)\n\n        \"\"\"\n        Process other '*.pickle'\n        \"\"\"\n        pickle_files = glob.glob(jobdir+'/*.pickle')\n\n        # Iterate over all found pickle files\n        for picklefile in pickle_files:\n            filename = os.path.basename(picklefile)\n            tag = filename.replace('.pickle', '')\n            if tag == 'jobgeneration':\n                continue\n\n            if self.verbosity \u003e 5:\n                self.info(\"Loading pickle file '\"+filename+\"'\")\n\n            import pickle\n            with open(picklefile, 'rb') as f:\n                self.__job_raw_data[tag] = pickle.load(f)\n\n\n\n    def get_job_raw_data(self):\n        \"\"\"\n        Return the raw job information data\n        Warning: This storage format is likely to change!\n        \"\"\"\n        return self.__job_raw_data\n\n\n\n    def __create_flattened_data(self):\n        \"\"\"\n        Return a dictionary with a flattened hierarchy\n\n        This joins all hierarchical structures with a '.'\n        \n        E.g. the value\n            self.__job_raw_data['jobgeneration'].parallelization.num_cores_per_socket\n        is stored in\n            self.__flattened_job_data['jobgeneration.parallelization.num_cores_per_socket']\n        \"\"\"\n\n        self.__flattened_job_data = {}\n\n        # For all raw data\n        for raw_key, raw_value in self.__job_raw_data.items():\n            if raw_key == 'output':\n                for key, data in raw_value.items():\n                    key = 'output.'+key.lower()\n                    self.__flattened_job_data[key] = data\n\n            elif raw_key == 'jobgeneration':\n                for group_name, group_value in raw_value.items():\n                    if isinstance(group_value, dict):\n                        for key, attr in group_value.items():\n                            key = group_name+\".\"+key\n                            self.__flattened_job_data[key] = attr\n                    elif isinstance(group_value, (int, float, str)):\n                        self.__flattened_job_data[group_name] = group_value\n                    else:\n                        raise Exception(\"Unknown type in pickled data\")\n\n            else:\n                for key, data in raw_value.items():\n                    key = key.lower()\n                    self.__flattened_job_data[raw_key+'.'+key] = data\n    \n\n\n    def get_flattened_data(self):\n        return self.__flattened_job_data\n\n\n\n\n\nif __name__ == \"__main__\":\n\n    verbosity = 0\n\n    jobdirs = []\n\n    if len(sys.argv) \u003e 1:\n        jobdirs = sys.argv[1:]\n    else:\n        print(\"\")\n        print(\"Usage:\")\n        print(\"    \"+sys.argv[0]+\" [jobdir 1] [jobdir 2] ...\")\n        print(\"\")\n        sys.exit(1)\n\n    for j in jobdirs:\n\n        if verbosity \u003e 5:\n            print(\"*\"*80)\n            print(\"Job directory: \"+j)\n            print(\"*\"*80)\n\n        j = JobData(jobdir = j, verbosity=verbosity)\n        d = j.get_flattened_data()\n\n        for key, value in d.items():\n            print(key+\" =\u003e \"+str(value))\n    #            break\n"
}
{
    "repo_name": "KarlParkinson/practice",
    "ref": "refs/heads/master",
    "path": "algs/perms.py",
    "copies": "1",
    "content": "def perms(s):\n    if (len(s) == 1):\n        return [s]\n    p = []\n    for i in range(0, len(s)):\n        j = s[i]\n        c = list(s)\n        c.pop(i)\n        k = perms(c)\n        for item in k:\n            item.insert(0,j)\n            p.append(item)\n    return p\n\n\nprint perms([1,2,3,4,5,6,7,8,9])\n"
}
{
    "repo_name": "svschannak/pirate_materialverwaltung",
    "ref": "refs/heads/master",
    "path": "material/migrations/0002_auto__add_field_bestellungen_status.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\nimport datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Adding field 'Bestellungen.status'\n        db.add_column(u'material_bestellungen', 'status',\n                      self.gf('django.db.models.fields.CharField')(default=0, max_length=255),\n                      keep_default=False)\n\n\n    def backwards(self, orm):\n        # Deleting field 'Bestellungen.status'\n        db.delete_column(u'material_bestellungen', 'status')\n\n\n    models = {\n        u'auth.group': {\n            'Meta': {'object_name': 'Group'},\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '80'}),\n            'permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': u\"orm['auth.Permission']\", 'symmetrical': 'False', 'blank': 'True'})\n        },\n        u'auth.permission': {\n            'Meta': {'ordering': \"(u'content_type__app_label', u'content_type__model', u'codename')\", 'unique_together': \"((u'content_type', u'codename'),)\", 'object_name': 'Permission'},\n            'codename': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['contenttypes.ContentType']\"}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '50'})\n        },\n        u'auth.user': {\n            'Meta': {'object_name': 'User'},\n            'date_joined': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'email': ('django.db.models.fields.EmailField', [], {'max_length': '75', 'blank': 'True'}),\n            'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': u\"orm['auth.Group']\", 'symmetrical': 'False', 'blank': 'True'}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),\n            'is_staff': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'is_superuser': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),\n            'last_login': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}),\n            'last_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),\n            'password': ('django.db.models.fields.CharField', [], {'max_length': '128'}),\n            'user_permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': u\"orm['auth.Permission']\", 'symmetrical': 'False', 'blank': 'True'}),\n            'username': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '30'})\n        },\n        u'contenttypes.contenttype': {\n            'Meta': {'ordering': \"('name',)\", 'unique_together': \"(('app_label', 'model'),)\", 'object_name': 'ContentType', 'db_table': \"'django_content_type'\"},\n            'app_label': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'model': ('django.db.models.fields.CharField', [], {'max_length': '100'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '100'})\n        },\n        u'material.ausleihe': {\n            'Meta': {'object_name': 'Ausleihe', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'bemerkungen': ('django.db.models.fields.TextField', [], {}),\n            'material': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.MaterialStueck']\"}),\n            'reservation_begin': ('django.db.models.fields.DateTimeField', [], {}),\n            'reservation_end': ('django.db.models.fields.DateTimeField', [], {}),\n            'user': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['auth.User']\"})\n        },\n        u'material.basemodel': {\n            'Meta': {'object_name': 'BaseModel'},\n            'created': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'last_update': ('django.db.models.fields.DateTimeField', [], {'auto_now': 'True', 'blank': 'True'}),\n            'notice': ('django.db.models.fields.TextField', [], {})\n        },\n        u'material.bestellungen': {\n            'Meta': {'object_name': 'Bestellungen', '_ormbases': [u'material.BaseModel']},\n            'auftrag': ('django.db.models.fields.files.FileField', [], {'max_length': '100'}),\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'link': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'material_stueck': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.MaterialStueck']\"}),\n            'rechnung': ('django.db.models.fields.files.FileField', [], {'max_length': '100'}),\n            'status': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'typ': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'material.bilder': {\n            'Meta': {'object_name': 'Bilder', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['contenttypes.ContentType']\"}),\n            'file': ('django.db.models.fields.files.ImageField', [], {'max_length': '100'}),\n            'object_id': ('django.db.models.fields.PositiveIntegerField', [], {})\n        },\n        u'material.dokumente': {\n            'Meta': {'object_name': 'Dokumente', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['contenttypes.ContentType']\"}),\n            'file': ('django.db.models.fields.files.FileField', [], {'max_length': '100'}),\n            'object_id': ('django.db.models.fields.PositiveIntegerField', [], {})\n        },\n        u'material.lagerort': {\n            'Meta': {'object_name': 'LagerOrt', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'description': ('django.db.models.fields.TextField', [], {}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'material.material': {\n            'Meta': {'object_name': 'Material', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'kategorie': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.MaterialKategorie']\"}),\n            'lagerort': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.LagerOrt']\"}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'typ': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'zustand': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'material.materialkategorie': {\n            'Meta': {'object_name': 'MaterialKategorie', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'standard_type': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'material.materialstueck': {\n            'Meta': {'object_name': 'MaterialStueck', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'lagerort': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.LagerOrt']\"}),\n            'material': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.Material']\"}),\n            'stueckzahl': ('django.db.models.fields.FloatField', [], {}),\n            'zustand': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'material.reservierung': {\n            'Meta': {'object_name': 'Reservierung', '_ormbases': [u'material.BaseModel']},\n            u'basemodel_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': u\"orm['material.BaseModel']\", 'unique': 'True', 'primary_key': 'True'}),\n            'bemerkungen': ('django.db.models.fields.TextField', [], {}),\n            'material': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['material.MaterialStueck']\"}),\n            'reservation_begin': ('django.db.models.fields.DateTimeField', [], {}),\n            'reservation_end': ('django.db.models.fields.DateTimeField', [], {}),\n            'user': ('django.db.models.fields.related.ForeignKey', [], {'to': u\"orm['auth.User']\"})\n        }\n    }\n\n    complete_apps = ['material']"
}
{
    "repo_name": "inconvergent/differential-lattice",
    "ref": "refs/heads/master",
    "path": "modules/helpers.py",
    "copies": "1",
    "content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\ndef load_kernel(fn, name, subs={}):\n\n  from pycuda.compiler import SourceModule\n\n  with open(fn, 'r') as f:\n    kernel = f.read()\n\n  for k,v in list(subs.items()):\n    kernel = kernel.replace(k, str(v))\n\n  mod = SourceModule(kernel)\n  return mod.get_function(name)\n\ndef spawn_darts(dl, n, xy, dst, rad=0.4):\n\n  from dddUtils.random import darts\n  num = dl.num\n\n  new_xy = darts(n, 0.5, 0.5, rad, dst)\n  new_num = len(new_xy)\n  if new_num\u003e0:\n    dl.xy[num:num+new_num,:] = new_xy\n\n  dl.num += new_num\n  return new_num\n\ndef spawn_circle(dl, n, xy, dst, rad=0.4):\n\n  from numpy.random import random\n  from numpy import pi\n  from numpy import column_stack\n  from numpy import sin\n  from numpy import cos\n\n  num = dl.num\n  theta = random(n)*2*pi\n  new_xy = xy + column_stack([cos(theta), sin(theta)])*rad\n  new_num = len(new_xy)\n  if new_num\u003e0:\n    dl.xy[num:num+new_num,:] = new_xy\n\n  dl.num += new_num\n  return new_num\n\ndef get_colors(f, do_shuffle=True):\n  from numpy import array\n  try:\n    import Image\n  except Exception:\n    from PIL import Image\n\n  im = Image.open(f)\n  data = array(list(im.convert('RGB').getdata()),'float')/255.0\n\n  res = []\n  for rgb in data:\n    res.append(list(rgb))\n\n  if do_shuffle:\n    from numpy.random import shuffle\n    shuffle(res)\n  return res\n"
}
{
    "repo_name": "VojtechBartos/smsgw",
    "ref": "refs/heads/master",
    "path": "smsgw/resources/contacts/schemas/put.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\n# http://google-styleguide.googlecode.com/svn/trunk/pyguide.html\n\nfrom smsgw.resources import patterns\n\nschema = {\n    \"description\": \"Schema for the contacts PUT endpoint\",\n    \"type\": \"object\",\n    \"method\": \"PUT\",\n    \"required\": [ \"firstName\", \"lastName\", \"phoneNumber\" ],\n    \"additionalProperties\": False,\n    \"properties\": {\n        \"firstName\": {\n            \"type\": \"string\",\n            \"minLength\": 2,\n            \"maxLength\": 16,\n            \"messages\": {\n                \"minLength\": \"Max length of first name is 2 characters.\",\n                \"maxLength\": \"Max length of first name is 16 characters.\"\n            }\n        },\n        \"lastName\": {\n            \"type\": \"string\",\n            \"minLength\": 2,\n            \"maxLength\": 16,\n            \"messages\": {\n                \"minLength\": \"Max length of last name is 2 characters.\",\n                \"maxLength\": \"Max length of last name is 16 characters.\"\n            }\n        },\n        \"email\": {\n            \"type\": [\"string\", \"null\"],\n            \"pattern\": \"^(%s)?$\" % patterns.EMAIL,\n            \"messages\": {\n                \"pattern\": \"E-mail is in wrong format.\"\n            }\n        },\n        \"phoneNumber\": {\n            \"type\": \"string\",\n            \"pattern\": patterns.PHONE_NUMBER,\n            \"messages\": {\n                \"type\": \"Phone number needs to be string.\",\n                \"pattern\": \"Phone number has invalid format. (+420736202512 as an example.)\"\n            }\n        },\n        \"note\": {\n            \"type\": [\"string\", \"null\"],\n            \"maxLength\": 255,\n            \"messages\": {\n                \"maxLength\": \"Max length of note is 255 characters.\"\n            }\n        },\n        \"tags\": {\n            \"type\": [\"array\", \"null\"],\n            \"items\": {\n                \"type\": \"string\",\n                \"minLength\": 1,\n                \"maxLength\": 23\n            }\n        }\n    }\n}\n"
}
{
    "repo_name": "paul-xxx/micropython",
    "ref": "refs/heads/master",
    "path": "docs/conf.py",
    "copies": "1",
    "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# Micro Python documentation build configuration file, created by\n# sphinx-quickstart on Sun Sep 21 11:42:03 2014.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.todo',\n    'sphinx.ext.coverage',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'Micro Python'\ncopyright = '2014, Damien P. George'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '1.4'\n# The full version, including alpha/beta/rc tags.\nrelease = '1.4.1'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['build']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# on_rtd is whether we are on readthedocs.org\non_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n\nif not on_rtd:  # only import and set the theme if we're building docs locally\n    try:\n        import sphinx_rtd_theme\n        html_theme = 'sphinx_rtd_theme'\n        html_theme_path = [sphinx_rtd_theme.get_html_theme_path(), '.']\n    except:\n        html_theme = 'default'\n        html_theme_path = ['.']\nelse:\n    html_theme_path = ['.']\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = ['.']\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"\u003cproject\u003e v\u003crelease\u003e documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = '../logo/trans-logo.png'\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['static']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = '%d %b %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\nhtml_additional_pages = {\"index\": \"topindex.html\"}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a \u003clink\u003e tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'MicroPythondoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  ('index', 'MicroPython.tex', 'Micro Python Documentation',\n   'Damien P. George', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'micropython', 'Micro Python Documentation',\n     ['Damien P. George'], 1),\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'MicroPython', 'Micro Python Documentation',\n   'Damien P. George', 'MicroPython', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'http://docs.python.org/': None}\n"
}
{
    "repo_name": "behavior3/behavior3py",
    "ref": "refs/heads/master",
    "path": "b3/core/condition.py",
    "copies": "2",
    "content": "import b3\n\n__all__ = ['Condition']\n\n\nclass Condition(b3.BaseNode):\n    category = b3.CONDITION\n\n    def __init__(self):\n        super(Condition, self).__init__()\n        \n"
}
{
    "repo_name": "OpenSourcePolicyCenter/multi-country",
    "ref": "refs/heads/master",
    "path": "Python/Archive/Stage3/AuxiliaryDemographics.py",
    "copies": "2",
    "content": "from __future__ import division\nimport csv\nimport numpy as np\nimport scipy as sp\nimport scipy.optimize as opt\nimport time as time\nfrom matplotlib import pyplot as plt\n\n\n#DEMOGRAPHICS FUNCTIONS\ndef getkeyages(S, PrintAges):\n    \"\"\"\n    Description:\n        -Gets the key ages for calculating demographics based on S\n\n    Inputs:\n        S                = Int in [10,80], Number of cohorts\n        PrintAges        = Boolean, Prints key ages if true\n\n    Functions called:\n        -None\n\n    Objects in Function:\n        agestopull        = (S) List, Contains the ages from the data files to use for the demographic data in the program\n        LeaveHouseAge     = Int, Age where children become adults and are no longer dependent on parents for consumption\n        FirstFertilityAge = Int in (0,S), First age when agents bear children\n        LastFertilityAge  = Int in (0,S), Last age when agents bear children\n        FirstDyingAge     = Int in (0,S), First age when agents die\n        MaxImmigrantAge   = Int in (0,S), Age of the oldest immigrants\n       \n    Returns: LeaveHouseAge, FirstFertilityAge, LastFertilityAge, MaxImmigrantAge, FirstDyingAge, agestopull\n    \"\"\"\n\n    agestopull = np.arange(80)[np.where(np.in1d(np.arange(80), np.round(np.linspace(0, 80, num=S, endpoint=False))))]\n    LeaveHouseAge = np.abs(agestopull - 21).argmin()#The age when agents become independent households and don't rely on parents consumption\n    FirstFertilityAge = np.abs(agestopull - 23).argmin()#The age when agents have their first children\n    LastFertilityAge = np.abs(agestopull - 45).argmin()#The age when agents have their last children\n    MaxImmigrantAge = np.abs(agestopull - 65).argmin()#All immigrants are between ages 0 and MaxImmigrantAge\n    FirstDyingAge = np.abs(agestopull - 68).argmin()#The first age agents can begin to die\n\n    #Insures that parents don't die with kids in the home\n    if FirstDyingAge - (LeaveHouseAge + LastFertilityAge) \u003c= 0:\n        FirstDyingAge+=1\n\n    #Makes sure we aren't pulling anything out of bounds from the data files\n    if agestopull[FirstDyingAge] \u003c 68:\n        agestopull[FirstDyingAge] = 45\n    if agestopull[FirstFertilityAge] \u003c 23:\n        agestopull[FirstFertilityAge] = 23\n    if agestopull[LastFertilityAge] \u003e 45:\n        agestopull[LastFertilityAge] = 45\n\n    return LeaveHouseAge, FirstFertilityAge, LastFertilityAge, MaxImmigrantAge, FirstDyingAge, agestopull\n\ndef plotDemographics(ages, datasets, I, S, T, I_touse, T_touse = None, compare_across = \"T\", data_year = 0):\n    \"\"\"\n    Description:\n        - Displays two plots that show the following:\n            plot1: For each country: Mortality, fertility, and immigration rates, \n                                     initial and steady state population shares, \n                                     and the transition path of the total population\n\n            plot2: Will show 2 different plots depending on the input value 'compare_across'.\n                     If compare_across == \"T\": plot2 will display a plot of Nhat for each year in the function input T_touse with each country \n                     If compare_across == \"I\": plot2 will display a plot for each country of Nhat in whatever year in input value 'data_year' is\n\n    Inputs:\n        - ages                  = tuple: Contains FirstFertilityAge, LastFertilityAge, FirstDyingAge, and MaxImmigrantAge from the OLG class\n        - datasets              = tuple: Contains the arrays FertilityRates, MortalityRates, ImmigrationRates, and Nhat from the OLG class\n        - I                     = Int: Number of Countries\n        - S                     = Int: Number of Cohorts\n        - T                     = Int: Number of the total amount of time periods\n        - I_touse               = List: [I], Roster of countries that are being used\n        - T_touse               = List: [Unknown], List of years in plot2 given from user input\n        - compare_across        = String: (Either \"T\" or \"I\"), changes the output of plot2 (see function description)\n        - data_year             = Int: The year plot1 will display for each countries demographic data\n\n    Variables Called from Object:\n        - FirstFertilityAge     = Int: First age where agents give birth\n        - LastFertilityAge      = Int: Last age where agents give birth\n        - FirstDyingAge         = Int: First age where mortality rates effect agents\n        - MaxImmigrantAge       = Int: No immigration takes place for cohorts older than this age\n        - FertilityRates        = Array: [I,S,T], Fertility rates for each country, cohort, and year\n        - MortalityRates        = Array: [I,S,T], Mortality rates for each country, cohort, and year\n        - ImmigrationRates      = Array: [I,S,T], Immigration rates for each country, cohort, and year\n        - Nhat                  = Array: [I,S,T], Population shares for each country, cohort, and year\n\n    Other Functions Called:\n        -None\n\n    Objects in Function:\n        - subplotdim_dict       = Dictionary: [6], Contains keys for each int 2-7 that maps to a dimensionality of the subplots of plot2\n        - magic_int             = Int: Value from subplotdim_dict that indicates the number and dimensionality of subplots of plot2\n\n    Outputs:\n        - None\n    \"\"\"\n\n    FirstFertilityAge, LastFertilityAge, FirstDyingAge, MaxImmigrantAge = ages\n    FertilityRates, MortalityRates, ImmigrationRates, Nhat = datasets\n\n    if T_touse is None or T_touse == \"default\":\n        T_touse = [0, S//4, S//2, S, T]\n\n    #The firstPlot is a subplot of key demographic data and the population dynamics for each country\n    def firstPlot():\n        plt.subplot(231)\n        for i in range(I):\n            plt.plot(range(FirstDyingAge, S-1), MortalityRates[i,FirstDyingAge:-1,data_year])\n        plt.title(\"Mortality Rates\", fontsize=14)\n        plt.xlabel('Age')\n        plt.ylabel('Mortality Rate')\n\n\n        plt.subplot(232)\n        for i in range(I):\n            plt.plot(range(FirstFertilityAge,LastFertilityAge+1), FertilityRates[i,FirstFertilityAge:LastFertilityAge+1,data_year])\n        plt.legend(I_touse, prop={'size':11}, loc=\"upper right\")\n        plt.title(\"Fertility Rates\", fontsize=14)\n        plt.xlabel('Age')\n        plt.ylabel('Fertility Rate')\n\n\n        plt.subplot(233)\n        for i in range(I):\n            plt.plot(range(MaxImmigrantAge), ImmigrationRates[i,:MaxImmigrantAge,data_year])\n        plt.title(\"Immigration Rates\", fontsize=14)\n        plt.xlabel('Age')\n        plt.ylabel('Immigration Rate')\n\n        plt.subplot(234)\n        for i in range(I):\n            plt.plot(range(S), Nhat[i,:,data_year])\n        plt.xlabel('Age')\n        plt.ylabel('Population Share')\n        plt.title(\"Initial Population Shares\", fontsize=14)\n\n        #Population Shares at the SS\n        plt.subplot(235)\n        for i in range(I):\n            plt.plot(range(S), Nhat[i,:,-1])\n        plt.xlabel('Age')\n        plt.ylabel('Population Share')\n        plt.title(\"Steady State Population Shares\", fontsize=14)\n\n        #Transition path for total population of each country from the initial shares to the steady-state\n        plt.subplot(236)           \n        for i in range(I):\n            plt.plot(range(T), np.sum(Nhat[i,:,:T], axis=0))\n        plt.title(\"Total Pop Shares Transition Path\", fontsize=14)\n        plt.xlabel('Year')\n        plt.ylabel('Total Population Share')\n\n        plt.show()\n\n    #The secondPlot compares population shares across years or across countries depending on the function input 'compare_across'\n    def secondPlot():\n\n        #Dictionary that contains ideal dimensions of the subplot depending on how many countries are being used\n        subplotdim_dict = {2:222, 3:222, 4:222, 5:232, 6:232, 7:242}\n\n        #If we want to compare each country in a given year...\n        if compare_across == \"T\":\n\n            if len(T_touse) == 1:\n                for i in range(I):\n                    plt.plot(range(S), Nhat[i,:,T_touse[0]])\n                if T_touse[0] \u003c 0:\n                    T_touse[0] += T+1\n                plt.title(\"Time t =\" + str(T_touse[0]), fontsize=14)\n                plt.xlabel('Age')\n                plt.ylabel('Population Share')\n                plt.legend(I_touse, loc=\"upper right\")\n\n            else:\n\n                if len(T_touse) \u003e 8: raise ValueError(\"Too many years to plot\")\n                magic_int = subplotdim_dict[len(T_touse)]\n\n                plt.subplot(magic_int-1)\n                for i in range(I): plt.plot(0,0)\n                plt.legend(I_touse, loc=\"center\")\n                plt.gca().axes.get_xaxis().set_visible(False)\n                plt.gca().axes.get_yaxis().set_visible(False)\n\n                for count, t in enumerate(T_touse):\n\n                    plt.subplot(magic_int+count)\n                    for i in range(I):\n                        plt.plot(range(S), Nhat[i,:,t])\n                        if t \u003c 0:\n                            t += T+1\n                        plt.title(\"Time t =\" + str(t), fontsize=14)\n                        plt.xlabel('Age')\n                        plt.ylabel('Population Share')\n\n        #If we want to compare years for each country...\n        elif compare_across == \"I\":\n            magic_int = subplotdim_dict[I]\n\n            plt.subplot(magic_int-1)\n            for t in range(T): plt.plot(0,0)\n            legend = [\"t = \" + str(T_touse[index]) if t \u003e= 0 else \"t = \" + str(T+1+T_touse[index]) for index, t in enumerate(T_touse)]\n            plt.legend(legend, loc=\"center\")\n            plt.gca().axes.get_xaxis().set_visible(False)\n            plt.gca().axes.get_yaxis().set_visible(False)\n\n            for i in range(I):\n                plt.subplot(magic_int+i)\n\n                for t in T_touse:\n                    plt.plot(range(S), Nhat[i,:,t])\n                    plt.title(I_touse[i], fontsize=14)\n                    plt.xlabel('Age')\n                    plt.ylabel('Population Share')\n        \n        else: raise TypeError(compare_across + \" is not a valid name for 'compare_across'. Choose either 'T' or 'I'\")\n\n        plt.tight_layout()\n\n        plt.show()\n\n    firstPlot()\n    secondPlot()"
}
{
    "repo_name": "andrewyoung1991/scons",
    "ref": "refs/heads/master",
    "path": "test/Deprecated/SourceCode/CVS/CVS.py",
    "copies": "2",
    "content": "#!/usr/bin/env python\n#\n# __COPYRIGHT__\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n__revision__ = \"__FILE__ __REVISION__ __DATE__ __DEVELOPER__\"\n\n\"\"\"\nTest fetching source files from CVS.\n\"\"\"\n\nimport os\n\nimport TestSCons\n\ntest = TestSCons.TestSCons(match = TestSCons.match_re_dotall)\n\ntest.write('SConscript', \"\"\"\nEnvironment(tools = ['CVS']).CVS('')\n\"\"\")\n\nmsg_cvs = \"\"\"The CVS() factory is deprecated and there is no replacement.\"\"\"\nwarn_cvs = test.deprecated_fatal('deprecated-build-dir', msg_cvs)\nmsg_sc = \"\"\"SourceCode() has been deprecated and there is no replacement.\n\\tIf you need this function, please contact scons-dev@scons.org\"\"\"\nwarn_sc = test.deprecated_wrap(msg_sc)\n\ncvs = test.where_is('cvs')\nif not cvs:\n    test.skip_test(\"Could not find 'cvs'; skipping remaining tests.\\n\")\n\ntest.subdir('CVS', 'import', ['import', 'sub'], 'work1', 'work2')\n\nfoo_aaa_in = os.path.join('foo', 'aaa.in')\nfoo_bbb_in = os.path.join('foo', 'bbb.in')\nfoo_ccc_in = os.path.join('foo', 'ccc.in')\nfoo_sub_ddd_in = os.path.join('foo', 'sub', 'ddd.in')\nfoo_sub_ddd_out = os.path.join('foo', 'sub', 'ddd.out')\nfoo_sub_eee_in = os.path.join('foo', 'sub', 'eee.in')\nfoo_sub_eee_out = os.path.join('foo', 'sub', 'eee.out')\nfoo_sub_fff_in = os.path.join('foo', 'sub', 'fff.in')\nfoo_sub_fff_out = os.path.join('foo', 'sub', 'fff.out')\nfoo_sub_all = os.path.join('foo', 'sub', 'all')\n\nsub_SConscript = os.path.join('sub', 'SConscript')\nsub_ddd_in = os.path.join('sub', 'ddd.in')\nsub_ddd_out = os.path.join('sub', 'ddd.out')\nsub_eee_in = os.path.join('sub', 'eee.in')\nsub_eee_out = os.path.join('sub', 'eee.out')\nsub_fff_in = os.path.join('sub', 'fff.in')\nsub_fff_out = os.path.join('sub', 'fff.out')\nsub_all = os.path.join('sub', 'all')\n\n# Set up the CVS repository.\ncvsroot = test.workpath('CVS')\n\nos.environ['CVSROOT'] = cvsroot\ntest.run(program = cvs, arguments = 'init')\n\ntest.write(['import', 'aaa.in'], \"import/aaa.in\\n\")\ntest.write(['import', 'bbb.in'], \"import/bbb.in\\n\")\ntest.write(['import', 'ccc.in'], \"import/ccc.in\\n\")\n\ntest.write(['import', 'sub', 'SConscript'], \"\"\"\\\nImport(\"env\")\nenv.Cat('ddd.out', 'ddd.in')\nenv.Cat('eee.out', 'eee.in')\nenv.Cat('fff.out', 'fff.in')\nenv.Cat('all', ['ddd.out', 'eee.out', 'fff.out'])\n\"\"\")\n\ntest.write(['import', 'sub', 'ddd.in'], \"import/sub/ddd.in\\n\")\ntest.write(['import', 'sub', 'eee.in'], \"import/sub/eee.in\\n\")\ntest.write(['import', 'sub', 'fff.in'], \"import/sub/fff.in\\n\")\n\ntest.run(chdir = 'import',\n         program = cvs,\n         arguments = '-q import -m import foo v v-r')\n\n# Test the most straightforward CVS checkouts, using the module name.\ntest.write(['work1', 'SConstruct'], \"\"\"\nSetOption('warn', 'deprecated-source-code')\nimport os\ndef cat(env, source, target):\n    target = str(target[0])\n    f = open(target, \"wb\")\n    for src in source:\n        f.write(open(str(src), \"rb\").read())\n    f.close()\nenv = Environment(ENV = { 'PATH' : os.environ['PATH'],\n                          'EDITOR' : os.environ.get('EDITOR', 'ed') },\n                  BUILDERS={'Cat':Builder(action=cat)})\nenv.Prepend(CVSFLAGS='-Q')\nenv.Cat('aaa.out', 'foo/aaa.in')\nenv.Cat('bbb.out', 'foo/bbb.in')\nenv.Cat('ccc.out', 'foo/ccc.in')\nenv.Cat('all', ['aaa.out', 'bbb.out', 'ccc.out'])\nenv.SourceCode('.', env.CVS(r'%(cvsroot)s'))\nSConscript('foo/sub/SConscript', \"env\")\n\"\"\" % locals())\n\ntest.subdir(['work1', 'foo'])\ntest.write(['work1', 'foo', 'bbb.in'], \"work1/foo/bbb.in\\n\")\n\ntest.subdir(['work1', 'foo', 'sub',])\ntest.write(['work1', 'foo', 'sub', 'eee.in'], \"work1/foo/sub/eee.in\\n\")\n\nread_str = \"\"\"\\\ncvs -Q -d %(cvsroot)s co foo/sub/SConscript\n\"\"\" % locals()\n\nbuild_str = \"\"\"\\\ncvs -Q -d %(cvsroot)s co foo/aaa.in\ncat([\"aaa.out\"], [\"%(foo_aaa_in)s\"])\ncat([\"bbb.out\"], [\"%(foo_bbb_in)s\"])\ncvs -Q -d %(cvsroot)s co foo/ccc.in\ncat([\"ccc.out\"], [\"%(foo_ccc_in)s\"])\ncat([\"all\"], [\"aaa.out\", \"bbb.out\", \"ccc.out\"])\ncvs -Q -d %(cvsroot)s co foo/sub/ddd.in\ncat([\"%(foo_sub_ddd_out)s\"], [\"%(foo_sub_ddd_in)s\"])\ncat([\"%(foo_sub_eee_out)s\"], [\"%(foo_sub_eee_in)s\"])\ncvs -Q -d %(cvsroot)s co foo/sub/fff.in\ncat([\"%(foo_sub_fff_out)s\"], [\"%(foo_sub_fff_in)s\"])\ncat([\"%(foo_sub_all)s\"], [\"%(foo_sub_ddd_out)s\", \"%(foo_sub_eee_out)s\", \"%(foo_sub_fff_out)s\"])\n\"\"\" % locals()\n\nstdout = test.wrap_stdout(read_str = read_str, build_str = build_str)\n\ntest.run(chdir = 'work1',\n         arguments = '.',\n         stdout = TestSCons.re_escape(stdout),\n         stderr = warn_cvs + warn_sc)\n\n# Checking things back out of CVS apparently messes with the line\n# endings, so read the result files in non-binary mode.\n\ntest.must_match(['work1', 'all'],\n                \"import/aaa.in\\nwork1/foo/bbb.in\\nimport/ccc.in\\n\",\n                mode='r')\n\ntest.must_match(['work1', 'foo', 'sub', 'all'],\n                \"import/sub/ddd.in\\nwork1/foo/sub/eee.in\\nimport/sub/fff.in\\n\",\n                mode='r')\n\ntest.must_be_writable(test.workpath('work1', 'foo', 'sub', 'SConscript'))\ntest.must_be_writable(test.workpath('work1', 'foo', 'aaa.in'))\ntest.must_be_writable(test.workpath('work1', 'foo', 'ccc.in'))\ntest.must_be_writable(test.workpath('work1', 'foo', 'sub', 'ddd.in'))\ntest.must_be_writable(test.workpath('work1', 'foo', 'sub', 'fff.in'))\n\n# Test CVS checkouts when the module name is specified.\ntest.write(['work2', 'SConstruct'], \"\"\"\nSetOption('warn', 'deprecated-source-code')\nimport os\ndef cat(env, source, target):\n    target = str(target[0])\n    f = open(target, \"wb\")\n    for src in source:\n        f.write(open(str(src), \"rb\").read())\n    f.close()\nenv = Environment(ENV = { 'PATH' : os.environ['PATH'],\n                          'EDITOR' : os.environ.get('EDITOR', 'ed') },\n                  BUILDERS={'Cat':Builder(action=cat)})\nenv.Prepend(CVSFLAGS='-q')\nenv.Cat('aaa.out', 'aaa.in')\nenv.Cat('bbb.out', 'bbb.in')\nenv.Cat('ccc.out', 'ccc.in')\nenv.Cat('all', ['aaa.out', 'bbb.out', 'ccc.out'])\nenv.SourceCode('.', env.CVS(r'%(cvsroot)s', 'foo'))\nSConscript('sub/SConscript', \"env\")\n\"\"\" % locals())\n\ntest.write(['work2', 'bbb.in'], \"work2/bbb.in\\n\")\n\ntest.subdir(['work2', 'sub'])\ntest.write(['work2', 'sub', 'eee.in'], \"work2/sub/eee.in\\n\")\n\nread_str = \"\"\"\\\ncvs -q -d %(cvsroot)s co -d sub foo/sub/SConscript\nU sub/SConscript\n\"\"\" % locals()\n\nbuild_str = \"\"\"\\\ncvs -q -d %(cvsroot)s co -d . foo/aaa.in\nU ./aaa.in\ncat([\"aaa.out\"], [\"aaa.in\"])\ncat([\"bbb.out\"], [\"bbb.in\"])\ncvs -q -d %(cvsroot)s co -d . foo/ccc.in\nU ./ccc.in\ncat([\"ccc.out\"], [\"ccc.in\"])\ncat([\"all\"], [\"aaa.out\", \"bbb.out\", \"ccc.out\"])\ncvs -q -d %(cvsroot)s co -d sub foo/sub/ddd.in\nU sub/ddd.in\ncat([\"%(sub_ddd_out)s\"], [\"%(sub_ddd_in)s\"])\ncat([\"%(sub_eee_out)s\"], [\"%(sub_eee_in)s\"])\ncvs -q -d %(cvsroot)s co -d sub foo/sub/fff.in\nU sub/fff.in\ncat([\"%(sub_fff_out)s\"], [\"%(sub_fff_in)s\"])\ncat([\"%(sub_all)s\"], [\"%(sub_ddd_out)s\", \"%(sub_eee_out)s\", \"%(sub_fff_out)s\"])\n\"\"\" % locals()\n\nstdout = test.wrap_stdout(read_str = read_str, build_str = build_str)\n\ntest.run(chdir = 'work2',\n         arguments = '.',\n         stdout = TestSCons.re_escape(stdout),\n         stderr = warn_cvs + warn_sc)\n\n# Checking things back out of CVS apparently messes with the line\n# endings, so read the result files in non-binary mode.\n\ntest.must_match(['work2', 'all'],\n                \"import/aaa.in\\nwork2/bbb.in\\nimport/ccc.in\\n\",\n                mode='r')\n\ntest.must_match(['work2', 'sub', 'all'],\n                \"import/sub/ddd.in\\nwork2/sub/eee.in\\nimport/sub/fff.in\\n\",\n                mode='r')\n\ntest.must_be_writable(test.workpath('work2', 'sub', 'SConscript'))\ntest.must_be_writable(test.workpath('work2', 'aaa.in'))\ntest.must_be_writable(test.workpath('work2', 'ccc.in'))\ntest.must_be_writable(test.workpath('work2', 'sub', 'ddd.in'))\ntest.must_be_writable(test.workpath('work2', 'sub', 'fff.in'))\n\n# Test checking out specific file name(s), and expanding\n# the repository name with a variable.\ntest.subdir(['work3'])\n\ntest.write(['work3', 'SConstruct'], \"\"\"\\\nSetOption('warn', 'deprecated-source-code')\nimport os\ndef cat(env, source, target):\n    target = str(target[0])\n    f = open(target, \"wb\")\n    for src in source:\n        f.write(open(str(src), \"rb\").read())\n    f.close()\nenv = Environment(ENV = { 'PATH' : os.environ['PATH'],\n                          'EDITOR' : os.environ.get('EDITOR', 'ed') },\n                  BUILDERS={'Cat':Builder(action=cat)},\n                  CVSROOT=r'%s')\nenv.Prepend(CVSFLAGS='-q')\nenv.Cat('aaa.out', 'aaa.in')\nenv.Cat('bbb.out', 'bbb.in')\nenv.Cat('ccc.out', 'ccc.in')\nenv.Cat('all', ['aaa.out', 'bbb.out', 'ccc.out'])\ncvs = env.CVS('$CVSROOT', 'foo')\nenv.SourceCode('aaa.in', cvs)\nenv.SourceCode('bbb.in', cvs)\nenv.SourceCode('ccc.in', cvs)\n\"\"\" % cvsroot)\n\nbuild_str = \"\"\"\\\ncvs -q -d %(cvsroot)s co -d . foo/aaa.in\nU ./aaa.in\ncat([\"aaa.out\"], [\"aaa.in\"])\ncvs -q -d %(cvsroot)s co -d . foo/bbb.in\nU ./bbb.in\ncat([\"bbb.out\"], [\"bbb.in\"])\ncvs -q -d %(cvsroot)s co -d . foo/ccc.in\nU ./ccc.in\ncat([\"ccc.out\"], [\"ccc.in\"])\ncat([\"all\"], [\"aaa.out\", \"bbb.out\", \"ccc.out\"])\n\"\"\" % locals()\n\nstdout = test.wrap_stdout(build_str = build_str)\n\ntest.run(chdir = 'work3',\n         arguments = '.',\n         stdout = TestSCons.re_escape(stdout),\n         stderr = warn_cvs + 3*warn_sc)\n\ntest.must_match(['work3', 'aaa.out'],\n                \"import/aaa.in\\n\",\n                mode='r')\ntest.must_match(['work3', 'bbb.out'],\n                \"import/bbb.in\\n\",\n                mode='r')\ntest.must_match(['work3', 'ccc.out'],\n                \"import/ccc.in\\n\",\n                mode='r')\ntest.must_match(['work3', 'all'],\n                \"import/aaa.in\\nimport/bbb.in\\nimport/ccc.in\\n\",\n                mode='r')\n\n# Test CVS checkouts from a remote server (Tigris.org).\n#test.subdir(['work4'])\n#\n#test.write(['work4', 'SConstruct'], \"\"\"\\\n#SetOption('warn', 'deprecated-source-code')\n#import os\n#env = Environment(ENV = { 'PATH' : os.environ['PATH'] })\n## We used to use the SourceForge server, but SourceForge has restrictions\n## that make them deny access on occasion.  Leave the incantation here\n## in case we need to use it again some day.\n##cvs = env.CVS(':pserver:anonymous@cvs.sourceforge.net:/cvsroot/scons')\n#cvs = env.CVS(':pserver:anoncvs@cvs.tigris.org:/cvs')\n#env.SourceCode('.', cvs)\n#env.Install('install', 'scons/SConstruct')\n#\"\"\")\n#\n#test.run(chdir = 'work4', arguments = '.')\n#\n#test.must_exist(test.workpath('work4', 'install', 'SConstruct'))\n\n\ntest.pass_test()\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n"
}
{
    "repo_name": "Alofoxx/club-websystem",
    "ref": "refs/heads/master",
    "path": "src/contentblocks/templatetags/contentblock_renderer.py",
    "copies": "2",
    "content": "import json\nimport fnmatch\n\nfrom contentblocks.models import Block\n\nfrom contentblocks.views import NAVBAR_BLOCK_ID, wrap_blob_with_editablediv, render_blob\n\nfrom django import template\nregister = template.Library()\n\ndef _generate_navbar(nd, curpage):\n  output = \"\"\n  for pgref in nd:\n    pagename = pgref[0]\n    if pagename==\"-\":\n      # Just show a divider; nothing else\n      output += \"\u003cli class='divider'\u003e\u003c/li\u003e\"\n    else:\n      url = pgref[1]\n      if (type(url)==type([])) or (type(url)==type(())):\n        # Call ourselves recursively to build a sub-menu\n        output += \"\u003cli class='%s dropdown'\u003e\u003ca href='#' class='dropdown-toggle' data-toggle='dropdown' role='button' aria-expanded='false'\u003e%s \u003cspan class='caret'\u003e\u003c/span\u003e\u003c/a\u003e\u003cul class='dropdown-menu' role='menu'\u003e%s\u003c/ul\u003e\u003c/li\u003e\" % (\n          'active' if fnmatch.fnmatch(curpage, pgref[2]) else '',\n          pagename,\n          _generate_navbar(url, curpage)\n        )\n      else:\n        # Create a standard menu item\n        output += \"\u003cli class='%s'\u003e\u003ca href='%s'\u003e%s\u003c/a\u003e\u003c/li\u003e\" % (\n          'active' if url==curpage else '',\n          url,\n          pagename\n        )\n  return output\n\n@register.simple_tag(takes_context=True)\ndef generate_navbar(context):\n  try:\n    # Because Block has a custom manager, the results are filtered already even before adding this filter\n    contentblock = Block.objects.get(uniquetitle=NAVBAR_BLOCK_ID, datatype=Block.JSON)\n    navdata = json.loads(contentblock.blob)\n  except Block.DoesNotExist:\n    navdata = [['{Cannot find block with a unique title of \"%s\"}' % NAVBAR_BLOCK_ID, '/']]\n  except:\n    navdata = [['{Error processing block \"%s\"}' % NAVBAR_BLOCK_ID, '/']]\n\n  return _generate_navbar(navdata, context.request.path_info)\n\n\n\n\n\n@register.simple_tag(takes_context=True)\ndef load_contentblock(context, uniquetitle):\n  user = context.request.user\n  user_auth = user.is_authenticated()\n\n  try:\n    contentblock = Block.objects.get(uniquetitle=uniquetitle)\n\n    # Certain blocks may be marked as auth_required\n    if contentblock.auth_required and not user_auth:\n      blob = \"\u003cspan style='color:red'\u003eERROR: You must be logged in to view this block\u003c/span\u003e\"\n    else:\n      blob = render_blob(contentblock.blob, contentblock.datatype)\n\n  except Block.DoesNotExist:\n    blob = \"\u003cspan style='color:red'\u003eERROR: Cannot find block with a unique title of '%s'\u003c/span\u003e\" % uniquetitle\n\n  if user_auth and user.may_edit_blocks:\n    blob = wrap_blob_with_editablediv(blob, uniquetitle)\n\n  return blob\n"
}
{
    "repo_name": "cmbruns/osgswig",
    "ref": "refs/heads/master",
    "path": "examples/python/pyramid.py",
    "copies": "2",
    "content": "#!/usr/bin/env python\n\n# A simple example of how to create geometry\n# Pyramid geometry creation follows this tutorial:\n# http://www.openscenegraph.org/documentation/NPSTutorials/osgGeometry.html\n\n# import all necessary stuff\nimport sys\nimport osg\nimport osgDB\nimport osgGA\nimport osgViewer\n\n\n\n# create a root node\nnode = osg.Group()\n\n# Line Geometry\nlineGeode = osg.Geode()\nlineGeometry = osg.Geometry()\nlineGeode.addDrawable(lineGeometry)\nlineStateSet = lineGeode.getOrCreateStateSet()\n\nlineVertices = osg.Vec3Array()\nlineVertices.push_back(osg.Vec3(-10,10,0))\nlineVertices.push_back(osg.Vec3(-10,-10,0))\nlineGeometry.setVertexArray(lineVertices)\n\nlineBase = osg.DrawElementsUInt(osg.PrimitiveSet.LINES,0)\nlineBase.push_back(0)\nlineBase.push_back(1)\nlineGeometry.addPrimitiveSet(lineBase)\n\nnode.addChild(lineGeode)\n\n# Pyramid geometry, following tutorial \npyramidGeode = osg.Geode()\npyramidGeometry = osg.Geometry()\npyramidGeode.addDrawable(pyramidGeometry)\npyramidStateSet = pyramidGeode.getOrCreateStateSet()\n\npyramidVertices = osg.Vec3Array()\npyramidVertices.push_back(osg.Vec3(0,0,0))\npyramidVertices.push_back(osg.Vec3(10,0,0))\npyramidVertices.push_back(osg.Vec3(10,10,0))\npyramidVertices.push_back(osg.Vec3(0,10,0))\npyramidVertices.push_back(osg.Vec3(5,5,10))\npyramidGeometry.setVertexArray(pyramidVertices)\n\npyramidBase = osg.DrawElementsUInt(osg.PrimitiveSet.QUADS,0)\npyramidBase.push_back(3)\npyramidBase.push_back(2)\npyramidBase.push_back(1)\npyramidBase.push_back(0)\npyramidGeometry.addPrimitiveSet(pyramidBase)\n\npyramidFace1 = osg.DrawElementsUInt(osg.PrimitiveSet.TRIANGLES,0)\npyramidFace1.push_back(0)\npyramidFace1.push_back(1)\npyramidFace1.push_back(4)\npyramidGeometry.addPrimitiveSet(pyramidFace1)\n\npyramidFace2 = osg.DrawElementsUInt(osg.PrimitiveSet.TRIANGLES,0)\npyramidFace2.push_back(1)\npyramidFace2.push_back(2)\npyramidFace2.push_back(4)\npyramidGeometry.addPrimitiveSet(pyramidFace2)\n\npyramidFace3 = osg.DrawElementsUInt(osg.PrimitiveSet.TRIANGLES,0)\npyramidFace3.push_back(2)\npyramidFace3.push_back(3)\npyramidFace3.push_back(4)\npyramidGeometry.addPrimitiveSet(pyramidFace3)\n\npyramidFace4 = osg.DrawElementsUInt(osg.PrimitiveSet.TRIANGLES,0)\npyramidFace4.push_back(3)\npyramidFace4.push_back(0)\npyramidFace4.push_back(4)\npyramidGeometry.addPrimitiveSet(pyramidFace4)\n\ncolors= osg.Vec4Array()\ncolors.push_back(osg.Vec4(1,0,0,1)) #index 0 red\ncolors.push_back(osg.Vec4(0,1,0,1)) #index 1 green\ncolors.push_back(osg.Vec4(0,0,1,1)) #index 2 blue\ncolors.push_back(osg.Vec4(1,1,1,1)) #index 3 white\n\ncolorIndexArray = osg.UIntArray()\ncolorIndexArray.push_back(0)\ncolorIndexArray.push_back(1)\ncolorIndexArray.push_back(2)\ncolorIndexArray.push_back(3)\ncolorIndexArray.push_back(0)\n\npyramidGeometry.setColorArray(colors)\npyramidGeometry.setColorIndices(colorIndexArray)\npyramidGeometry.setColorBinding(osg.Geometry.BIND_PER_VERTEX)\n\n# if you want to manipulate the values later on with immediate feedback, don't use a display list\npyramidGeometry.setSupportsDisplayList(False)\n\nnode.addChild(pyramidGeode)\n\n# create a viewer\nviewer = osgViewer.Viewer()\n\n# configure\nviewer.setThreadingModel(osgViewer.Viewer.SingleThreaded)\n\n# add to the scene\nviewer.setSceneData(node)\n\n# loop until done\nviewer.run()\n"
}
{
    "repo_name": "chStaiger/ACES-Training",
    "ref": "refs/heads/master",
    "path": "code/BinaryNearestMeanClassifier.py",
    "copies": "2",
    "content": "\nimport numpy\n\nclass BinaryNearestMeanClassifierFactory(object):\n\n    def __init__(self, scoringFunction):\n\n        self.scoringFunction = scoringFunction\n        self.productName     = \"BinaryNearestMeanClassifier_\" + self.scoringFunction.__name__\n\n    def train(self, featureValues, classLabels):\n\n        # Check dimensionality of training data\n        (ns, nf) = featureValues.shape\n        assert classLabels.shape == (ns, )\n\n        # Check that the class labels are all booleans\n        assert frozenset(classLabels) == frozenset([False, True])\n\n        # Find the mean values for the False and True classes\n\n        meanFalse = numpy.mean(featureValues[numpy.where(classLabels == False)], axis = 0)\n        meanTrue  = numpy.mean(featureValues[numpy.where(classLabels == True )], axis = 0)\n\n        return BinaryNearestMeanClassifier(self.scoringFunction, meanFalse, meanTrue)\n\n\nclass BinaryNearestMeanClassifier(object):\n\n    def __init__(self, scoringFunction, meanFalse, meanTrue):\n\n        self.scoringFunction = scoringFunction\n        self.meanFalse       = meanFalse\n        self.meanTrue        = meanTrue\n\n    def score(self, samples):\n\n        return numpy.apply_along_axis(lambda sample : self.scoringFunction(self.meanFalse, self.meanTrue, sample), 1, samples)\n\n# V1, V2a, V2b, and V3 implement different distance metrics for the NMC classifiers.\n#\n# Note that the calculations as given here are normalized in such a way that the scores are invariant under\n# rotations, translations, and scalings of the vectors {sample, meanFalse, meanTrue}.\n\ndef V1(meanFalse, meanTrue, sample):\n    \"\"\"\n    This NMC distance metric projects the sample onto the line from meanFalse -\u003e meanTrue, and normalizes the value;\n    Points that project to meanFalse are scored as 0 (zero), points that project to meanTrue are scored as 1 (one).\n    \"\"\"\n    return numpy.inner(sample - meanFalse, meanTrue - meanFalse) / numpy.inner(meanTrue - meanFalse, meanTrue - meanFalse)\n\ndef V2a(meanFalse, meanTrue, sample):\n    \"\"\"\n    This NMC distance metric scores a sample by considering the (meanFalse, sample, meanTrue) triangle;\n    The score is calculated as the length (Euclidean distance) of the (meanFalse, sample) side, divided over the distance of meanFalse meanTrue via 'sample'.\n    \"\"\"\n    return numpy.linalg.norm(sample - meanFalse) / (numpy.linalg.norm(sample - meanFalse) + numpy.linalg.norm(sample - meanTrue))\n\n\ndef V2b(meanFalse, meanTrue, sample):\n    \"\"\"\n    This NMC distance metric scores a sample by subtracting its Euclidean distance to meanTrue from its distance to meanFalse.\n    The score is normalized to the distance between {meanFalse, meanTrue}.\n    The iso-score lines in this case are hyperbolas.\n    \"\"\"\n    return (numpy.linalg.norm(sample - meanFalse) - numpy.linalg.norm(sample - meanTrue)) / numpy.linalg.norm(meanTrue - meanFalse)\n\n\ndef V3(meanFalse, meanTrue, sample):\n    \"\"\"\n    This NMC distance metric scores samples by considering a point that is halfway {meanFalse, meanTrue}, then calculating the\n    cosine of the angle {sample, halfway, meanTrue}.\n    Points towards meanTrue get a score of close to +1, while points towards meanFalse get a score close to -1.\n    \"\"\"\n    halfway = 0.5 * (meanFalse + meanTrue)\n    return numpy.inner(sample - halfway, meanTrue - halfway) / (numpy.linalg.norm(sample - halfway) * numpy.linalg.norm(meanTrue - halfway))\n"
}
{
    "repo_name": "apibitsco/rainforestapp-python",
    "ref": "refs/heads/master",
    "path": "rainforest/apibits/version.py",
    "copies": "3",
    "content": "import os \nversionfile = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"../VERSION\")\nVERSION = open(versionfile).read()"
}
{
    "repo_name": "Khan/pyobjc-framework-Cocoa",
    "ref": "refs/heads/master",
    "path": "PyObjCTest/test_nsgraphics.py",
    "copies": "3",
    "content": "\nfrom PyObjCTools.TestSupport import *\nfrom AppKit import *\n\ntry:\n    unicode\nexcept NameError:\n    unicode = str\n\n\ntry:\n    long\nexcept NameError:\n    long = int\n\nclass TestNSGraphics (TestCase):\n    def testConstants(self):\n        self.assertEqual(NSCompositeClear, 0)\n        self.assertEqual(NSCompositeCopy, 1)\n        self.assertEqual(NSCompositeSourceOver, 2)\n        self.assertEqual(NSCompositeSourceIn, 3)\n        self.assertEqual(NSCompositeSourceOut, 4)\n        self.assertEqual(NSCompositeSourceAtop, 5)\n        self.assertEqual(NSCompositeDestinationOver, 6)\n        self.assertEqual(NSCompositeDestinationIn, 7)\n        self.assertEqual(NSCompositeDestinationOut, 8)\n        self.assertEqual(NSCompositeDestinationAtop, 9)\n        self.assertEqual(NSCompositeXOR, 10)\n        self.assertEqual(NSCompositePlusDarker, 11)\n        self.assertEqual(NSCompositeHighlight, 12)\n        self.assertEqual(NSCompositePlusLighter, 13)\n        self.assertEqual(NSBackingStoreRetained, 0)\n        self.assertEqual(NSBackingStoreNonretained, 1)\n        self.assertEqual(NSBackingStoreBuffered, 2)\n        self.assertEqual(NSWindowAbove,  1)\n        self.assertEqual(NSWindowBelow, -1)\n        self.assertEqual(NSWindowOut,  0)\n        self.assertEqual(NSFocusRingOnly, 0)\n        self.assertEqual(NSFocusRingBelow, 1)\n        self.assertEqual(NSFocusRingAbove, 2)\n        self.assertEqual(NSFocusRingTypeDefault, 0)\n        self.assertEqual(NSFocusRingTypeNone, 1)\n        self.assertEqual(NSFocusRingTypeExterior, 2)\n\n        self.assertIsInstance(NSCalibratedWhiteColorSpace, unicode)\n        self.assertIsInstance(NSCalibratedBlackColorSpace, unicode)\n        self.assertIsInstance(NSCalibratedRGBColorSpace, unicode)\n        self.assertIsInstance(NSDeviceWhiteColorSpace, unicode)\n        self.assertIsInstance(NSDeviceBlackColorSpace, unicode)\n        self.assertIsInstance(NSDeviceRGBColorSpace, unicode)\n        self.assertIsInstance(NSDeviceCMYKColorSpace, unicode)\n        self.assertIsInstance(NSNamedColorSpace, unicode)\n        self.assertIsInstance(NSPatternColorSpace, unicode)\n        self.assertIsInstance(NSCustomColorSpace, unicode)\n        self.assertIsInstance(NSWhite, float)\n        self.assertIsInstance(NSLightGray, float)\n        self.assertIsInstance(NSDarkGray, float)\n        self.assertIsInstance(NSBlack, float)\n\n        self.assertIsInstance(NSDeviceResolution, unicode)\n        self.assertIsInstance(NSDeviceColorSpaceName, unicode)\n        self.assertIsInstance(NSDeviceBitsPerSample, unicode)\n        self.assertIsInstance(NSDeviceIsScreen, unicode)\n        self.assertIsInstance(NSDeviceIsPrinter, unicode)\n        self.assertIsInstance(NSDeviceSize, unicode)\n        self.assertEqual(NSAnimationEffectDisappearingItemDefault, 0)\n        self.assertEqual(NSAnimationEffectPoof, 10)\n\n\n    def testFunctions(self):\n        app = NSApplication.sharedApplication()\n\n        self.assertArgHasType(NSBestDepth, 4, b'o^' + objc._C_NSBOOL)\n        self.assertArgIsBOOL(NSBestDepth, 3)\n        d, e = NSBestDepth(NSDeviceRGBColorSpace, 8, 32, False, None)\n        self.assertIsInstance(d, (int, long))\n        self.assertIsInstance(e, bool)\n\n        self.assertResultIsBOOL(NSPlanarFromDepth)\n        self.assertIsInstance(NSPlanarFromDepth(0), bool)\n\n        self.assertIsInstance(NSColorSpaceFromDepth(0), unicode)\n        self.assertIsInstance(NSBitsPerSampleFromDepth(0), (int, long))\n        self.assertIsInstance(NSBitsPerPixelFromDepth(0), (int, long))\n        self.assertIsInstance(NSNumberOfColorComponents(NSDeviceRGBColorSpace), (int, long))\n\n        v = NSAvailableWindowDepths()\n        self.assertIsInstance(v, tuple)\n        self.assertNotEqual(len(v), 0)\n        self.assertIsInstance(v[0], int)\n\n        img = NSBitmapImageRep.alloc().initWithBitmapDataPlanes_pixelsWide_pixelsHigh_bitsPerSample_samplesPerPixel_hasAlpha_isPlanar_colorSpaceName_bitmapFormat_bytesPerRow_bitsPerPixel_(\n                None, 255, 255, 8, 4, True, False, NSCalibratedRGBColorSpace, 0, 0, 0)\n        context = NSGraphicsContext.graphicsContextWithBitmapImageRep_(img)\n        current = NSGraphicsContext.currentContext()\n        try:\n            NSGraphicsContext.setCurrentContext_(context)\n            NSRectFill(((0, 0), (1, 2)))\n\n            self.assertArgSizeInArg(NSRectFillList, 0, 1)\n            NSRectFillList([((0, 0), (1, 2)), ((10, 50), (9, 9))], 2)\n\n            self.assertArgSizeInArg(NSRectFillListWithGrays, 0, 2)\n            self.assertArgSizeInArg(NSRectFillListWithGrays, 1, 2)\n            NSRectFillListWithGrays([((0, 0), (1, 2)), ((10, 50), (9, 9))], (0.5, 0.6), 2)\n\n            self.assertArgSizeInArg(NSRectFillListWithColors, 0, 2)\n            self.assertArgSizeInArg(NSRectFillListWithColors, 1, 2)\n            NSRectFillListWithColors([((0, 0), (1, 2)), ((10, 50), (9, 9))], (NSColor.blueColor(), NSColor.redColor()), 2)\n\n            NSRectFillUsingOperation(((0, 0), (1, 2)), NSCompositeSourceOver)\n\n            self.assertArgSizeInArg(NSRectFillListUsingOperation, 0, 1)\n            NSRectFillListUsingOperation([((0, 0), (1, 2)), ((10, 50), (9, 9))], 2, NSCompositeSourceOver)\n\n            self.assertArgSizeInArg(NSRectFillListWithColorsUsingOperation, 0, 2)\n            self.assertArgSizeInArg(NSRectFillListWithColorsUsingOperation, 1, 2)\n            NSRectFillListWithColorsUsingOperation([((0, 0), (1, 2)), ((10, 50), (9, 9))], (NSColor.blueColor(), NSColor.redColor()), 2, NSCompositeSourceOver)\n\n            NSFrameRect(((5, 5), (20, 30)))\n            NSFrameRectWithWidth(((5, 5), (20, 30)), 4)\n            NSFrameRectWithWidthUsingOperation(((5, 5), (20, 30)), 4, NSCompositeSourceOver)\n\n            NSRectClip(((5, 5), (200, 200)))\n            self.assertArgSizeInArg(NSRectClipList, 0, 1)\n            NSRectClipList([((5, 5), (200, 200)), ((50, 50), (90, 100))], 2)\n\n            color = NSReadPixel((5, 5))\n            self.assertIsInstance(color, NSColor)\n\n            self.assertArgSizeInArg(NSDrawTiledRects, 2, 4)\n            self.assertArgSizeInArg(NSDrawTiledRects, 3, 4)\n            self.assertArgIsIn(NSDrawTiledRects, 2)\n            self.assertArgIsIn(NSDrawTiledRects, 3)\n            NSDrawTiledRects(((10, 10), (50, 50)), ((15, 15), (10, 10)),  [NSMinXEdge, NSMaxXEdge], [0.8, 0.9], 2)\n\n            NSDrawGrayBezel(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSDrawGroove(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSDrawWhiteBezel(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSDrawButton(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSEraseRect(((0, 0), (10, 10)))\n            NSCopyBits(0, ((10, 10), (50, 50)), (50, 50))\n            NSHighlightRect(((10, 10), (50, 50)))\n            NSDrawDarkBezel(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSDrawLightBezel(((0, 0), (10, 10)), ((0, 0), (50, 50)))\n            NSDottedFrameRect(((10, 10), (50, 50)))\n            NSDrawWindowBackground(((10, 10), (50, 50)))\n\n        finally:\n            NSGraphicsContext.setCurrentContext_(current)\n\n            NSSetFocusRingStyle(NSFocusRingAbove)\n\n            self.assertArgIsOut(NSGetWindowServerMemory, 1)\n            self.assertArgIsOut(NSGetWindowServerMemory, 2)\n            self.assertArgIsOut(NSGetWindowServerMemory, 3)\n            r = NSGetWindowServerMemory(0, None, None, None)\n            self.assertIsInstance(r[0], (int, long))\n            self.assertIsInstance(r[1], (int, long))\n            self.assertIsInstance(r[2], (int, long))\n\n            self.assertArgSizeInArg(NSDrawColorTiledRects, 2, 4)\n            self.assertArgSizeInArg(NSDrawColorTiledRects, 3, 4)\n            self.assertArgIsIn(NSDrawColorTiledRects, 2)\n            self.assertArgIsIn(NSDrawColorTiledRects, 3)\n            NSDrawColorTiledRects(((10, 10), (50, 50)), ((15, 15), (10, 10)),  [NSMinXEdge, NSMaxXEdge], [NSColor.redColor(), NSColor.blueColor()], 2)\n\n            #self.assertArgIsBOOL(NSDrawBitmap, 7)\n            #self.assertArgIsBOOL(NSDrawBitmap, 8)\n            #NSDrawBitmap(((0, 0), (10, 10)), 10, 20, 8, 4, 32, 40, False, True,\n            #        NSDeviceRGBColorSpace, [' '*4*10*20, '', '', '', ''])\n\n            self.assertArgSizeInArg(NSWindowList, 1, 0)\n            self.assertArgIsOut(NSWindowList, 1)\n            v = NSWindowList(5, None)\n            self.assertIsInstance(v, tuple)\n            self.assertEqual(len(v), 5)\n            self.assertIsInstance(v[0], (int, long))\n\n            self.assertArgIsOut(NSCountWindowsForContext, 1)\n            v = NSCountWindowsForContext(1, None)\n            self.assertIsInstance(v, (int, long))\n\n            self.assertArgIsOut(NSWindowListForContext, 2)\n            self.assertArgSizeInArg(NSWindowListForContext, 2, 1)\n            v = NSWindowListForContext(0, 5, None)\n            self.assertIsInstance(v, tuple)\n            self.assertEqual(len(v), 5)\n            self.assertIsInstance(v[0], (int, long))\n\n\n\n\n        NSBeep()\n        count = NSCountWindows(None)\n        self.assertIsInstance(count, (int, long))\n\n        try:\n            NSDisableScreenUpdates()\n        except objc.error:\n            pass\n\n        try:\n            NSEnableScreenUpdates()\n        except objc.error:\n            pass\n\n        self.assertArgIsSEL(NSShowAnimationEffect, 4, b'v@:^v')\n        self.assertArgHasType(NSShowAnimationEffect, 5, b'^v')\n        try:\n            NSShowAnimationEffect(NSAnimationEffectPoof, (10, 10), (20, 30), None, None, None)\n        except objc.error:\n            pass\n\n    @min_os_level('10.5')\n    def testConstants10_5(self):\n        self.assertEqual(NSColorRenderingIntentDefault, 0)\n        self.assertEqual(NSColorRenderingIntentAbsoluteColorimetric, 1)\n        self.assertEqual(NSColorRenderingIntentRelativeColorimetric, 2)\n        self.assertEqual(NSColorRenderingIntentPerceptual, 3)\n        self.assertEqual(NSColorRenderingIntentSaturation, 4)\n\n        self.assertEqual(NSImageInterpolationDefault, 0)\n        self.assertEqual(NSImageInterpolationNone, 1)\n        self.assertEqual(NSImageInterpolationLow, 2)\n        self.assertEqual(NSImageInterpolationHigh, 3)\n\n    @min_os_level('10.6')\n    def testConstants10_6(self):\n        self.assertEqual(NSWindowDepthTwentyfourBitRGB, 0x208)\n        self.assertEqual(NSWindowDepthSixtyfourBitRGB, 0x210)\n        self.assertEqual(NSWindowDepthOnehundredtwentyeightBitRGB, 0x220)\n\n        self.assertEqual(NSImageInterpolationMedium, 4)\n\n\nif __name__ == \"__main__\":\n    NSApplication.sharedApplication()\n    main()\n"
}
{
    "repo_name": "MalloyPower/parsing-python",
    "ref": "refs/heads/master",
    "path": "front-end/testsuite-python-lib/Python-2.4/Lib/distutils/command/__init__.py",
    "copies": "4",
    "content": "\"\"\"distutils.command\n\nPackage containing implementation of all the standard Distutils\ncommands.\"\"\"\n\n# This module should be kept compatible with Python 2.1.\n\n__revision__ = \"$Id: __init__.py,v 1.21 2004/11/10 22:23:14 loewis Exp $\"\n\n__all__ = ['build',\n           'build_py',\n           'build_ext',\n           'build_clib',\n           'build_scripts',\n           'clean',\n           'install',\n           'install_lib',\n           'install_headers',\n           'install_scripts',\n           'install_data',\n           'sdist',\n           'register',\n           'bdist',\n           'bdist_dumb',\n           'bdist_rpm',\n           'bdist_wininst',\n           # These two are reserved for future use:\n           #'bdist_sdux',\n           #'bdist_pkgtool',\n           # Note:\n           # bdist_packager is not included because it only provides\n           # an abstract base class\n          ]\n"
}
{
    "repo_name": "rsnakamura/theape",
    "ref": "refs/heads/master",
    "path": "theape/documentation/developer/explorations/explore_dateutil/index.py",
    "copies": "4",
    "content": "\n# this package\nfrom ape.commoncode.index_builder import create_toctree\n\n\ncreate_toctree(maxdepth=1)\n"
}
{
    "repo_name": "play113/swer",
    "ref": "refs/heads/master",
    "path": "opencamlib-read-only/scripts/cutter_shapes.py",
    "copies": "7",
    "content": "import ocl\nimport camvtk\nimport time\nimport vtk\nimport datetime\nimport math\n\ndef drawLoops(myscreen, loops, loopcolor):\n    nloop = 0\n    for lop in loops:\n        n = 0\n        N = len(lop)\n        first_point=ocl.Point(-1,-1,5)\n        previous=ocl.Point(-1,-1,5)\n        for p in lop:\n            if n==0: # don't draw anything on the first iteration\n                previous=p \n                first_point = p\n            elif n== (N-1): # the last point\n                myscreen.addActor( camvtk.Line(p1=(previous.x,previous.y,previous.z),p2=(p.x,p.y,p.z),color=loopcolor) ) # the normal line\n                # and a line from p to the first point\n                myscreen.addActor( camvtk.Line(p1=(p.x,p.y,p.z),p2=(first_point.x,first_point.y,first_point.z),color=loopcolor) )\n            else:\n                myscreen.addActor( camvtk.Line(p1=(previous.x,previous.y,previous.z),p2=(p.x,p.y,p.z),color=loopcolor) )\n                previous=p\n            n=n+1\n        print \"rendered loop \",nloop, \" with \", len(lop), \" points\"\n        nloop = nloop+1\n        \ndef getWaterline(s, cutter, zh, sampling):\n    wl = ocl.Waterline()\n    #wl.setThreads(1) # single thread for easier debug\n    wl.setSTL(s)\n    wl.setCutter(cutter)\n    wl.setZ(zh)\n    wl.setSampling(sampling)\n    wl.run()\n    loops = wl.getLoops()\n    return loops\n\ndef getPathsY(s,cutter,sampling,y):\n    #apdc = ocl.PathDropCutter()\n    apdc = ocl.AdaptivePathDropCutter()\n    apdc.setSTL(s)\n    apdc.setCutter(cutter) \n    apdc.setZ( -20 ) \n    apdc.setSampling(sampling)\n    apdc.setMinSampling(sampling/700)\n    path = ocl.Path() \n    p1 = ocl.Point(-1.52*cutter.getDiameter() , y,-111)   # start-point of line\n    p2 = ocl.Point(+1.52*cutter.getDiameter(), y,-111)   # end-point of line\n    l = ocl.Line(p1,p2)     # line-object\n    path.append( l )  \n    apdc.setPath( path )\n    apdc.run() \n    return apdc.getCLPoints()\n\ndef getPathsX(s,cutter,sampling,x):\n    #apdc = ocl.PathDropCutter()\n    apdc = ocl.AdaptivePathDropCutter()\n    apdc.setSTL(s)\n    apdc.setCutter(cutter) \n    apdc.setZ( -20 ) \n    apdc.setSampling(sampling)\n    apdc.setMinSampling(sampling/700)\n    path = ocl.Path() \n    p1 = ocl.Point(x, -1.52*cutter.getDiameter() , -111)   # start-point of line\n    p2 = ocl.Point(x, +1.52*cutter.getDiameter(), -111)   # end-point of line\n    l = ocl.Line(p1,p2)     # line-object\n    path.append( l )  \n    apdc.setPath( path )\n    apdc.run() \n    return apdc.getCLPoints()\n\nif __name__ == \"__main__\":  \n    print ocl.revision()\n    myscreen = camvtk.VTKScreen()\n    #stl = camvtk.STLSurf(\"../stl/demo.stl\")\n    #stl = camvtk.STLSurf(\"../stl/30sphere.stl\")\n    #myscreen.addActor(stl)\n    \n    base=0.1\n    tip=10\n    a=ocl.Point(base,0,-tip)\n    myscreen.addActor(camvtk.Point(center=(a.x,a.y,a.z), color=(1,0,1)));\n    b=ocl.Point(-base,0,-tip)    \n    myscreen.addActor(camvtk.Point(center=(b.x,b.y,b.z), color=(1,0,1)));\n    c=ocl.Point(0,0,0)\n    myscreen.addActor( camvtk.Point(center=(c.x,c.y,c.z), color=(1,0,1)));\n    #myscreen.addActor( camvtk.Line(p1=(1,0,0),p2=(0,0,0.3)) )\n    #myscreen.addActor( camvtk.Line(p1=(0,0,0.3),p2=(0,1,0)) )\n    #myscreen.addActor( camvtk.Line(p1=(1,0,0),p2=(0,1,0)) )\n    t = ocl.Triangle(a,b,c)\n    s = ocl.STLSurf()\n    s.addTriangle(t)\n    \n    print \"STL surface read,\", s.size(), \"triangles\"\n    \n    Nwaterlines = 40\n    zh=[-0.15*x for x in xrange(Nwaterlines)]\n    #zh=[15]\n    diam = 3.01\n    length = 50\n    loops = []\n    sampling = 0.1\n    \n    #cutter = ocl.CylCutter( diam , length )\n    #cutter = ocl.BallCutter( diam , length )\n    #cutter = ocl.BullCutter( diam , diam/5, length )\n    #cutter = ocl.ConeCutter(diam, math.pi/3, length)\n    #cutter =  ocl.CylConeCutter(diam/3,diam,math.pi/9)\n    #cutter = ocl.BallConeCutter(diam/3,diam,math.pi/9)\n    #cutter = ocl.BullConeCutter(diam/2, diam/10, diam, math.pi/10)\n    cutter = ocl.ConeConeCutter(diam/2,math.pi/3,diam,math.pi/6)\n    \n    \n    ptsy_all = []\n    ptsx_all = []\n    yvals=[]\n    Nmax=15\n    for i in range(Nmax):\n        yvals.append( diam* float(i)/float(Nmax) )\n        yvals.append( -diam* float(i)/float(Nmax) )\n        \n    for y in yvals: #[diam*0.4, diam*0.2, 0, -diam*0.2,diam*(-0.4)]:\n        ptsy = getPathsY(s,cutter,sampling, y)\n        ptsx = getPathsX(s,cutter,sampling, y)\n        ptsy_all.append(ptsy)\n        ptsx_all.append(ptsx)\n        \n    #print \" got \",len(pts),\" cl-points\"\n    #for p in pts:\n    #    print p.x,\" \",p.y,\" \",p.z\n    #exit()\n    \n    loops = []\n    for z in zh:\n        \n        z_loops = getWaterline(s, cutter, z, sampling)\n        for l in z_loops:\n            loops.append(l)\n    \n    \n    #for l in line:\n        \n    #drawLoops(myscreen, line, camvtk.cyan)\n    #for l in cutter_loops:\n    #    loops.append(l)\n    \n    print \"All waterlines done. Got\", len(loops),\" loops in total.\"\n    # draw the loops\n    drawLoops(myscreen, loops, camvtk.yellow)\n    drawLoops(myscreen, ptsy_all, camvtk.red)\n    drawLoops(myscreen, ptsx_all, camvtk.green)\n    \n    print \"done.\"\n    myscreen.camera.SetPosition(15, 13, 7)\n    myscreen.camera.SetFocalPoint(5, 5, 0)\n    camvtk.drawArrows(myscreen,center=(0,0,3))\n    camvtk.drawOCLtext(myscreen)\n    myscreen.render()    \n    myscreen.iren.Start()\n    #raw_input(\"Press Enter to terminate\") \n"
}
{
    "repo_name": "aqfaridi/Code-Online-Judge",
    "ref": "refs/heads/master",
    "path": "build/web/env/Main1139/Main1139.py",
    "copies": "7",
    "content": "for i in range(1,90000000-1):\r\n   p =1 \r\nwhile True:\r\n    n = input()\r\n    if(n == 42):\r\n        break\r\n    else:\r\n        print n\r\n\r\n"
}
{
    "repo_name": "JeyZeta/Dangerous",
    "ref": "refs/heads/master",
    "path": "Dangerous/Golismero/tools/sqlmap/plugins/generic/search.py",
    "copies": "8",
    "content": "#!/usr/bin/env python\n\n\"\"\"\nCopyright (c) 2006-2013 sqlmap developers (http://sqlmap.org/)\nSee the file 'doc/COPYING' for copying permission\n\"\"\"\n\nfrom lib.core.agent import agent\nfrom lib.core.common import arrayizeValue\nfrom lib.core.common import Backend\nfrom lib.core.common import filterPairValues\nfrom lib.core.common import getLimitRange\nfrom lib.core.common import isInferenceAvailable\nfrom lib.core.common import isNoneValue\nfrom lib.core.common import isNumPosStrValue\nfrom lib.core.common import isTechniqueAvailable\nfrom lib.core.common import readInput\nfrom lib.core.common import safeSQLIdentificatorNaming\nfrom lib.core.common import safeStringFormat\nfrom lib.core.common import unArrayizeValue\nfrom lib.core.common import unsafeSQLIdentificatorNaming\nfrom lib.core.data import conf\nfrom lib.core.data import kb\nfrom lib.core.data import logger\nfrom lib.core.data import paths\nfrom lib.core.data import queries\nfrom lib.core.enums import CHARSET_TYPE\nfrom lib.core.enums import DBMS\nfrom lib.core.enums import EXPECTED\nfrom lib.core.enums import PAYLOAD\nfrom lib.core.exception import SqlmapMissingMandatoryOptionException\nfrom lib.core.exception import SqlmapUserQuitException\nfrom lib.core.settings import CURRENT_DB\nfrom lib.core.settings import METADB_SUFFIX\nfrom lib.request import inject\nfrom lib.techniques.brute.use import columnExists\nfrom lib.techniques.brute.use import tableExists\n\nclass Search:\n    \"\"\"\n    This class defines search functionalities for plugins.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def searchDb(self):\n        foundDbs = []\n        rootQuery = queries[Backend.getIdentifiedDbms()].search_db\n        dbList = conf.db.split(\",\")\n\n        if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n            dbCond = rootQuery.inband.condition2\n        else:\n            dbCond = rootQuery.inband.condition\n\n        dbConsider, dbCondParam = self.likeOrExact(\"database\")\n\n        for db in dbList:\n            values = []\n            db = safeSQLIdentificatorNaming(db)\n\n            if Backend.getIdentifiedDbms() in (DBMS.ORACLE, DBMS.DB2):\n                db = db.upper()\n\n            infoMsg = \"searching database\"\n            if dbConsider == \"1\":\n                infoMsg += \"s like\"\n            infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(db)\n            logger.info(infoMsg)\n\n            if conf.excludeSysDbs:\n                exclDbsQuery = \"\".join(\" AND '%s' != %s\" % (unsafeSQLIdentificatorNaming(db), dbCond) for db in self.excludeDbsList)\n                infoMsg = \"skipping system database%s '%s'\" % (\"s\" if len(self.excludeDbsList) \u003e 1 else \"\", \", \".join(db for db in self.excludeDbsList))\n                logger.info(infoMsg)\n            else:\n                exclDbsQuery = \"\"\n\n            dbQuery = \"%s%s\" % (dbCond, dbCondParam)\n            dbQuery = dbQuery % unsafeSQLIdentificatorNaming(db)\n\n            if any(isTechniqueAvailable(_) for _ in (PAYLOAD.TECHNIQUE.UNION, PAYLOAD.TECHNIQUE.ERROR, PAYLOAD.TECHNIQUE.QUERY)) or conf.direct:\n                if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n                    query = rootQuery.inband.query2\n                else:\n                    query = rootQuery.inband.query\n\n                query = query % (dbQuery + exclDbsQuery)\n                values = inject.getValue(query, blind=False, time=False)\n\n                if not isNoneValue(values):\n                    values = arrayizeValue(values)\n\n                    for value in values:\n                        value = safeSQLIdentificatorNaming(value)\n                        foundDbs.append(value)\n\n            if not values and isInferenceAvailable() and not conf.direct:\n                infoMsg = \"fetching number of database\"\n                if dbConsider == \"1\":\n                    infoMsg += \"s like\"\n                infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(db)\n                logger.info(infoMsg)\n\n                if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n                    query = rootQuery.blind.count2\n                else:\n                    query = rootQuery.blind.count\n\n                query = query % (dbQuery + exclDbsQuery)\n                count = inject.getValue(query, union=False, error=False, expected=EXPECTED.INT, charsetType=CHARSET_TYPE.DIGITS)\n\n                if not isNumPosStrValue(count):\n                    warnMsg = \"no database\"\n                    if dbConsider == \"1\":\n                        warnMsg += \"s like\"\n                    warnMsg += \" '%s' found\" % unsafeSQLIdentificatorNaming(db)\n                    logger.warn(warnMsg)\n\n                    continue\n\n                indexRange = getLimitRange(count)\n\n                for index in indexRange:\n                    if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n                        query = rootQuery.blind.query2\n                    else:\n                        query = rootQuery.blind.query\n\n                    query = query % (dbQuery + exclDbsQuery)\n                    query = agent.limitQuery(index, query, dbCond)\n\n                    value = unArrayizeValue(inject.getValue(query, union=False, error=False))\n                    value = safeSQLIdentificatorNaming(value)\n                    foundDbs.append(value)\n\n        conf.dumper.lister(\"found databases\", foundDbs)\n\n    def searchTable(self):\n        bruteForce = False\n\n        if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n            errMsg = \"information_schema not available, \"\n            errMsg += \"back-end DBMS is MySQL \u003c 5.0\"\n            bruteForce = True\n\n        if bruteForce:\n            message = \"do you want to use common table existence check? %s\" % (\"[Y/n/q]\" if Backend.getIdentifiedDbms() in (DBMS.ACCESS,) else \"[y/N/q]\")\n            test = readInput(message, default=\"Y\" if \"Y\" in message else \"N\")\n\n            if test[0] in (\"n\", \"N\"):\n                return\n            elif test[0] in (\"q\", \"Q\"):\n                raise SqlmapUserQuitException\n            else:\n                regex = \"|\".join(conf.tbl.split(\",\"))\n                return tableExists(paths.COMMON_TABLES, regex)\n\n        foundTbls = {}\n        tblList = conf.tbl.split(\",\")\n        rootQuery = queries[Backend.getIdentifiedDbms()].search_table\n        tblCond = rootQuery.inband.condition\n        dbCond = rootQuery.inband.condition2\n        tblConsider, tblCondParam = self.likeOrExact(\"table\")\n\n        for tbl in tblList:\n            values = []\n            tbl = safeSQLIdentificatorNaming(tbl, True)\n\n            if Backend.getIdentifiedDbms() in (DBMS.ORACLE, DBMS.DB2, DBMS.FIREBIRD):\n                tbl = tbl.upper()\n\n            infoMsg = \"searching table\"\n            if tblConsider == \"1\":\n                infoMsg += \"s like\"\n            infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(tbl)\n\n            if dbCond and conf.db and conf.db != CURRENT_DB:\n                _ = conf.db.split(\",\")\n                whereDbsQuery = \" AND (\" + \" OR \".join(\"%s = '%s'\" % (dbCond, unsafeSQLIdentificatorNaming(db)) for db in _) + \")\"\n                infoMsg += \" for database%s '%s'\" % (\"s\" if len(_) \u003e 1 else \"\", \", \".join(db for db in _))\n            elif conf.excludeSysDbs:\n                whereDbsQuery = \"\".join(\" AND '%s' != %s\" % (unsafeSQLIdentificatorNaming(db), dbCond) for db in self.excludeDbsList)\n                infoMsg2 = \"skipping system database%s '%s'\" % (\"s\" if len(self.excludeDbsList) \u003e 1 else \"\", \", \".join(db for db in self.excludeDbsList))\n                logger.info(infoMsg2)\n            else:\n                whereDbsQuery = \"\"\n\n            logger.info(infoMsg)\n\n            tblQuery = \"%s%s\" % (tblCond, tblCondParam)\n            tblQuery = tblQuery % unsafeSQLIdentificatorNaming(tbl)\n\n            if any(isTechniqueAvailable(_) for _ in (PAYLOAD.TECHNIQUE.UNION, PAYLOAD.TECHNIQUE.ERROR, PAYLOAD.TECHNIQUE.QUERY)) or conf.direct:\n                query = rootQuery.inband.query\n\n                query = query % (tblQuery + whereDbsQuery)\n                values = inject.getValue(query, blind=False, time=False)\n\n                if values and Backend.getIdentifiedDbms() in (DBMS.SQLITE, DBMS.FIREBIRD):\n                    newValues = []\n\n                    if isinstance(values, basestring):\n                        values = [values]\n                    for value in values:\n                        dbName = \"SQLite\" if Backend.isDbms(DBMS.SQLITE) else \"Firebird\"\n                        newValues.append([\"%s%s\" % (dbName, METADB_SUFFIX), value])\n\n                    values = newValues\n\n                for foundDb, foundTbl in filterPairValues(values):\n                    foundDb = safeSQLIdentificatorNaming(foundDb)\n                    foundTbl = safeSQLIdentificatorNaming(foundTbl, True)\n\n                    if foundDb is None or foundTbl is None:\n                        continue\n\n                    if foundDb in foundTbls:\n                        foundTbls[foundDb].append(foundTbl)\n                    else:\n                        foundTbls[foundDb] = [foundTbl]\n\n            if not values and isInferenceAvailable() and not conf.direct:\n                if Backend.getIdentifiedDbms() not in (DBMS.SQLITE, DBMS.FIREBIRD):\n                    if len(whereDbsQuery) == 0:\n                        infoMsg = \"fetching number of databases with table\"\n                        if tblConsider == \"1\":\n                            infoMsg += \"s like\"\n                        infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(tbl)\n                        logger.info(infoMsg)\n\n                        query = rootQuery.blind.count\n                        query = query % (tblQuery + whereDbsQuery)\n                        count = inject.getValue(query, union=False, error=False, expected=EXPECTED.INT, charsetType=CHARSET_TYPE.DIGITS)\n\n                        if not isNumPosStrValue(count):\n                            warnMsg = \"no databases have table\"\n                            if tblConsider == \"1\":\n                                warnMsg += \"s like\"\n                            warnMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(tbl)\n                            logger.warn(warnMsg)\n\n                            continue\n\n                        indexRange = getLimitRange(count)\n\n                        for index in indexRange:\n                            query = rootQuery.blind.query\n                            query = query % (tblQuery + whereDbsQuery)\n                            query = agent.limitQuery(index, query)\n\n                            foundDb = unArrayizeValue(inject.getValue(query, union=False, error=False))\n                            foundDb = safeSQLIdentificatorNaming(foundDb)\n\n                            if foundDb not in foundTbls:\n                                foundTbls[foundDb] = []\n\n                            if tblConsider == \"2\":\n                                foundTbls[foundDb].append(tbl)\n\n                        if tblConsider == \"2\":\n                            continue\n                    else:\n                        for db in conf.db.split(\",\"):\n                            db = safeSQLIdentificatorNaming(db)\n                            if db not in foundTbls:\n                                foundTbls[db] = []\n                else:\n                    dbName = \"SQLite\" if Backend.isDbms(DBMS.SQLITE) else \"Firebird\"\n                    foundTbls[\"%s%s\" % (dbName, METADB_SUFFIX)] = []\n\n                for db in foundTbls.keys():\n                    db = safeSQLIdentificatorNaming(db)\n\n                    infoMsg = \"fetching number of table\"\n                    if tblConsider == \"1\":\n                        infoMsg += \"s like\"\n                    infoMsg += \" '%s' in database '%s'\" % (unsafeSQLIdentificatorNaming(tbl), unsafeSQLIdentificatorNaming(db))\n                    logger.info(infoMsg)\n\n                    query = rootQuery.blind.count2\n                    if Backend.getIdentifiedDbms() not in (DBMS.SQLITE, DBMS.FIREBIRD):\n                        query = query % unsafeSQLIdentificatorNaming(db)\n                    query += \" AND %s\" % tblQuery\n\n                    count = inject.getValue(query, union=False, error=False, expected=EXPECTED.INT, charsetType=CHARSET_TYPE.DIGITS)\n\n                    if not isNumPosStrValue(count):\n                        warnMsg = \"no table\"\n                        if tblConsider == \"1\":\n                            warnMsg += \"s like\"\n                        warnMsg += \" '%s' \" % unsafeSQLIdentificatorNaming(tbl)\n                        warnMsg += \"in database '%s'\" % unsafeSQLIdentificatorNaming(db)\n                        logger.warn(warnMsg)\n\n                        continue\n\n                    indexRange = getLimitRange(count)\n\n                    for index in indexRange:\n                        query = rootQuery.blind.query2\n\n                        if query.endswith(\"'%s')\"):\n                            query = query[:-1] + \" AND %s)\" % tblQuery\n                        else:\n                            query += \" AND %s\" % tblQuery\n\n                        if Backend.isDbms(DBMS.FIREBIRD):\n                            query = safeStringFormat(query, index)\n\n                        if Backend.getIdentifiedDbms() not in (DBMS.SQLITE, DBMS.FIREBIRD):\n                            query = safeStringFormat(query, unsafeSQLIdentificatorNaming(db))\n\n                        if not Backend.isDbms(DBMS.FIREBIRD):\n                            query = agent.limitQuery(index, query)\n\n                        foundTbl = unArrayizeValue(inject.getValue(query, union=False, error=False))\n                        if not isNoneValue(foundTbl):\n                            kb.hintValue = foundTbl\n                            foundTbl = safeSQLIdentificatorNaming(foundTbl, True)\n                            foundTbls[db].append(foundTbl)\n\n        for db in foundTbls.keys():\n            if isNoneValue(foundTbls[db]):\n                del foundTbls[db]\n\n        if not foundTbls:\n            warnMsg = \"no databases contain any of the provided tables\"\n            logger.warn(warnMsg)\n            return\n\n        conf.dumper.dbTables(foundTbls)\n        self.dumpFoundTables(foundTbls)\n\n    def searchColumn(self):\n        bruteForce = False\n\n        if Backend.isDbms(DBMS.MYSQL) and not kb.data.has_information_schema:\n            errMsg = \"information_schema not available, \"\n            errMsg += \"back-end DBMS is MySQL \u003c 5.0\"\n            bruteForce = True\n\n        if bruteForce:\n            message = \"do you want to use common column existence check? %s\" % (\"[Y/n/q]\" if Backend.getIdentifiedDbms() in (DBMS.ACCESS,) else \"[y/N/q]\")\n            test = readInput(message, default=\"Y\" if \"Y\" in message else \"N\")\n\n            if test[0] in (\"n\", \"N\"):\n                return\n            elif test[0] in (\"q\", \"Q\"):\n                raise SqlmapUserQuitException\n            else:\n                regex = \"|\".join(conf.col.split(\",\"))\n                conf.dumper.dbTableColumns(columnExists(paths.COMMON_COLUMNS, regex))\n\n                message = \"do you want to dump entries? [Y/n] \"\n                output = readInput(message, default=\"Y\")\n\n                if output and output[0] not in (\"n\", \"N\"):\n                    self.dumpAll()\n\n                return\n\n        rootQuery = queries[Backend.getIdentifiedDbms()].search_column\n        foundCols = {}\n        dbs = {}\n        whereDbsQuery = \"\"\n        whereTblsQuery = \"\"\n        infoMsgTbl = \"\"\n        infoMsgDb = \"\"\n        colList = conf.col.split(\",\")\n        origTbl = conf.tbl\n        origDb = conf.db\n        colCond = rootQuery.inband.condition\n        dbCond = rootQuery.inband.condition2\n        tblCond = rootQuery.inband.condition3\n        colConsider, colCondParam = self.likeOrExact(\"column\")\n\n        for column in colList:\n            values = []\n            column = safeSQLIdentificatorNaming(column)\n            conf.db = origDb\n            conf.tbl = origTbl\n\n            if Backend.getIdentifiedDbms() in (DBMS.ORACLE, DBMS.DB2):\n                column = column.upper()\n\n            infoMsg = \"searching column\"\n            if colConsider == \"1\":\n                infoMsg += \"s like\"\n            infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(column)\n\n            foundCols[column] = {}\n\n            if conf.tbl:\n                _ = conf.tbl.split(\",\")\n                whereTblsQuery = \" AND (\" + \" OR \".join(\"%s = '%s'\" % (tblCond, unsafeSQLIdentificatorNaming(tbl)) for tbl in _) + \")\"\n                infoMsgTbl = \" for table%s '%s'\" % (\"s\" if len(_) \u003e 1 else \"\", \", \".join(unsafeSQLIdentificatorNaming(tbl) for tbl in _))\n\n            if conf.db and conf.db != CURRENT_DB:\n                _ = conf.db.split(\",\")\n                whereDbsQuery = \" AND (\" + \" OR \".join(\"%s = '%s'\" % (dbCond, unsafeSQLIdentificatorNaming(db)) for db in _) + \")\"\n                infoMsgDb = \" in database%s '%s'\" % (\"s\" if len(_) \u003e 1 else \"\", \", \".join(unsafeSQLIdentificatorNaming(db) for db in _))\n            elif conf.excludeSysDbs:\n                whereDbsQuery = \"\".join(\" AND %s != '%s'\" % (dbCond, unsafeSQLIdentificatorNaming(db)) for db in self.excludeDbsList)\n                infoMsg2 = \"skipping system database%s '%s'\" % (\"s\" if len(self.excludeDbsList) \u003e 1 else \"\", \", \".join(unsafeSQLIdentificatorNaming(db) for db in self.excludeDbsList))\n                logger.info(infoMsg2)\n            else:\n                infoMsgDb = \" across all databases\"\n\n            logger.info(\"%s%s%s\" % (infoMsg, infoMsgTbl, infoMsgDb))\n\n            colQuery = \"%s%s\" % (colCond, colCondParam)\n            colQuery = colQuery % unsafeSQLIdentificatorNaming(column)\n\n            if any(isTechniqueAvailable(_) for _ in (PAYLOAD.TECHNIQUE.UNION, PAYLOAD.TECHNIQUE.ERROR, PAYLOAD.TECHNIQUE.QUERY)) or conf.direct:\n                if not all((conf.db, conf.tbl)):\n                    # Enumerate tables containing the column provided if\n                    # either of database(s) or table(s) is not provided\n                    query = rootQuery.inband.query\n                    query = query % (colQuery + whereDbsQuery + whereTblsQuery)\n                    values = inject.getValue(query, blind=False, time=False)\n                else:\n                    # Assume provided databases' tables contain the\n                    # column(s) provided\n                    values = []\n\n                    for db in conf.db.split(\",\"):\n                        for tbl in conf.tbl.split(\",\"):\n                            values.append([safeSQLIdentificatorNaming(db), safeSQLIdentificatorNaming(tbl, True)])\n\n                for db, tbl in filterPairValues(values):\n                    db = safeSQLIdentificatorNaming(db)\n                    tbls = tbl.split(\",\") if not isNoneValue(tbl) else []\n\n                    for tbl in tbls:\n                        tbl = safeSQLIdentificatorNaming(tbl, True)\n\n                        if db is None or tbl is None:\n                            continue\n\n                        conf.db = db\n                        conf.tbl = tbl\n                        conf.col = column\n\n                        self.getColumns(onlyColNames=True, colTuple=(colConsider, colCondParam), bruteForce=False)\n\n                        if db in kb.data.cachedColumns and tbl in kb.data.cachedColumns[db]:\n                            if db not in dbs:\n                                dbs[db] = {}\n\n                            if tbl not in dbs[db]:\n                                dbs[db][tbl] = {}\n\n                            dbs[db][tbl].update(kb.data.cachedColumns[db][tbl])\n\n                            if db in foundCols[column]:\n                                foundCols[column][db].append(tbl)\n                            else:\n                                foundCols[column][db] = [tbl]\n\n                        kb.data.cachedColumns = {}\n\n            if not values and isInferenceAvailable() and not conf.direct:\n                if not conf.db:\n                    infoMsg = \"fetching number of databases with tables containing column\"\n                    if colConsider == \"1\":\n                        infoMsg += \"s like\"\n                    infoMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(column)\n                    logger.info(\"%s%s%s\" % (infoMsg, infoMsgTbl, infoMsgDb))\n\n                    query = rootQuery.blind.count\n                    query = query % (colQuery + whereDbsQuery + whereTblsQuery)\n                    count = inject.getValue(query, union=False, error=False, expected=EXPECTED.INT, charsetType=CHARSET_TYPE.DIGITS)\n\n                    if not isNumPosStrValue(count):\n                        warnMsg = \"no databases have tables containing column\"\n                        if colConsider == \"1\":\n                            warnMsg += \"s like\"\n                        warnMsg += \" '%s'\" % unsafeSQLIdentificatorNaming(column)\n                        logger.warn(\"%s%s\" % (warnMsg, infoMsgTbl))\n\n                        continue\n\n                    indexRange = getLimitRange(count)\n\n                    for index in indexRange:\n                        query = rootQuery.blind.query\n                        query = query % (colQuery + whereDbsQuery + whereTblsQuery)\n                        query = agent.limitQuery(index, query)\n\n                        db = unArrayizeValue(inject.getValue(query, union=False, error=False))\n                        db = safeSQLIdentificatorNaming(db)\n\n                        if db not in dbs:\n                            dbs[db] = {}\n\n                        if db not in foundCols[column]:\n                            foundCols[column][db] = []\n                else:\n                    for db in conf.db.split(\",\"):\n                        db = safeSQLIdentificatorNaming(db)\n                        if db not in foundCols[column]:\n                            foundCols[column][db] = []\n\n                origDb = conf.db\n                origTbl = conf.tbl\n\n                for column, dbData in foundCols.items():\n                    colQuery = \"%s%s\" % (colCond, colCondParam)\n                    colQuery = colQuery % unsafeSQLIdentificatorNaming(column)\n\n                    for db in dbData:\n                        conf.db = origDb\n                        conf.tbl = origTbl\n\n                        infoMsg = \"fetching number of tables containing column\"\n                        if colConsider == \"1\":\n                            infoMsg += \"s like\"\n                        infoMsg += \" '%s' in database '%s'\" % (unsafeSQLIdentificatorNaming(column), unsafeSQLIdentificatorNaming(db))\n                        logger.info(infoMsg)\n\n                        query = rootQuery.blind.count2\n                        query = query % unsafeSQLIdentificatorNaming(db)\n                        query += \" AND %s\" % colQuery\n                        query += whereTblsQuery\n\n                        count = inject.getValue(query, union=False, error=False, expected=EXPECTED.INT, charsetType=CHARSET_TYPE.DIGITS)\n\n                        if not isNumPosStrValue(count):\n                            warnMsg = \"no tables contain column\"\n                            if colConsider == \"1\":\n                                warnMsg += \"s like\"\n                            warnMsg += \" '%s' \" % unsafeSQLIdentificatorNaming(column)\n                            warnMsg += \"in database '%s'\" % unsafeSQLIdentificatorNaming(db)\n                            logger.warn(warnMsg)\n\n                            continue\n\n                        indexRange = getLimitRange(count)\n\n                        for index in indexRange:\n                            query = rootQuery.blind.query2\n\n                            if query.endswith(\"'%s')\"):\n                                query = query[:-1] + \" AND %s)\" % (colQuery + whereTblsQuery)\n                            else:\n                                query += \" AND %s\" % (colQuery + whereTblsQuery)\n\n                            query = safeStringFormat(query, unsafeSQLIdentificatorNaming(db))\n                            query = agent.limitQuery(index, query)\n\n                            tbl = unArrayizeValue(inject.getValue(query, union=False, error=False))\n                            kb.hintValue = tbl\n\n                            tbl = safeSQLIdentificatorNaming(tbl, True)\n\n                            conf.db = db\n                            conf.tbl = tbl\n                            conf.col = column\n\n                            self.getColumns(onlyColNames=True, colTuple=(colConsider, colCondParam), bruteForce=False)\n\n                            if db in kb.data.cachedColumns and tbl in kb.data.cachedColumns[db]:\n                                if db not in dbs:\n                                    dbs[db] = {}\n\n                                if tbl not in dbs[db]:\n                                    dbs[db][tbl] = {}\n\n                                dbs[db][tbl].update(kb.data.cachedColumns[db][tbl])\n\n                            kb.data.cachedColumns = {}\n\n                            if db in foundCols[column]:\n                                foundCols[column][db].append(tbl)\n                            else:\n                                foundCols[column][db] = [tbl]\n\n        if dbs:\n            conf.dumper.dbColumns(foundCols, colConsider, dbs)\n            self.dumpFoundColumn(dbs, foundCols, colConsider)\n        else:\n            warnMsg = \"no databases have tables containing any of the \"\n            warnMsg += \"provided columns\"\n            logger.warn(warnMsg)\n\n    def search(self):\n        if Backend.getIdentifiedDbms() in (DBMS.ORACLE, DBMS.DB2):\n            for item in ('db', 'tbl', 'col'):\n                if getattr(conf, item, None):\n                    setattr(conf, item, getattr(conf, item).upper())\n\n        if conf.col:\n            self.searchColumn()\n        elif conf.tbl:\n            self.searchTable()\n        elif conf.db:\n            self.searchDb()\n        else:\n            errMsg = \"missing parameter, provide -D, -T or -C along \"\n            errMsg += \"with --search\"\n            raise SqlmapMissingMandatoryOptionException(errMsg)\n"
}
{
    "repo_name": "vileopratama/vitech",
    "ref": "refs/heads/master",
    "path": "src/addons/l10n_ro/__init__.py",
    "copies": "47",
    "content": "# -*- encoding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\n# @author -  Fekete Mihai \u003cfeketemihai@gmail.com\u003e, Tatár Attila \u003catta@nvm.ro\u003e\n# Copyright (C) 2015 Tatár Attila\n# Copyright (C) 2015 Forest and Biomass Services Romania (http://www.forbiom.eu).\n# Copyright (C) 2011 TOTAL PC SYSTEMS (http://www.erpsystems.ro).\n# Copyright (C) 2009 (\u003chttp://www.filsystem.ro\u003e)\n\nimport res_partner\n"
}
{
    "repo_name": "ensemblr/llvm-project-boilerplate",
    "ref": "refs/heads/master",
    "path": "compiler-hw-2-master/hw3/llvm-3.8.0.src/utils/lldbDataFormatters.py",
    "copies": "56",
    "content": "\"\"\"\nLLDB Formatters for LLVM data types.\n\nLoad into LLDB with 'command script import /path/to/lldbDataFormatters.py'\n\"\"\"\n\ndef __lldb_init_module(debugger, internal_dict):\n    debugger.HandleCommand('type category define -e llvm -l c++')\n    debugger.HandleCommand('type synthetic add -w llvm '\n                           '-l lldbDataFormatters.SmallVectorSynthProvider '\n                           '-x \"^llvm::SmallVectorImpl\u003c.+\u003e$\"')\n    debugger.HandleCommand('type synthetic add -w llvm '\n                           '-l lldbDataFormatters.SmallVectorSynthProvider '\n                           '-x \"^llvm::SmallVector\u003c.+,.+\u003e$\"')\n    debugger.HandleCommand('type synthetic add -w llvm '\n                           '-l lldbDataFormatters.ArrayRefSynthProvider '\n                           '-x \"^llvm::ArrayRef\u003c.+\u003e$\"')\n    debugger.HandleCommand('type summary add -w llvm '\n                           '-F lldbDataFormatters.OptionalSummaryProvider '\n                           '-x \"^llvm::Optional\u003c.+\u003e$\"')\n\n# Pretty printer for llvm::SmallVector/llvm::SmallVectorImpl\nclass SmallVectorSynthProvider:\n    def __init__(self, valobj, dict):\n        self.valobj = valobj;\n        self.update() # initialize this provider\n\n    def num_children(self):\n        begin = self.begin.GetValueAsUnsigned(0)\n        end = self.end.GetValueAsUnsigned(0)\n        return (end - begin)/self.type_size\n\n    def get_child_index(self, name):\n        try:\n            return int(name.lstrip('[').rstrip(']'))\n        except:\n            return -1;\n\n    def get_child_at_index(self, index):\n        # Do bounds checking.\n        if index \u003c 0:\n            return None\n        if index \u003e= self.num_children():\n            return None;\n\n        offset = index * self.type_size\n        return self.begin.CreateChildAtOffset('['+str(index)+']',\n                                              offset, self.data_type)\n\n    def update(self):\n        self.begin = self.valobj.GetChildMemberWithName('BeginX')\n        self.end = self.valobj.GetChildMemberWithName('EndX')\n        the_type = self.valobj.GetType()\n        # If this is a reference type we have to dereference it to get to the\n        # template parameter.\n        if the_type.IsReferenceType():\n            the_type = the_type.GetDereferencedType()\n\n        self.data_type = the_type.GetTemplateArgumentType(0)\n        self.type_size = self.data_type.GetByteSize()\n        assert self.type_size != 0\n\nclass ArrayRefSynthProvider:\n    \"\"\" Provider for llvm::ArrayRef \"\"\"\n    def __init__(self, valobj, dict):\n        self.valobj = valobj;\n        self.update() # initialize this provider\n\n    def num_children(self):\n        return self.length\n\n    def get_child_index(self, name):\n        try:\n            return int(name.lstrip('[').rstrip(']'))\n        except:\n            return -1;\n\n    def get_child_at_index(self, index):\n        if index \u003c 0 or index \u003e= self.num_children():\n            return None;\n        offset = index * self.type_size\n        return self.data.CreateChildAtOffset('[' + str(index) + ']',\n                                             offset, self.data_type)\n\n    def update(self):\n        self.data = self.valobj.GetChildMemberWithName('Data')\n        length_obj = self.valobj.GetChildMemberWithName('Length')\n        self.length = length_obj.GetValueAsUnsigned(0)\n        self.data_type = self.data.GetType().GetPointeeType()\n        self.type_size = self.data_type.GetByteSize()\n        assert self.type_size != 0\n\ndef OptionalSummaryProvider(valobj, internal_dict):\n    if not valobj.GetChildMemberWithName('hasVal').GetValueAsUnsigned(0):\n        return 'None'\n    underlying_type = valobj.GetType().GetTemplateArgumentType(0)\n    storage = valobj.GetChildMemberWithName('storage')\n    return str(storage.Cast(underlying_type))\n"
}
{
    "repo_name": "marrow/web.component.page",
    "ref": "refs/heads/develop",
    "path": "web/component/page/block/content.py",
    "copies": "1",
    "content": "# encoding: utf-8\n\nfrom mongoengine import StringField, MapField, URLField, ImageField\n\nfrom web.contentment.util import D_\n\nfrom .base import Block\nfrom .content_ import render_text_block\n\nfrom web.component.asset.xml.templates import text_block_content\nfrom web.component.asset.xml.importers import text_block_content as text_block_content_importer\nfrom web.contentment.util.model import Properties\n\n\nclass DescriptionBlock(Block):\n\t__icon__ = 'asterisk'\n\n\nclass TextBlock(Block):\n\t__icon__ = 'font'\n\n\t__xml_exporters__ = {\n\t\t'content': text_block_content,\n\t}\n\n\t__xml_importers__ = {\n\t\t'content': text_block_content_importer,\n\t}\n\t\n\tcontent = MapField(StringField(), default=dict, simple=False)  # TODO: TranslatedField.\n\tformat = StringField(default='html', choices=['html', 'textile', 'md', 'rest'])  # TODO: Dynamic.\n\t\n\t# Data Portability\n\t\n\tdef __html_stream__(self, context=None):\n\t\treturn render_text_block(context, self, D_(self.content))\n\t\n\tdef __json__(self):\n\t\treturn dict(super(TextBlock, self).as_json,\n\t\t\t\ttarget = self._data['target'].id\n\t\t\t)\n\t\n\tdef __text__(self):\n\t\treturn ''  # TODO: Content extraction.\n\n\nclass QuoteBlock(Block):\n\t__icon__ = 'wc-b-quote'\n\t\n\tpass\n\n\nclass ButtonBlock(Block):\n\t__icon__ = 'wc-b-button'\n\t\n\tlabel = MapField(StringField(), db_field='c', default=dict)  # TODO: TranslatedField.\n\ttarget = URLField(db_field='t')\n\n\nclass ImageBlock(Block):\n\t__icon__ = 'wc-b-image'\n\t\n\timage = ImageField(db_field='i')\n\tsource = URLField(db_field='s')\n\ttarget = URLField(db_field='t')\n\tcaption = MapField(StringField(), db_field='c', default=dict)  # TODO: TranslatedField.\n"
}
{
    "repo_name": "fisele/slimta-abusix",
    "ref": "refs/heads/develop",
    "path": "slimta/smtp/auth.py",
    "copies": "1",
    "content": "# Copyright (c) 2012 Ian C. Good\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n#\n\nfrom __future__ import absolute_import\n\nimport re\nimport base64\n\nfrom pysasl import AuthenticationError, ServerChallenge, \\\n    AuthenticationCredentials\n\nfrom . import SmtpError\nfrom .reply import Reply\n\n__all__ = ['ServerAuthError', 'AuthSession']\n\nnoarg_pattern = re.compile(br'^([a-zA-Z0-9_-]+)$')\nwitharg_pattern = re.compile(br'^([a-zA-Z0-9_-]+)\\s+(.+)$')\n\n\nclass ServerAuthError(SmtpError):\n\n    def __init__(self, msg, reply):\n        super(ServerAuthError, self).__init__(msg)\n        self.reply = reply\n\n\nclass InvalidAuthString(ServerAuthError):\n\n    def __init__(self):\n        msg = 'Invalid authentication string'\n        reply = Reply('501', '5.5.2 '+msg)\n        super(InvalidAuthString, self).__init__(msg, reply)\n\n\nclass InvalidMechanismError(ServerAuthError):\n\n    def __init__(self):\n        msg = 'Invalid authentication mechanism'\n        reply = Reply('504', '5.5.4 '+msg)\n        super(InvalidMechanismError, self).__init__(msg, reply)\n\n\nclass AuthenticationCanceled(ServerAuthError):\n\n    def __init__(self):\n        msg = 'Authentication canceled by client'\n        reply = Reply('501', '5.7.0 '+msg)\n        super(AuthenticationCanceled, self).__init__(msg, reply)\n\n\nclass UnexpectedAuthError(ServerAuthError):\n\n    def __init__(self, exc):\n        reply = Reply('501', '5.5.2 '+str(exc))\n        super(UnexpectedAuthError, self).__init__(str(exc), reply)\n\n\nclass AuthSession(object):\n\n    def __init__(self, auth, io):\n        super(AuthSession, self).__init__()\n        self.auth = auth\n        self.io = io\n\n    def __str__(self):\n        available = self.server_mechanisms\n        if available:\n            return ' '.join(sorted([mech.name.decode('ascii')\n                                    for mech in available]))\n        else:\n            raise ValueError('No mechanisms available')\n\n    def _parse_arg(self, arg):\n        match = noarg_pattern.match(arg)\n        if match:\n            return match.group(1).upper(), None\n        match = witharg_pattern.match(arg)\n        if match:\n            return match.group(1).upper(), match.group(2)\n        raise InvalidMechanismError()\n\n    @property\n    def server_mechanisms(self):\n        return [mech for mech in self.auth.server_mechanisms\n                if self.io.encrypted or not getattr(mech, 'insecure', False)]\n\n    @property\n    def client_mechanisms(self):\n        return [mech for mech in self.auth.client_mechanisms\n                if self.io.encrypted or not getattr(mech, 'insecure', False)]\n\n    def _server_challenge(self, challenge, response=None):\n        if not response:\n            challenge_raw = base64.b64encode(challenge).decode('ascii')\n            Reply('334', challenge_raw).send(self.io, flush=True)\n            response = self.io.recv_line()\n        if response == b'*':\n            raise AuthenticationCanceled()\n        try:\n            return base64.b64decode(response)\n        except TypeError:\n            raise InvalidAuthString()\n\n    def server_attempt(self, arg):\n        mechanism_name, mechanism_arg = self._parse_arg(arg)\n        for mechanism in self.server_mechanisms:\n            if mechanism.name == mechanism_name:\n                responses = []\n                while True:\n                    try:\n                        return mechanism.server_attempt(responses)\n                    except AuthenticationError as exc:\n                        raise UnexpectedAuthError(exc)\n                    except ServerChallenge as chal:\n                        resp = self._server_challenge(chal.challenge,\n                                                      mechanism_arg)\n                        mechanism_arg = None\n                        chal.set_response(resp)\n                        responses.append(chal)\n        raise InvalidMechanismError()\n\n    def _client_respond(self, mech, response, first=False):\n        if first:\n            command = b' '.join((b'AUTH', mech.name))\n            if response:\n                response_raw = base64.b64encode(response)\n                command = b' '.join((command, response_raw))\n        else:\n            command = base64.b64encode(response)\n        self.io.send_command(command)\n        self.io.flush_send()\n        ret = Reply(command=b'AUTH')\n        ret.recv(self.io)\n        if ret.code == '334':\n            return base64.b64decode(ret.message), ret\n        return None, ret\n\n    def client_attempt(self, authcid, secret, authzid, mech_name):\n        mechanism = self.auth.get(mech_name)\n        if not mechanism:\n            raise InvalidMechanismError()\n        creds = AuthenticationCredentials(authcid, secret, authzid)\n        resp = mechanism.client_attempt(creds, [])\n        chal, reply = self._client_respond(\n            mechanism, resp.get_response(), True)\n        responses = [resp]\n        while chal is not None:\n            resp.set_challenge(chal)\n            resp = mechanism.client_attempt(creds, responses)\n            responses.append(resp)\n            chal, reply = self._client_respond(mechanism, resp.get_response())\n        return reply\n\n\n# vim:et:fdm=marker:sts=4:sw=4:ts=4\n"
}
{
    "repo_name": "marrow/web.dispatch.meta",
    "ref": "refs/heads/develop",
    "path": "web/dispatch/meta/release.py",
    "copies": "1",
    "content": "# encoding: utf-8\n\n\"\"\"Release information about WebCore.\"\"\"\n\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\n\nversion_info = namedtuple('version_info', ('major', 'minor', 'micro', 'releaselevel', 'serial'))(1, 0, 0, 'alpha', 1)\nversion = \".\".join([str(i) for i in version_info[:3]]) + ((version_info.releaselevel[0] + str(version_info.serial)) if version_info.releaselevel != 'final' else '')\n\nauthor = namedtuple('Author', ['name', 'email'])(\"Alice Bevan-McGregor\", 'alice@gothcandy.com')\ndescription = \"A collection of common dispatch protocol middleware.\"\ncopyright = \"2009-2015, Alice Bevan-McGregor and contributors\"\nurl = 'https://docs.webcore.io/dispatch/meta'\n"
}
{
    "repo_name": "anhstudios/swganh",
    "ref": "refs/heads/develop",
    "path": "data/scripts/templates/object/draft_schematic/structure/city/shared_garden_exotic_dantooine.py",
    "copies": "2",
    "content": "#### NOTICE: THIS FILE IS AUTOGENERATED\n#### MODIFICATIONS MAY BE LOST IF DONE IMPROPERLY\n#### PLEASE SEE THE ONLINE DOCUMENTATION FOR EXAMPLES\n\nfrom swgpy.object import *\t\n\ndef create(kernel):\n\tresult = Intangible()\n\n\tresult.template = \"object/draft_schematic/structure/city/shared_garden_exotic_dantooine.iff\"\n\tresult.attribute_template_id = -1\n\tresult.stfName(\"string_id_table\",\"\")\t\t\n\t\n\t#### BEGIN MODIFICATIONS ####\n\t####  END MODIFICATIONS  ####\n\t\n\treturn result"
}
{
    "repo_name": "briehl/narrative",
    "ref": "refs/heads/develop",
    "path": "src/biokbase/narrative/tests/test_log_proxy.py",
    "copies": "2",
    "content": "\"\"\"\nTest log proxy and kblogging\n\"\"\"\nimport logging\nimport os\nimport signal\nimport time\nimport unittest\nfrom biokbase.narrative.common import log_proxy as proxy\n\n__author__ = \"Dan Gunter \u003cdkgunter@lbl.gov\u003e\"\n\n\nclass MainTestCase(unittest.TestCase):\n    # name of var lines up with cmdline arg 'dest', don't change\n    conf = \"/tmp/kbase_logforward.conf\"\n    vb = 0\n    smpcfg = \"sample\"\n    meta = None\n\n    def setUp(self):\n        self._config([\"db: test\", \"collection: kblog\"])\n        if proxy.g_log is None:\n            proxy.g_log = logging.getLogger(proxy.LOGGER_NAME)\n\n    def _config(self, lines):\n        text = \"\\n\".join(lines)\n        with open(self.conf, \"w\") as f:\n            f.write(text)\n\n    def test_run_proxy(self):\n        pid = os.fork()\n        if pid == 0:\n            print(\"Run child\")\n            proxy.run(self)\n        else:\n            time.sleep(1)\n            print(\"Wait for child to start\")\n            # let it start\n            time.sleep(4)\n            # send it a HUP to stop it\n            print(\"Send child ({:d}) a HUP\".format(pid))\n            os.kill(pid, signal.SIGHUP)\n            # wait for it to stop\n            print(\"Wait for child ({:d}) to stop\".format(pid))\n            cpid, r = os.waitpid(pid, 0)\n            print(\"cpid, r: {}, {}\".format(cpid, r))\n            self.assertTrue(r \u003c 2, \"Bad exit status ({:d}) from proxy\".format(r))\n\n    def test_configuration(self):\n        # empty\n        self._config([])\n        self.assertRaises(ValueError, proxy.DBConfiguration, self.conf)\n        # missing collection\n        self._config([\"db: test\"])\n        self.assertRaises(KeyError, proxy.DBConfiguration, self.conf)\n        # bad db name\n        self._config([\"db: 1test\", \"collection: kblog\"])\n        self.assertRaises(ValueError, proxy.DBConfiguration, self.conf)\n        # bad db name\n        self._config([\"db: test.er\", \"collection: kblog\"])\n        self.assertRaises(ValueError, proxy.DBConfiguration, self.conf)\n        # too long\n        self._config(\n            [\n                \"db: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\",\n                \"collection: bbbbbbbbbbbbbbbbbbcccccccccccccccccccccccc\"\n                \"ddddddddddddddddddddddddddddddd\",\n            ]\n        )\n        self.assertRaises(ValueError, proxy.DBConfiguration, self.conf)\n        # bad collection\n        self._config([\"db: test\", \"collection: kb$log\"])\n        self.assertRaises(ValueError, proxy.DBConfiguration, self.conf)\n        # user, no pass\n        self._config([\"db: test\", \"collection: kblog\", \"user: joe\"])\n        self.assertRaises(KeyError, proxy.DBConfiguration, self.conf)\n\n\nclass LogRecordTest(unittest.TestCase):\n    def setUp(self):\n        if proxy.g_log is None:\n            proxy.g_log = logging.getLogger(proxy.LOGGER_NAME)\n\n    def test_basic(self):\n        for input in {}, {\"message\": \"hello\"}:\n            kbrec = proxy.DBRecord(input)\n        kbrec = proxy.DBRecord({\"message\": \"greeting;Hello=World\"})\n        self.assertEqual(kbrec.record[\"event\"], \"greeting\")\n        self.assertEqual(kbrec.record[\"Hello\"], \"World\")\n\n    def test_strict(self):\n        for inp in (\n            {\"xanthium\": 12},\n            {12: \"xanthium\"},\n            {\"message\": \"Hello=World;greeting\"},\n        ):\n            proxy.DBRecord(inp)\n            self.assertRaises(ValueError, proxy.DBRecord, inp, strict=True)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"
}
{
    "repo_name": "Xaltotun/conan",
    "ref": "refs/heads/develop",
    "path": "conans/server/rest/multipart_encoder.py",
    "copies": "8",
    "content": "'''\nFor generate Response in streaming with N files (chunked, happy memory server)\nIt uses multipart/mixed format\nExample:\n\nHTTP/1.0 200 OK\nConnection: close\nDate: Wed, 24 Jun 2009 23:41:40 GMT\nContent-Type: multipart/mixed;boundary=AMZ90RFX875LKMFasdf09DDFF3\nClient-Date: Wed, 24 Jun 2009 23:41:40 GMT\nClient-Peer: 127.0.0.1:3000\nClient-Response-Num: 1\nMIME-Version: 1.0\nStatus: 200\n\n--AMZ90RFX875LKMFasdf09DDFF3\nContent-type: image/jpeg\nContent-transfer-encoding: binary\nContent-disposition: attachment; filename=\"001.jpg\"\n\n\u003c\u003c here goes binary data \u003e\u003e--AMZ90RFX875LKMFasdf09DDFF3\nContent-type: image/jpeg\nContent-transfer-encoding: binary\nContent-disposition: attachment; filename=\"002.jpg\"\n\n\u003c\u003c here goes binary data \u003e\u003e--AMZ90RFX875LKMFasdf09DDFF3\n--AMZ90RFX875LKMFasdf09DDFF3--\n\n'''\nimport os\n\nDEFAULT_BOUNDARY = \"AMZ90RFX875LKMFasdf09DDFF3\"\n\n\ndef get_response_chunk(response, basepath, filepaths, boundary=DEFAULT_BOUNDARY):\n    \"\"\"\n    File is a list of filepaths\n    \"\"\"\n    response.add_header(\"Content-Type\", \"multipart/mixed;charset=utf-8;boundary=%s\" % boundary)\n\n    for filepath in filepaths:\n        yield \"--%s\\n\" % boundary + _headers_for_file(filepath)\n        for data in iter(read_file_chunked(os.path.join(basepath, filepath))):\n            yield data.encode(\"hex\")\n\n    yield (\"--%s\\n--%s\\n\" % (boundary, boundary))\n\n\ndef _headers_for_file(filename):\n    return '''Content-type: application/octet-stream;charset=hex\nContent-transfer-encoding: binary\nContent-disposition: attachment; filename=\"%s\"\n\n''' % filename\n\n\ndef read_file_chunked(filepath):\n    thefile = open(filepath, \"rb\")\n    for data in iter(read_in_chunks(thefile)):\n        yield data\n\n\ndef read_in_chunks(file_object, chunk_size=1024):\n    \"\"\"Lazy function (generator) to read a file piece by piece.\n    Default chunk size: 1k.\n    Warn: Python file.read is returning \\n instead of EOF,\n    we read two chunks before return to know if EOF is reached\n    so used memory is chunk_size*2\"\"\"\n    while True:\n        buf1 = file_object.read(chunk_size)\n        if not buf1:\n            break\n\n#        buf2 = file_object.read(chunk_size)\n#         if not buf2:  # Last buf1 has and ending \\n that is a EOF\n#             buf1 = buf1[:-1] if buf1[-1] == \"\\n\" else buf1\n#             yield buf1\n#         else:\n        yield buf1\n        #yield buf2\n\n\nif __name__ == \"__main__\":\n    # MAKE A TEST WITH THIS! I WOULD NOT LIKE TO CHECK THAT file.read bug with\n    # EOF IS NOT HAPPENING IN OTHER SO\n    paths = [\"/home/laso/.conans/openssl/2.0.1/lasote/testing/reg/cosa2.txt\",\n             \"/home/laso/.conans/openssl/2.0.1/lasote/testing/reg/conandigest.txt\"]\n    allt = \"\"\n    for tmp in iter(get_response_chunk(paths)):\n        allt += tmp\n\n    print(allt)\n"
}
{
    "repo_name": "onelab-eu/sfa",
    "ref": "refs/heads/geni-v3",
    "path": "sfa/methods/Describe.py",
    "copies": "2",
    "content": "import zlib\n\nfrom sfa.util.xrn import urn_to_hrn\nfrom sfa.util.method import Method\nfrom sfa.util.sfatablesRuntime import run_sfatables\nfrom sfa.util.faults import SfaInvalidArgument\nfrom sfa.trust.credential import Credential\n\nfrom sfa.storage.parameter import Parameter, Mixed\n\nclass Describe(Method):\n    \"\"\"\n    Retrieve a manifest RSpec describing the resources contained by the \n    named entities, e.g. a single slice or a set of the slivers in a \n    slice. This listing and description should be sufficiently \n    descriptive to allow experimenters to use the resources.    \n    @param credential list\n    @param options dictionary\n    @return dict\n    \"\"\"\n    interfaces = ['aggregate', 'slicemgr']\n    accepts = [\n        Parameter(type([str]), \"List of URNs\"),\n        Mixed(Parameter(str, \"Credential string\"), \n              Parameter(type([str]), \"List of credentials\")),\n        Parameter(dict, \"Options\")\n        ]\n    returns = Parameter(str, \"List of resources\")\n\n    def call(self, urns, creds, options):\n        self.api.logger.info(\"interface: %s\\tmethod-name: %s\" % (self.api.interface, self.name))\n       \n        # client must specify a version\n        if not options.get('geni_rspec_version'):\n            if options.get('rspec_version'):\n                options['geni_rspec_version'] = options['rspec_version']\n            else:\n                raise SfaInvalidArgument('Must specify an rspec version option. geni_rspec_version cannot be null')\n        valid_creds = self.api.auth.checkCredentialsSpeaksFor(creds, 'listnodes', urns, \n                                                              check_sliver_callback = self.api.driver.check_sliver_credentials,\n                                                              options=options)\n\n        # get hrn of the original caller \n        origin_hrn = options.get('origin_hrn', None)\n        if not origin_hrn:\n            origin_hrn = Credential(cred=valid_creds[0]).get_gid_caller().get_hrn()\n        desc = self.api.manager.Describe(self.api, creds, urns, options)\n\n        # filter rspec through sfatables \n        if self.api.interface in ['aggregate']:\n            chain_name = 'OUTGOING'\n        elif self.api.interface in ['slicemgr']: \n            chain_name = 'FORWARD-OUTGOING'\n        self.api.logger.debug(\"ListResources: sfatables on chain %s\"%chain_name)\n        desc['geni_rspec'] = run_sfatables(chain_name, '', origin_hrn, desc['geni_rspec']) \n \n        if options.has_key('geni_compressed') and options['geni_compressed'] == True:\n            desc['geni_rspec'] = zlib.compress(desc['geni_rspec']).encode('base64')\n\n        return desc  \n    \n    \n"
}
{
    "repo_name": "creamidea/Mushroom",
    "ref": "refs/heads/signal_page",
    "path": "NodeSite/NodeSite/accounts/models.py",
    "copies": "1",
    "content": "# -*- coding: utf-8 -*-\nfrom django.core.mail import send_mail\nfrom django.core import validators\n\nfrom django.contrib.auth.models import (BaseUserManager, Group)\n\nfrom django.db import models\n\nfrom django.utils.http import urlquote\nfrom django.utils import timezone\n\n# Create your models here.\n\nclass MRUser(BaseUserManager):\n    \"\"\"\n    创建用户\n    \"\"\"\n    \n    def create_user(self, username, email=None, password=None, **extra_fields):\n        \"\"\"\n        Create and saves a User with the given username, email and password\n        \"\"\"\n        now = timezone.now()\n        if not username:\n            raise ValueError(u'必须设置用户名')\n        email = BaseUserManager.normalize_email(email)\n        user = self.model(username=username, email=email,\n                          is_staff=False, is_active=True, is_superuser=False,\n                          last_login=now, date_joined=now, **extra_fields)\n        user.set_password(password)\n        user.save(using=self._db)\n        return user\n\n    def create_superuser(self, username, email, password, **extra_fields):\n        u = self.create_user(username, email, password, **extra_fields)\n        u.is_staff = True\n        u.is_active = True\n        u.is_superuser = True\n        u.save(using=self._db)\n        return u\n"
}